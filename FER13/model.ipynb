{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from network_full import *\n",
    "import os\n",
    "\n",
    "fer2013_label={'angry':0,'disgust':1,'fear':2,'happy':3,'neutral':4,'sad':5,'surprise':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data loading\n",
    "def load_data_fer(dataset,path):\n",
    "    labels= os.listdir(os.path.join(path,dataset))\n",
    "    X=[]\n",
    "    Y=[]\n",
    "\n",
    "    for label in labels:\n",
    "        for file in os.listdir(os.path.join(path,dataset,label)):\n",
    "            image=cv2.imread(os.path.join(path,dataset,label,file),cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            X.append(image)\n",
    "            Y.append(fer2013_label[label])\n",
    "    return np.array(X) , np.array(Y).astype('uint8')\n",
    "\n",
    "def create_data_fer(path):\n",
    "    X,Y=load_data_fer('train',path)\n",
    "    x_train,y_train=load_data_fer('test',path)\n",
    "    return X,Y,x_train,y_train\n",
    "\n",
    "x,y,x_test,y_test= create_data_fer('FER2013')\n",
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flattening & normalisation\n",
    "x=(x.reshape(x.shape[0],x.shape[1]*x.shape[2]).astype(np.float32)-127.5)/127.5\n",
    "x_test=(x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]).astype(np.float32)-127.5)/127.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 2, 5, 2], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shuffle data \n",
    "keys= np.array(range(x.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "x=x[keys]\n",
    "y=y[keys]\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model build up\n",
    "model=Model()\n",
    "model.add(layer_dense(x.shape[1],512))\n",
    "model.add(Activation_ReLu())\n",
    "model.add(Layer_Dropout(0.25))\n",
    "model.add(layer_dense(512,256))\n",
    "model.add(Activation_ReLu())\n",
    "model.add(Layer_Dropout(0.25))\n",
    "model.add(layer_dense(256,128))\n",
    "model.add(Activation_ReLu())\n",
    "model.add(Layer_Dropout(0.25))\n",
    "model.add(layer_dense(128,7))\n",
    "model.add(Activation_SoftMax())\n",
    "model.set(loss=Loss_CategoricalCrossEntropy(),\n",
    "          accuracy=Accuracy_Categorical(),\n",
    "          optimizer=Optimizer_Adam(learning_rate=0.001,decay=5e-5))\n",
    "model.finalize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      " step: 0, acc: 0.141, loss: 1.946,data_loss: 1.946, reg_loss: 0.000 lr: 0.001000\n",
      " step: 1, acc: 0.172, loss: 1.945,data_loss: 1.945, reg_loss: 0.000 lr: 0.001000\n",
      " step: 2, acc: 0.141, loss: 1.943,data_loss: 1.943, reg_loss: 0.000 lr: 0.001000\n",
      " step: 3, acc: 0.203, loss: 1.937,data_loss: 1.937, reg_loss: 0.000 lr: 0.001000\n",
      " step: 4, acc: 0.258, loss: 1.924,data_loss: 1.924, reg_loss: 0.000 lr: 0.001000\n",
      " step: 5, acc: 0.250, loss: 1.904,data_loss: 1.904, reg_loss: 0.000 lr: 0.001000\n",
      " step: 6, acc: 0.289, loss: 1.860,data_loss: 1.860, reg_loss: 0.000 lr: 0.001000\n",
      " step: 7, acc: 0.172, loss: 1.899,data_loss: 1.899, reg_loss: 0.000 lr: 0.001000\n",
      " step: 8, acc: 0.273, loss: 1.814,data_loss: 1.814, reg_loss: 0.000 lr: 0.001000\n",
      " step: 9, acc: 0.266, loss: 1.832,data_loss: 1.832, reg_loss: 0.000 lr: 0.001000\n",
      " step: 10, acc: 0.344, loss: 1.825,data_loss: 1.825, reg_loss: 0.000 lr: 0.001000\n",
      " step: 11, acc: 0.320, loss: 1.765,data_loss: 1.765, reg_loss: 0.000 lr: 0.000999\n",
      " step: 12, acc: 0.273, loss: 1.897,data_loss: 1.897, reg_loss: 0.000 lr: 0.000999\n",
      " step: 13, acc: 0.281, loss: 1.827,data_loss: 1.827, reg_loss: 0.000 lr: 0.000999\n",
      " step: 14, acc: 0.250, loss: 1.786,data_loss: 1.786, reg_loss: 0.000 lr: 0.000999\n",
      " step: 15, acc: 0.320, loss: 1.817,data_loss: 1.817, reg_loss: 0.000 lr: 0.000999\n",
      " step: 16, acc: 0.297, loss: 1.777,data_loss: 1.777, reg_loss: 0.000 lr: 0.000999\n",
      " step: 17, acc: 0.172, loss: 1.836,data_loss: 1.836, reg_loss: 0.000 lr: 0.000999\n",
      " step: 18, acc: 0.188, loss: 1.880,data_loss: 1.880, reg_loss: 0.000 lr: 0.000999\n",
      " step: 19, acc: 0.234, loss: 1.812,data_loss: 1.812, reg_loss: 0.000 lr: 0.000999\n",
      " step: 20, acc: 0.219, loss: 1.806,data_loss: 1.806, reg_loss: 0.000 lr: 0.000999\n",
      " step: 21, acc: 0.219, loss: 1.820,data_loss: 1.820, reg_loss: 0.000 lr: 0.000999\n",
      " step: 22, acc: 0.297, loss: 1.741,data_loss: 1.741, reg_loss: 0.000 lr: 0.000999\n",
      " step: 23, acc: 0.227, loss: 1.791,data_loss: 1.791, reg_loss: 0.000 lr: 0.000999\n",
      " step: 24, acc: 0.234, loss: 1.729,data_loss: 1.729, reg_loss: 0.000 lr: 0.000999\n",
      " step: 25, acc: 0.180, loss: 1.813,data_loss: 1.813, reg_loss: 0.000 lr: 0.000999\n",
      " step: 26, acc: 0.227, loss: 1.850,data_loss: 1.850, reg_loss: 0.000 lr: 0.000999\n",
      " step: 27, acc: 0.297, loss: 1.801,data_loss: 1.801, reg_loss: 0.000 lr: 0.000999\n",
      " step: 28, acc: 0.188, loss: 1.884,data_loss: 1.884, reg_loss: 0.000 lr: 0.000999\n",
      " step: 29, acc: 0.242, loss: 1.869,data_loss: 1.869, reg_loss: 0.000 lr: 0.000999\n",
      " step: 30, acc: 0.219, loss: 1.850,data_loss: 1.850, reg_loss: 0.000 lr: 0.000999\n",
      " step: 31, acc: 0.258, loss: 1.779,data_loss: 1.779, reg_loss: 0.000 lr: 0.000998\n",
      " step: 32, acc: 0.242, loss: 1.800,data_loss: 1.800, reg_loss: 0.000 lr: 0.000998\n",
      " step: 33, acc: 0.219, loss: 1.811,data_loss: 1.811, reg_loss: 0.000 lr: 0.000998\n",
      " step: 34, acc: 0.273, loss: 1.813,data_loss: 1.813, reg_loss: 0.000 lr: 0.000998\n",
      " step: 35, acc: 0.312, loss: 1.782,data_loss: 1.782, reg_loss: 0.000 lr: 0.000998\n",
      " step: 36, acc: 0.297, loss: 1.781,data_loss: 1.781, reg_loss: 0.000 lr: 0.000998\n",
      " step: 37, acc: 0.211, loss: 1.790,data_loss: 1.790, reg_loss: 0.000 lr: 0.000998\n",
      " step: 38, acc: 0.250, loss: 1.752,data_loss: 1.752, reg_loss: 0.000 lr: 0.000998\n",
      " step: 39, acc: 0.234, loss: 1.748,data_loss: 1.748, reg_loss: 0.000 lr: 0.000998\n",
      " step: 40, acc: 0.281, loss: 1.832,data_loss: 1.832, reg_loss: 0.000 lr: 0.000998\n",
      " step: 41, acc: 0.297, loss: 1.785,data_loss: 1.785, reg_loss: 0.000 lr: 0.000998\n",
      " step: 42, acc: 0.273, loss: 1.847,data_loss: 1.847, reg_loss: 0.000 lr: 0.000998\n",
      " step: 43, acc: 0.250, loss: 1.754,data_loss: 1.754, reg_loss: 0.000 lr: 0.000998\n",
      " step: 44, acc: 0.227, loss: 1.818,data_loss: 1.818, reg_loss: 0.000 lr: 0.000998\n",
      " step: 45, acc: 0.258, loss: 1.719,data_loss: 1.719, reg_loss: 0.000 lr: 0.000998\n",
      " step: 46, acc: 0.320, loss: 1.800,data_loss: 1.800, reg_loss: 0.000 lr: 0.000998\n",
      " step: 47, acc: 0.250, loss: 1.806,data_loss: 1.806, reg_loss: 0.000 lr: 0.000998\n",
      " step: 48, acc: 0.352, loss: 1.712,data_loss: 1.712, reg_loss: 0.000 lr: 0.000998\n",
      " step: 49, acc: 0.359, loss: 1.683,data_loss: 1.683, reg_loss: 0.000 lr: 0.000998\n",
      " step: 50, acc: 0.289, loss: 1.747,data_loss: 1.747, reg_loss: 0.000 lr: 0.000998\n",
      " step: 51, acc: 0.219, loss: 1.768,data_loss: 1.768, reg_loss: 0.000 lr: 0.000997\n",
      " step: 52, acc: 0.289, loss: 1.830,data_loss: 1.830, reg_loss: 0.000 lr: 0.000997\n",
      " step: 53, acc: 0.234, loss: 1.745,data_loss: 1.745, reg_loss: 0.000 lr: 0.000997\n",
      " step: 54, acc: 0.234, loss: 1.842,data_loss: 1.842, reg_loss: 0.000 lr: 0.000997\n",
      " step: 55, acc: 0.297, loss: 1.768,data_loss: 1.768, reg_loss: 0.000 lr: 0.000997\n",
      " step: 56, acc: 0.219, loss: 1.774,data_loss: 1.774, reg_loss: 0.000 lr: 0.000997\n",
      " step: 57, acc: 0.359, loss: 1.726,data_loss: 1.726, reg_loss: 0.000 lr: 0.000997\n",
      " step: 58, acc: 0.281, loss: 1.740,data_loss: 1.740, reg_loss: 0.000 lr: 0.000997\n",
      " step: 59, acc: 0.312, loss: 1.678,data_loss: 1.678, reg_loss: 0.000 lr: 0.000997\n",
      " step: 60, acc: 0.258, loss: 1.741,data_loss: 1.741, reg_loss: 0.000 lr: 0.000997\n",
      " step: 61, acc: 0.227, loss: 1.733,data_loss: 1.733, reg_loss: 0.000 lr: 0.000997\n",
      " step: 62, acc: 0.289, loss: 1.713,data_loss: 1.713, reg_loss: 0.000 lr: 0.000997\n",
      " step: 63, acc: 0.312, loss: 1.708,data_loss: 1.708, reg_loss: 0.000 lr: 0.000997\n",
      " step: 64, acc: 0.289, loss: 1.781,data_loss: 1.781, reg_loss: 0.000 lr: 0.000997\n",
      " step: 65, acc: 0.242, loss: 1.740,data_loss: 1.740, reg_loss: 0.000 lr: 0.000997\n",
      " step: 66, acc: 0.336, loss: 1.771,data_loss: 1.771, reg_loss: 0.000 lr: 0.000997\n",
      " step: 67, acc: 0.375, loss: 1.686,data_loss: 1.686, reg_loss: 0.000 lr: 0.000997\n",
      " step: 68, acc: 0.242, loss: 1.801,data_loss: 1.801, reg_loss: 0.000 lr: 0.000997\n",
      " step: 69, acc: 0.297, loss: 1.721,data_loss: 1.721, reg_loss: 0.000 lr: 0.000997\n",
      " step: 70, acc: 0.336, loss: 1.675,data_loss: 1.675, reg_loss: 0.000 lr: 0.000997\n",
      " step: 71, acc: 0.336, loss: 1.682,data_loss: 1.682, reg_loss: 0.000 lr: 0.000996\n",
      " step: 72, acc: 0.281, loss: 1.740,data_loss: 1.740, reg_loss: 0.000 lr: 0.000996\n",
      " step: 73, acc: 0.266, loss: 1.735,data_loss: 1.735, reg_loss: 0.000 lr: 0.000996\n",
      " step: 74, acc: 0.273, loss: 1.774,data_loss: 1.774, reg_loss: 0.000 lr: 0.000996\n",
      " step: 75, acc: 0.312, loss: 1.621,data_loss: 1.621, reg_loss: 0.000 lr: 0.000996\n",
      " step: 76, acc: 0.289, loss: 1.791,data_loss: 1.791, reg_loss: 0.000 lr: 0.000996\n",
      " step: 77, acc: 0.312, loss: 1.707,data_loss: 1.707, reg_loss: 0.000 lr: 0.000996\n",
      " step: 78, acc: 0.336, loss: 1.634,data_loss: 1.634, reg_loss: 0.000 lr: 0.000996\n",
      " step: 79, acc: 0.250, loss: 1.704,data_loss: 1.704, reg_loss: 0.000 lr: 0.000996\n",
      " step: 80, acc: 0.273, loss: 1.763,data_loss: 1.763, reg_loss: 0.000 lr: 0.000996\n",
      " step: 81, acc: 0.305, loss: 1.772,data_loss: 1.772, reg_loss: 0.000 lr: 0.000996\n",
      " step: 82, acc: 0.352, loss: 1.683,data_loss: 1.683, reg_loss: 0.000 lr: 0.000996\n",
      " step: 83, acc: 0.336, loss: 1.688,data_loss: 1.688, reg_loss: 0.000 lr: 0.000996\n",
      " step: 84, acc: 0.328, loss: 1.752,data_loss: 1.752, reg_loss: 0.000 lr: 0.000996\n",
      " step: 85, acc: 0.312, loss: 1.772,data_loss: 1.772, reg_loss: 0.000 lr: 0.000996\n",
      " step: 86, acc: 0.281, loss: 1.733,data_loss: 1.733, reg_loss: 0.000 lr: 0.000996\n",
      " step: 87, acc: 0.367, loss: 1.642,data_loss: 1.642, reg_loss: 0.000 lr: 0.000996\n",
      " step: 88, acc: 0.242, loss: 1.777,data_loss: 1.777, reg_loss: 0.000 lr: 0.000996\n",
      " step: 89, acc: 0.336, loss: 1.684,data_loss: 1.684, reg_loss: 0.000 lr: 0.000996\n",
      " step: 90, acc: 0.312, loss: 1.727,data_loss: 1.727, reg_loss: 0.000 lr: 0.000996\n",
      " step: 91, acc: 0.305, loss: 1.734,data_loss: 1.734, reg_loss: 0.000 lr: 0.000995\n",
      " step: 92, acc: 0.320, loss: 1.735,data_loss: 1.735, reg_loss: 0.000 lr: 0.000995\n",
      " step: 93, acc: 0.328, loss: 1.668,data_loss: 1.668, reg_loss: 0.000 lr: 0.000995\n",
      " step: 94, acc: 0.258, loss: 1.794,data_loss: 1.794, reg_loss: 0.000 lr: 0.000995\n",
      " step: 95, acc: 0.297, loss: 1.722,data_loss: 1.722, reg_loss: 0.000 lr: 0.000995\n",
      " step: 96, acc: 0.281, loss: 1.710,data_loss: 1.710, reg_loss: 0.000 lr: 0.000995\n",
      " step: 97, acc: 0.328, loss: 1.692,data_loss: 1.692, reg_loss: 0.000 lr: 0.000995\n",
      " step: 98, acc: 0.312, loss: 1.651,data_loss: 1.651, reg_loss: 0.000 lr: 0.000995\n",
      " step: 99, acc: 0.344, loss: 1.668,data_loss: 1.668, reg_loss: 0.000 lr: 0.000995\n",
      " step: 100, acc: 0.266, loss: 1.760,data_loss: 1.760, reg_loss: 0.000 lr: 0.000995\n",
      " step: 101, acc: 0.297, loss: 1.693,data_loss: 1.693, reg_loss: 0.000 lr: 0.000995\n",
      " step: 102, acc: 0.305, loss: 1.695,data_loss: 1.695, reg_loss: 0.000 lr: 0.000995\n",
      " step: 103, acc: 0.312, loss: 1.710,data_loss: 1.710, reg_loss: 0.000 lr: 0.000995\n",
      " step: 104, acc: 0.359, loss: 1.650,data_loss: 1.650, reg_loss: 0.000 lr: 0.000995\n",
      " step: 105, acc: 0.344, loss: 1.718,data_loss: 1.718, reg_loss: 0.000 lr: 0.000995\n",
      " step: 106, acc: 0.336, loss: 1.605,data_loss: 1.605, reg_loss: 0.000 lr: 0.000995\n",
      " step: 107, acc: 0.289, loss: 1.877,data_loss: 1.877, reg_loss: 0.000 lr: 0.000995\n",
      " step: 108, acc: 0.289, loss: 1.780,data_loss: 1.780, reg_loss: 0.000 lr: 0.000995\n",
      " step: 109, acc: 0.352, loss: 1.665,data_loss: 1.665, reg_loss: 0.000 lr: 0.000995\n",
      " step: 110, acc: 0.305, loss: 1.678,data_loss: 1.678, reg_loss: 0.000 lr: 0.000995\n",
      " step: 111, acc: 0.305, loss: 1.698,data_loss: 1.698, reg_loss: 0.000 lr: 0.000994\n",
      " step: 112, acc: 0.266, loss: 1.779,data_loss: 1.779, reg_loss: 0.000 lr: 0.000994\n",
      " step: 113, acc: 0.328, loss: 1.652,data_loss: 1.652, reg_loss: 0.000 lr: 0.000994\n",
      " step: 114, acc: 0.258, loss: 1.729,data_loss: 1.729, reg_loss: 0.000 lr: 0.000994\n",
      " step: 115, acc: 0.219, loss: 1.783,data_loss: 1.783, reg_loss: 0.000 lr: 0.000994\n",
      " step: 116, acc: 0.352, loss: 1.744,data_loss: 1.744, reg_loss: 0.000 lr: 0.000994\n",
      " step: 117, acc: 0.344, loss: 1.652,data_loss: 1.652, reg_loss: 0.000 lr: 0.000994\n",
      " step: 118, acc: 0.305, loss: 1.752,data_loss: 1.752, reg_loss: 0.000 lr: 0.000994\n",
      " step: 119, acc: 0.305, loss: 1.642,data_loss: 1.642, reg_loss: 0.000 lr: 0.000994\n",
      " step: 120, acc: 0.320, loss: 1.698,data_loss: 1.698, reg_loss: 0.000 lr: 0.000994\n",
      " step: 121, acc: 0.352, loss: 1.767,data_loss: 1.767, reg_loss: 0.000 lr: 0.000994\n",
      " step: 122, acc: 0.266, loss: 1.697,data_loss: 1.697, reg_loss: 0.000 lr: 0.000994\n",
      " step: 123, acc: 0.242, loss: 1.723,data_loss: 1.723, reg_loss: 0.000 lr: 0.000994\n",
      " step: 124, acc: 0.289, loss: 1.755,data_loss: 1.755, reg_loss: 0.000 lr: 0.000994\n",
      " step: 125, acc: 0.320, loss: 1.690,data_loss: 1.690, reg_loss: 0.000 lr: 0.000994\n",
      " step: 126, acc: 0.305, loss: 1.730,data_loss: 1.730, reg_loss: 0.000 lr: 0.000994\n",
      " step: 127, acc: 0.289, loss: 1.708,data_loss: 1.708, reg_loss: 0.000 lr: 0.000994\n",
      " step: 128, acc: 0.289, loss: 1.671,data_loss: 1.671, reg_loss: 0.000 lr: 0.000994\n",
      " step: 129, acc: 0.336, loss: 1.745,data_loss: 1.745, reg_loss: 0.000 lr: 0.000994\n",
      " step: 130, acc: 0.289, loss: 1.796,data_loss: 1.796, reg_loss: 0.000 lr: 0.000994\n",
      " step: 131, acc: 0.281, loss: 1.783,data_loss: 1.783, reg_loss: 0.000 lr: 0.000993\n",
      " step: 132, acc: 0.281, loss: 1.704,data_loss: 1.704, reg_loss: 0.000 lr: 0.000993\n",
      " step: 133, acc: 0.359, loss: 1.671,data_loss: 1.671, reg_loss: 0.000 lr: 0.000993\n",
      " step: 134, acc: 0.398, loss: 1.643,data_loss: 1.643, reg_loss: 0.000 lr: 0.000993\n",
      " step: 135, acc: 0.336, loss: 1.664,data_loss: 1.664, reg_loss: 0.000 lr: 0.000993\n",
      " step: 136, acc: 0.320, loss: 1.726,data_loss: 1.726, reg_loss: 0.000 lr: 0.000993\n",
      " step: 137, acc: 0.352, loss: 1.700,data_loss: 1.700, reg_loss: 0.000 lr: 0.000993\n",
      " step: 138, acc: 0.336, loss: 1.629,data_loss: 1.629, reg_loss: 0.000 lr: 0.000993\n",
      " step: 139, acc: 0.344, loss: 1.636,data_loss: 1.636, reg_loss: 0.000 lr: 0.000993\n",
      " step: 140, acc: 0.383, loss: 1.605,data_loss: 1.605, reg_loss: 0.000 lr: 0.000993\n",
      " step: 141, acc: 0.297, loss: 1.721,data_loss: 1.721, reg_loss: 0.000 lr: 0.000993\n",
      " step: 142, acc: 0.336, loss: 1.655,data_loss: 1.655, reg_loss: 0.000 lr: 0.000993\n",
      " step: 143, acc: 0.289, loss: 1.701,data_loss: 1.701, reg_loss: 0.000 lr: 0.000993\n",
      " step: 144, acc: 0.273, loss: 1.845,data_loss: 1.845, reg_loss: 0.000 lr: 0.000993\n",
      " step: 145, acc: 0.352, loss: 1.775,data_loss: 1.775, reg_loss: 0.000 lr: 0.000993\n",
      " step: 146, acc: 0.312, loss: 1.698,data_loss: 1.698, reg_loss: 0.000 lr: 0.000993\n",
      " step: 147, acc: 0.305, loss: 1.712,data_loss: 1.712, reg_loss: 0.000 lr: 0.000993\n",
      " step: 148, acc: 0.469, loss: 1.600,data_loss: 1.600, reg_loss: 0.000 lr: 0.000993\n",
      " step: 149, acc: 0.352, loss: 1.673,data_loss: 1.673, reg_loss: 0.000 lr: 0.000993\n",
      " step: 150, acc: 0.406, loss: 1.576,data_loss: 1.576, reg_loss: 0.000 lr: 0.000993\n",
      " step: 151, acc: 0.320, loss: 1.638,data_loss: 1.638, reg_loss: 0.000 lr: 0.000993\n",
      " step: 152, acc: 0.375, loss: 1.635,data_loss: 1.635, reg_loss: 0.000 lr: 0.000992\n",
      " step: 153, acc: 0.297, loss: 1.583,data_loss: 1.583, reg_loss: 0.000 lr: 0.000992\n",
      " step: 154, acc: 0.297, loss: 1.781,data_loss: 1.781, reg_loss: 0.000 lr: 0.000992\n",
      " step: 155, acc: 0.398, loss: 1.567,data_loss: 1.567, reg_loss: 0.000 lr: 0.000992\n",
      " step: 156, acc: 0.359, loss: 1.655,data_loss: 1.655, reg_loss: 0.000 lr: 0.000992\n",
      " step: 157, acc: 0.375, loss: 1.634,data_loss: 1.634, reg_loss: 0.000 lr: 0.000992\n",
      " step: 158, acc: 0.312, loss: 1.720,data_loss: 1.720, reg_loss: 0.000 lr: 0.000992\n",
      " step: 159, acc: 0.320, loss: 1.747,data_loss: 1.747, reg_loss: 0.000 lr: 0.000992\n",
      " step: 160, acc: 0.242, loss: 1.797,data_loss: 1.797, reg_loss: 0.000 lr: 0.000992\n",
      " step: 161, acc: 0.289, loss: 1.774,data_loss: 1.774, reg_loss: 0.000 lr: 0.000992\n",
      " step: 162, acc: 0.320, loss: 1.723,data_loss: 1.723, reg_loss: 0.000 lr: 0.000992\n",
      " step: 163, acc: 0.336, loss: 1.628,data_loss: 1.628, reg_loss: 0.000 lr: 0.000992\n",
      " step: 164, acc: 0.391, loss: 1.600,data_loss: 1.600, reg_loss: 0.000 lr: 0.000992\n",
      " step: 165, acc: 0.266, loss: 1.787,data_loss: 1.787, reg_loss: 0.000 lr: 0.000992\n",
      " step: 166, acc: 0.383, loss: 1.640,data_loss: 1.640, reg_loss: 0.000 lr: 0.000992\n",
      " step: 167, acc: 0.305, loss: 1.670,data_loss: 1.670, reg_loss: 0.000 lr: 0.000992\n",
      " step: 168, acc: 0.391, loss: 1.663,data_loss: 1.663, reg_loss: 0.000 lr: 0.000992\n",
      " step: 169, acc: 0.352, loss: 1.629,data_loss: 1.629, reg_loss: 0.000 lr: 0.000992\n",
      " step: 170, acc: 0.219, loss: 1.793,data_loss: 1.793, reg_loss: 0.000 lr: 0.000992\n",
      " step: 171, acc: 0.352, loss: 1.671,data_loss: 1.671, reg_loss: 0.000 lr: 0.000992\n",
      " step: 172, acc: 0.336, loss: 1.667,data_loss: 1.667, reg_loss: 0.000 lr: 0.000991\n",
      " step: 173, acc: 0.398, loss: 1.601,data_loss: 1.601, reg_loss: 0.000 lr: 0.000991\n",
      " step: 174, acc: 0.352, loss: 1.663,data_loss: 1.663, reg_loss: 0.000 lr: 0.000991\n",
      " step: 175, acc: 0.336, loss: 1.731,data_loss: 1.731, reg_loss: 0.000 lr: 0.000991\n",
      " step: 176, acc: 0.320, loss: 1.717,data_loss: 1.717, reg_loss: 0.000 lr: 0.000991\n",
      " step: 177, acc: 0.320, loss: 1.669,data_loss: 1.669, reg_loss: 0.000 lr: 0.000991\n",
      " step: 178, acc: 0.312, loss: 1.757,data_loss: 1.757, reg_loss: 0.000 lr: 0.000991\n",
      " step: 179, acc: 0.430, loss: 1.521,data_loss: 1.521, reg_loss: 0.000 lr: 0.000991\n",
      " step: 180, acc: 0.422, loss: 1.524,data_loss: 1.524, reg_loss: 0.000 lr: 0.000991\n",
      " step: 181, acc: 0.352, loss: 1.655,data_loss: 1.655, reg_loss: 0.000 lr: 0.000991\n",
      " step: 182, acc: 0.406, loss: 1.610,data_loss: 1.610, reg_loss: 0.000 lr: 0.000991\n",
      " step: 183, acc: 0.328, loss: 1.597,data_loss: 1.597, reg_loss: 0.000 lr: 0.000991\n",
      " step: 184, acc: 0.352, loss: 1.715,data_loss: 1.715, reg_loss: 0.000 lr: 0.000991\n",
      " step: 185, acc: 0.320, loss: 1.763,data_loss: 1.763, reg_loss: 0.000 lr: 0.000991\n",
      " step: 186, acc: 0.328, loss: 1.744,data_loss: 1.744, reg_loss: 0.000 lr: 0.000991\n",
      " step: 187, acc: 0.383, loss: 1.646,data_loss: 1.646, reg_loss: 0.000 lr: 0.000991\n",
      " step: 188, acc: 0.352, loss: 1.671,data_loss: 1.671, reg_loss: 0.000 lr: 0.000991\n",
      " step: 189, acc: 0.383, loss: 1.693,data_loss: 1.693, reg_loss: 0.000 lr: 0.000991\n",
      " step: 190, acc: 0.336, loss: 1.705,data_loss: 1.705, reg_loss: 0.000 lr: 0.000991\n",
      " step: 191, acc: 0.305, loss: 1.768,data_loss: 1.768, reg_loss: 0.000 lr: 0.000991\n",
      " step: 192, acc: 0.359, loss: 1.641,data_loss: 1.641, reg_loss: 0.000 lr: 0.000990\n",
      " step: 193, acc: 0.344, loss: 1.680,data_loss: 1.680, reg_loss: 0.000 lr: 0.000990\n",
      " step: 194, acc: 0.328, loss: 1.732,data_loss: 1.732, reg_loss: 0.000 lr: 0.000990\n",
      " step: 195, acc: 0.422, loss: 1.602,data_loss: 1.602, reg_loss: 0.000 lr: 0.000990\n",
      " step: 196, acc: 0.367, loss: 1.668,data_loss: 1.668, reg_loss: 0.000 lr: 0.000990\n",
      " step: 197, acc: 0.352, loss: 1.624,data_loss: 1.624, reg_loss: 0.000 lr: 0.000990\n",
      " step: 198, acc: 0.422, loss: 1.583,data_loss: 1.583, reg_loss: 0.000 lr: 0.000990\n",
      " step: 199, acc: 0.312, loss: 1.675,data_loss: 1.675, reg_loss: 0.000 lr: 0.000990\n",
      " step: 200, acc: 0.297, loss: 1.723,data_loss: 1.723, reg_loss: 0.000 lr: 0.000990\n",
      " step: 201, acc: 0.359, loss: 1.581,data_loss: 1.581, reg_loss: 0.000 lr: 0.000990\n",
      " step: 202, acc: 0.297, loss: 1.690,data_loss: 1.690, reg_loss: 0.000 lr: 0.000990\n",
      " step: 203, acc: 0.312, loss: 1.746,data_loss: 1.746, reg_loss: 0.000 lr: 0.000990\n",
      " step: 204, acc: 0.336, loss: 1.658,data_loss: 1.658, reg_loss: 0.000 lr: 0.000990\n",
      " step: 205, acc: 0.312, loss: 1.692,data_loss: 1.692, reg_loss: 0.000 lr: 0.000990\n",
      " step: 206, acc: 0.320, loss: 1.626,data_loss: 1.626, reg_loss: 0.000 lr: 0.000990\n",
      " step: 207, acc: 0.375, loss: 1.684,data_loss: 1.684, reg_loss: 0.000 lr: 0.000990\n",
      " step: 208, acc: 0.406, loss: 1.655,data_loss: 1.655, reg_loss: 0.000 lr: 0.000990\n",
      " step: 209, acc: 0.305, loss: 1.689,data_loss: 1.689, reg_loss: 0.000 lr: 0.000990\n",
      " step: 210, acc: 0.445, loss: 1.566,data_loss: 1.566, reg_loss: 0.000 lr: 0.000990\n",
      " step: 211, acc: 0.352, loss: 1.648,data_loss: 1.648, reg_loss: 0.000 lr: 0.000990\n",
      " step: 212, acc: 0.336, loss: 1.647,data_loss: 1.647, reg_loss: 0.000 lr: 0.000990\n",
      " step: 213, acc: 0.391, loss: 1.640,data_loss: 1.640, reg_loss: 0.000 lr: 0.000989\n",
      " step: 214, acc: 0.320, loss: 1.679,data_loss: 1.679, reg_loss: 0.000 lr: 0.000989\n",
      " step: 215, acc: 0.398, loss: 1.673,data_loss: 1.673, reg_loss: 0.000 lr: 0.000989\n",
      " step: 216, acc: 0.344, loss: 1.686,data_loss: 1.686, reg_loss: 0.000 lr: 0.000989\n",
      " step: 217, acc: 0.336, loss: 1.629,data_loss: 1.629, reg_loss: 0.000 lr: 0.000989\n",
      " step: 218, acc: 0.367, loss: 1.637,data_loss: 1.637, reg_loss: 0.000 lr: 0.000989\n",
      " step: 219, acc: 0.375, loss: 1.670,data_loss: 1.670, reg_loss: 0.000 lr: 0.000989\n",
      " step: 220, acc: 0.328, loss: 1.689,data_loss: 1.689, reg_loss: 0.000 lr: 0.000989\n",
      " step: 221, acc: 0.312, loss: 1.689,data_loss: 1.689, reg_loss: 0.000 lr: 0.000989\n",
      " step: 222, acc: 0.336, loss: 1.716,data_loss: 1.716, reg_loss: 0.000 lr: 0.000989\n",
      " step: 223, acc: 0.344, loss: 1.667,data_loss: 1.667, reg_loss: 0.000 lr: 0.000989\n",
      " step: 224, acc: 0.243, loss: 1.780,data_loss: 1.780, reg_loss: 0.000 lr: 0.000989\n",
      "training , acc: 0.307, loss: 1.725,data_loss: 1.725, reg_loss: 0.000 lr: 0.000989\n",
      "validation, acc:0.356 ,loss: 1.643 \n",
      "epoch: 2\n",
      " step: 0, acc: 0.234, loss: 1.783,data_loss: 1.783, reg_loss: 0.000 lr: 0.000989\n",
      " step: 1, acc: 0.336, loss: 1.685,data_loss: 1.685, reg_loss: 0.000 lr: 0.000989\n",
      " step: 2, acc: 0.312, loss: 1.668,data_loss: 1.668, reg_loss: 0.000 lr: 0.000989\n",
      " step: 3, acc: 0.398, loss: 1.645,data_loss: 1.645, reg_loss: 0.000 lr: 0.000989\n",
      " step: 4, acc: 0.406, loss: 1.586,data_loss: 1.586, reg_loss: 0.000 lr: 0.000989\n",
      " step: 5, acc: 0.336, loss: 1.652,data_loss: 1.652, reg_loss: 0.000 lr: 0.000989\n",
      " step: 6, acc: 0.398, loss: 1.640,data_loss: 1.640, reg_loss: 0.000 lr: 0.000989\n",
      " step: 7, acc: 0.266, loss: 1.733,data_loss: 1.733, reg_loss: 0.000 lr: 0.000989\n",
      " step: 8, acc: 0.359, loss: 1.636,data_loss: 1.636, reg_loss: 0.000 lr: 0.000988\n",
      " step: 9, acc: 0.289, loss: 1.684,data_loss: 1.684, reg_loss: 0.000 lr: 0.000988\n",
      " step: 10, acc: 0.344, loss: 1.747,data_loss: 1.747, reg_loss: 0.000 lr: 0.000988\n",
      " step: 11, acc: 0.359, loss: 1.578,data_loss: 1.578, reg_loss: 0.000 lr: 0.000988\n",
      " step: 12, acc: 0.391, loss: 1.621,data_loss: 1.621, reg_loss: 0.000 lr: 0.000988\n",
      " step: 13, acc: 0.312, loss: 1.674,data_loss: 1.674, reg_loss: 0.000 lr: 0.000988\n",
      " step: 14, acc: 0.328, loss: 1.656,data_loss: 1.656, reg_loss: 0.000 lr: 0.000988\n",
      " step: 15, acc: 0.344, loss: 1.642,data_loss: 1.642, reg_loss: 0.000 lr: 0.000988\n",
      " step: 16, acc: 0.328, loss: 1.647,data_loss: 1.647, reg_loss: 0.000 lr: 0.000988\n",
      " step: 17, acc: 0.352, loss: 1.623,data_loss: 1.623, reg_loss: 0.000 lr: 0.000988\n",
      " step: 18, acc: 0.273, loss: 1.726,data_loss: 1.726, reg_loss: 0.000 lr: 0.000988\n",
      " step: 19, acc: 0.414, loss: 1.602,data_loss: 1.602, reg_loss: 0.000 lr: 0.000988\n",
      " step: 20, acc: 0.297, loss: 1.657,data_loss: 1.657, reg_loss: 0.000 lr: 0.000988\n",
      " step: 21, acc: 0.344, loss: 1.647,data_loss: 1.647, reg_loss: 0.000 lr: 0.000988\n",
      " step: 22, acc: 0.352, loss: 1.576,data_loss: 1.576, reg_loss: 0.000 lr: 0.000988\n",
      " step: 23, acc: 0.383, loss: 1.620,data_loss: 1.620, reg_loss: 0.000 lr: 0.000988\n",
      " step: 24, acc: 0.328, loss: 1.603,data_loss: 1.603, reg_loss: 0.000 lr: 0.000988\n",
      " step: 25, acc: 0.367, loss: 1.570,data_loss: 1.570, reg_loss: 0.000 lr: 0.000988\n",
      " step: 26, acc: 0.359, loss: 1.662,data_loss: 1.662, reg_loss: 0.000 lr: 0.000988\n",
      " step: 27, acc: 0.430, loss: 1.633,data_loss: 1.633, reg_loss: 0.000 lr: 0.000988\n",
      " step: 28, acc: 0.281, loss: 1.793,data_loss: 1.793, reg_loss: 0.000 lr: 0.000988\n",
      " step: 29, acc: 0.367, loss: 1.693,data_loss: 1.693, reg_loss: 0.000 lr: 0.000987\n",
      " step: 30, acc: 0.234, loss: 1.834,data_loss: 1.834, reg_loss: 0.000 lr: 0.000987\n",
      " step: 31, acc: 0.359, loss: 1.622,data_loss: 1.622, reg_loss: 0.000 lr: 0.000987\n",
      " step: 32, acc: 0.383, loss: 1.658,data_loss: 1.658, reg_loss: 0.000 lr: 0.000987\n",
      " step: 33, acc: 0.320, loss: 1.730,data_loss: 1.730, reg_loss: 0.000 lr: 0.000987\n",
      " step: 34, acc: 0.352, loss: 1.680,data_loss: 1.680, reg_loss: 0.000 lr: 0.000987\n",
      " step: 35, acc: 0.375, loss: 1.613,data_loss: 1.613, reg_loss: 0.000 lr: 0.000987\n",
      " step: 36, acc: 0.383, loss: 1.555,data_loss: 1.555, reg_loss: 0.000 lr: 0.000987\n",
      " step: 37, acc: 0.320, loss: 1.689,data_loss: 1.689, reg_loss: 0.000 lr: 0.000987\n",
      " step: 38, acc: 0.328, loss: 1.664,data_loss: 1.664, reg_loss: 0.000 lr: 0.000987\n",
      " step: 39, acc: 0.312, loss: 1.626,data_loss: 1.626, reg_loss: 0.000 lr: 0.000987\n",
      " step: 40, acc: 0.398, loss: 1.680,data_loss: 1.680, reg_loss: 0.000 lr: 0.000987\n",
      " step: 41, acc: 0.312, loss: 1.750,data_loss: 1.750, reg_loss: 0.000 lr: 0.000987\n",
      " step: 42, acc: 0.352, loss: 1.758,data_loss: 1.758, reg_loss: 0.000 lr: 0.000987\n",
      " step: 43, acc: 0.398, loss: 1.621,data_loss: 1.621, reg_loss: 0.000 lr: 0.000987\n",
      " step: 44, acc: 0.367, loss: 1.641,data_loss: 1.641, reg_loss: 0.000 lr: 0.000987\n",
      " step: 45, acc: 0.352, loss: 1.601,data_loss: 1.601, reg_loss: 0.000 lr: 0.000987\n",
      " step: 46, acc: 0.406, loss: 1.557,data_loss: 1.557, reg_loss: 0.000 lr: 0.000987\n",
      " step: 47, acc: 0.359, loss: 1.677,data_loss: 1.677, reg_loss: 0.000 lr: 0.000987\n",
      " step: 48, acc: 0.391, loss: 1.569,data_loss: 1.569, reg_loss: 0.000 lr: 0.000987\n",
      " step: 49, acc: 0.391, loss: 1.554,data_loss: 1.554, reg_loss: 0.000 lr: 0.000986\n",
      " step: 50, acc: 0.406, loss: 1.593,data_loss: 1.593, reg_loss: 0.000 lr: 0.000986\n",
      " step: 51, acc: 0.281, loss: 1.711,data_loss: 1.711, reg_loss: 0.000 lr: 0.000986\n",
      " step: 52, acc: 0.344, loss: 1.732,data_loss: 1.732, reg_loss: 0.000 lr: 0.000986\n",
      " step: 53, acc: 0.352, loss: 1.648,data_loss: 1.648, reg_loss: 0.000 lr: 0.000986\n",
      " step: 54, acc: 0.320, loss: 1.756,data_loss: 1.756, reg_loss: 0.000 lr: 0.000986\n",
      " step: 55, acc: 0.359, loss: 1.623,data_loss: 1.623, reg_loss: 0.000 lr: 0.000986\n",
      " step: 56, acc: 0.336, loss: 1.627,data_loss: 1.627, reg_loss: 0.000 lr: 0.000986\n",
      " step: 57, acc: 0.406, loss: 1.622,data_loss: 1.622, reg_loss: 0.000 lr: 0.000986\n",
      " step: 58, acc: 0.328, loss: 1.645,data_loss: 1.645, reg_loss: 0.000 lr: 0.000986\n",
      " step: 59, acc: 0.398, loss: 1.567,data_loss: 1.567, reg_loss: 0.000 lr: 0.000986\n",
      " step: 60, acc: 0.320, loss: 1.692,data_loss: 1.692, reg_loss: 0.000 lr: 0.000986\n",
      " step: 61, acc: 0.328, loss: 1.670,data_loss: 1.670, reg_loss: 0.000 lr: 0.000986\n",
      " step: 62, acc: 0.398, loss: 1.588,data_loss: 1.588, reg_loss: 0.000 lr: 0.000986\n",
      " step: 63, acc: 0.352, loss: 1.620,data_loss: 1.620, reg_loss: 0.000 lr: 0.000986\n",
      " step: 64, acc: 0.375, loss: 1.569,data_loss: 1.569, reg_loss: 0.000 lr: 0.000986\n",
      " step: 65, acc: 0.320, loss: 1.684,data_loss: 1.684, reg_loss: 0.000 lr: 0.000986\n",
      " step: 66, acc: 0.391, loss: 1.619,data_loss: 1.619, reg_loss: 0.000 lr: 0.000986\n",
      " step: 67, acc: 0.398, loss: 1.585,data_loss: 1.585, reg_loss: 0.000 lr: 0.000986\n",
      " step: 68, acc: 0.344, loss: 1.675,data_loss: 1.675, reg_loss: 0.000 lr: 0.000986\n",
      " step: 69, acc: 0.336, loss: 1.647,data_loss: 1.647, reg_loss: 0.000 lr: 0.000986\n",
      " step: 70, acc: 0.383, loss: 1.574,data_loss: 1.574, reg_loss: 0.000 lr: 0.000985\n",
      " step: 71, acc: 0.445, loss: 1.555,data_loss: 1.555, reg_loss: 0.000 lr: 0.000985\n",
      " step: 72, acc: 0.320, loss: 1.608,data_loss: 1.608, reg_loss: 0.000 lr: 0.000985\n",
      " step: 73, acc: 0.328, loss: 1.601,data_loss: 1.601, reg_loss: 0.000 lr: 0.000985\n",
      " step: 74, acc: 0.336, loss: 1.653,data_loss: 1.653, reg_loss: 0.000 lr: 0.000985\n",
      " step: 75, acc: 0.359, loss: 1.541,data_loss: 1.541, reg_loss: 0.000 lr: 0.000985\n",
      " step: 76, acc: 0.352, loss: 1.632,data_loss: 1.632, reg_loss: 0.000 lr: 0.000985\n",
      " step: 77, acc: 0.367, loss: 1.608,data_loss: 1.608, reg_loss: 0.000 lr: 0.000985\n",
      " step: 78, acc: 0.406, loss: 1.550,data_loss: 1.550, reg_loss: 0.000 lr: 0.000985\n",
      " step: 79, acc: 0.375, loss: 1.586,data_loss: 1.586, reg_loss: 0.000 lr: 0.000985\n",
      " step: 80, acc: 0.336, loss: 1.621,data_loss: 1.621, reg_loss: 0.000 lr: 0.000985\n",
      " step: 81, acc: 0.320, loss: 1.737,data_loss: 1.737, reg_loss: 0.000 lr: 0.000985\n",
      " step: 82, acc: 0.453, loss: 1.557,data_loss: 1.557, reg_loss: 0.000 lr: 0.000985\n",
      " step: 83, acc: 0.398, loss: 1.615,data_loss: 1.615, reg_loss: 0.000 lr: 0.000985\n",
      " step: 84, acc: 0.297, loss: 1.690,data_loss: 1.690, reg_loss: 0.000 lr: 0.000985\n",
      " step: 85, acc: 0.383, loss: 1.619,data_loss: 1.619, reg_loss: 0.000 lr: 0.000985\n",
      " step: 86, acc: 0.344, loss: 1.613,data_loss: 1.613, reg_loss: 0.000 lr: 0.000985\n",
      " step: 87, acc: 0.391, loss: 1.579,data_loss: 1.579, reg_loss: 0.000 lr: 0.000985\n",
      " step: 88, acc: 0.289, loss: 1.645,data_loss: 1.645, reg_loss: 0.000 lr: 0.000985\n",
      " step: 89, acc: 0.430, loss: 1.598,data_loss: 1.598, reg_loss: 0.000 lr: 0.000985\n",
      " step: 90, acc: 0.344, loss: 1.610,data_loss: 1.610, reg_loss: 0.000 lr: 0.000984\n",
      " step: 91, acc: 0.352, loss: 1.642,data_loss: 1.642, reg_loss: 0.000 lr: 0.000984\n",
      " step: 92, acc: 0.336, loss: 1.684,data_loss: 1.684, reg_loss: 0.000 lr: 0.000984\n",
      " step: 93, acc: 0.422, loss: 1.565,data_loss: 1.565, reg_loss: 0.000 lr: 0.000984\n",
      " step: 94, acc: 0.289, loss: 1.699,data_loss: 1.699, reg_loss: 0.000 lr: 0.000984\n",
      " step: 95, acc: 0.383, loss: 1.662,data_loss: 1.662, reg_loss: 0.000 lr: 0.000984\n",
      " step: 96, acc: 0.359, loss: 1.613,data_loss: 1.613, reg_loss: 0.000 lr: 0.000984\n",
      " step: 97, acc: 0.383, loss: 1.632,data_loss: 1.632, reg_loss: 0.000 lr: 0.000984\n",
      " step: 98, acc: 0.375, loss: 1.606,data_loss: 1.606, reg_loss: 0.000 lr: 0.000984\n",
      " step: 99, acc: 0.398, loss: 1.523,data_loss: 1.523, reg_loss: 0.000 lr: 0.000984\n",
      " step: 100, acc: 0.320, loss: 1.673,data_loss: 1.673, reg_loss: 0.000 lr: 0.000984\n",
      " step: 101, acc: 0.359, loss: 1.615,data_loss: 1.615, reg_loss: 0.000 lr: 0.000984\n",
      " step: 102, acc: 0.336, loss: 1.613,data_loss: 1.613, reg_loss: 0.000 lr: 0.000984\n",
      " step: 103, acc: 0.320, loss: 1.637,data_loss: 1.637, reg_loss: 0.000 lr: 0.000984\n",
      " step: 104, acc: 0.422, loss: 1.530,data_loss: 1.530, reg_loss: 0.000 lr: 0.000984\n",
      " step: 105, acc: 0.312, loss: 1.671,data_loss: 1.671, reg_loss: 0.000 lr: 0.000984\n",
      " step: 106, acc: 0.328, loss: 1.541,data_loss: 1.541, reg_loss: 0.000 lr: 0.000984\n",
      " step: 107, acc: 0.328, loss: 1.753,data_loss: 1.753, reg_loss: 0.000 lr: 0.000984\n",
      " step: 108, acc: 0.312, loss: 1.718,data_loss: 1.718, reg_loss: 0.000 lr: 0.000984\n",
      " step: 109, acc: 0.352, loss: 1.648,data_loss: 1.648, reg_loss: 0.000 lr: 0.000984\n",
      " step: 110, acc: 0.406, loss: 1.573,data_loss: 1.573, reg_loss: 0.000 lr: 0.000984\n",
      " step: 111, acc: 0.359, loss: 1.610,data_loss: 1.610, reg_loss: 0.000 lr: 0.000983\n",
      " step: 112, acc: 0.312, loss: 1.660,data_loss: 1.660, reg_loss: 0.000 lr: 0.000983\n",
      " step: 113, acc: 0.383, loss: 1.591,data_loss: 1.591, reg_loss: 0.000 lr: 0.000983\n",
      " step: 114, acc: 0.344, loss: 1.641,data_loss: 1.641, reg_loss: 0.000 lr: 0.000983\n",
      " step: 115, acc: 0.297, loss: 1.658,data_loss: 1.658, reg_loss: 0.000 lr: 0.000983\n",
      " step: 116, acc: 0.359, loss: 1.671,data_loss: 1.671, reg_loss: 0.000 lr: 0.000983\n",
      " step: 117, acc: 0.367, loss: 1.595,data_loss: 1.595, reg_loss: 0.000 lr: 0.000983\n",
      " step: 118, acc: 0.367, loss: 1.673,data_loss: 1.673, reg_loss: 0.000 lr: 0.000983\n",
      " step: 119, acc: 0.336, loss: 1.604,data_loss: 1.604, reg_loss: 0.000 lr: 0.000983\n",
      " step: 120, acc: 0.367, loss: 1.687,data_loss: 1.687, reg_loss: 0.000 lr: 0.000983\n",
      " step: 121, acc: 0.336, loss: 1.716,data_loss: 1.716, reg_loss: 0.000 lr: 0.000983\n",
      " step: 122, acc: 0.359, loss: 1.615,data_loss: 1.615, reg_loss: 0.000 lr: 0.000983\n",
      " step: 123, acc: 0.320, loss: 1.679,data_loss: 1.679, reg_loss: 0.000 lr: 0.000983\n",
      " step: 124, acc: 0.320, loss: 1.715,data_loss: 1.715, reg_loss: 0.000 lr: 0.000983\n",
      " step: 125, acc: 0.297, loss: 1.609,data_loss: 1.609, reg_loss: 0.000 lr: 0.000983\n",
      " step: 126, acc: 0.328, loss: 1.659,data_loss: 1.659, reg_loss: 0.000 lr: 0.000983\n",
      " step: 127, acc: 0.281, loss: 1.640,data_loss: 1.640, reg_loss: 0.000 lr: 0.000983\n",
      " step: 128, acc: 0.344, loss: 1.636,data_loss: 1.636, reg_loss: 0.000 lr: 0.000983\n",
      " step: 129, acc: 0.367, loss: 1.623,data_loss: 1.623, reg_loss: 0.000 lr: 0.000983\n",
      " step: 130, acc: 0.250, loss: 1.806,data_loss: 1.806, reg_loss: 0.000 lr: 0.000983\n",
      " step: 131, acc: 0.375, loss: 1.662,data_loss: 1.662, reg_loss: 0.000 lr: 0.000983\n",
      " step: 132, acc: 0.344, loss: 1.600,data_loss: 1.600, reg_loss: 0.000 lr: 0.000982\n",
      " step: 133, acc: 0.359, loss: 1.634,data_loss: 1.634, reg_loss: 0.000 lr: 0.000982\n",
      " step: 134, acc: 0.383, loss: 1.473,data_loss: 1.473, reg_loss: 0.000 lr: 0.000982\n",
      " step: 135, acc: 0.375, loss: 1.591,data_loss: 1.591, reg_loss: 0.000 lr: 0.000982\n",
      " step: 136, acc: 0.344, loss: 1.709,data_loss: 1.709, reg_loss: 0.000 lr: 0.000982\n",
      " step: 137, acc: 0.375, loss: 1.637,data_loss: 1.637, reg_loss: 0.000 lr: 0.000982\n",
      " step: 138, acc: 0.422, loss: 1.502,data_loss: 1.502, reg_loss: 0.000 lr: 0.000982\n",
      " step: 139, acc: 0.406, loss: 1.516,data_loss: 1.516, reg_loss: 0.000 lr: 0.000982\n",
      " step: 140, acc: 0.430, loss: 1.493,data_loss: 1.493, reg_loss: 0.000 lr: 0.000982\n",
      " step: 141, acc: 0.328, loss: 1.593,data_loss: 1.593, reg_loss: 0.000 lr: 0.000982\n",
      " step: 142, acc: 0.367, loss: 1.592,data_loss: 1.592, reg_loss: 0.000 lr: 0.000982\n",
      " step: 143, acc: 0.344, loss: 1.650,data_loss: 1.650, reg_loss: 0.000 lr: 0.000982\n",
      " step: 144, acc: 0.320, loss: 1.757,data_loss: 1.757, reg_loss: 0.000 lr: 0.000982\n",
      " step: 145, acc: 0.406, loss: 1.673,data_loss: 1.673, reg_loss: 0.000 lr: 0.000982\n",
      " step: 146, acc: 0.344, loss: 1.634,data_loss: 1.634, reg_loss: 0.000 lr: 0.000982\n",
      " step: 147, acc: 0.305, loss: 1.636,data_loss: 1.636, reg_loss: 0.000 lr: 0.000982\n",
      " step: 148, acc: 0.430, loss: 1.550,data_loss: 1.550, reg_loss: 0.000 lr: 0.000982\n",
      " step: 149, acc: 0.344, loss: 1.639,data_loss: 1.639, reg_loss: 0.000 lr: 0.000982\n",
      " step: 150, acc: 0.453, loss: 1.478,data_loss: 1.478, reg_loss: 0.000 lr: 0.000982\n",
      " step: 151, acc: 0.344, loss: 1.596,data_loss: 1.596, reg_loss: 0.000 lr: 0.000982\n",
      " step: 152, acc: 0.484, loss: 1.580,data_loss: 1.580, reg_loss: 0.000 lr: 0.000981\n",
      " step: 153, acc: 0.359, loss: 1.534,data_loss: 1.534, reg_loss: 0.000 lr: 0.000981\n",
      " step: 154, acc: 0.297, loss: 1.704,data_loss: 1.704, reg_loss: 0.000 lr: 0.000981\n",
      " step: 155, acc: 0.445, loss: 1.529,data_loss: 1.529, reg_loss: 0.000 lr: 0.000981\n",
      " step: 156, acc: 0.414, loss: 1.565,data_loss: 1.565, reg_loss: 0.000 lr: 0.000981\n",
      " step: 157, acc: 0.430, loss: 1.535,data_loss: 1.535, reg_loss: 0.000 lr: 0.000981\n",
      " step: 158, acc: 0.383, loss: 1.621,data_loss: 1.621, reg_loss: 0.000 lr: 0.000981\n",
      " step: 159, acc: 0.359, loss: 1.697,data_loss: 1.697, reg_loss: 0.000 lr: 0.000981\n",
      " step: 160, acc: 0.359, loss: 1.742,data_loss: 1.742, reg_loss: 0.000 lr: 0.000981\n",
      " step: 161, acc: 0.375, loss: 1.698,data_loss: 1.698, reg_loss: 0.000 lr: 0.000981\n",
      " step: 162, acc: 0.352, loss: 1.664,data_loss: 1.664, reg_loss: 0.000 lr: 0.000981\n",
      " step: 163, acc: 0.344, loss: 1.547,data_loss: 1.547, reg_loss: 0.000 lr: 0.000981\n",
      " step: 164, acc: 0.484, loss: 1.515,data_loss: 1.515, reg_loss: 0.000 lr: 0.000981\n",
      " step: 165, acc: 0.273, loss: 1.787,data_loss: 1.787, reg_loss: 0.000 lr: 0.000981\n",
      " step: 166, acc: 0.375, loss: 1.619,data_loss: 1.619, reg_loss: 0.000 lr: 0.000981\n",
      " step: 167, acc: 0.312, loss: 1.627,data_loss: 1.627, reg_loss: 0.000 lr: 0.000981\n",
      " step: 168, acc: 0.398, loss: 1.560,data_loss: 1.560, reg_loss: 0.000 lr: 0.000981\n",
      " step: 169, acc: 0.367, loss: 1.546,data_loss: 1.546, reg_loss: 0.000 lr: 0.000981\n",
      " step: 170, acc: 0.281, loss: 1.735,data_loss: 1.735, reg_loss: 0.000 lr: 0.000981\n",
      " step: 171, acc: 0.406, loss: 1.576,data_loss: 1.576, reg_loss: 0.000 lr: 0.000981\n",
      " step: 172, acc: 0.391, loss: 1.603,data_loss: 1.603, reg_loss: 0.000 lr: 0.000981\n",
      " step: 173, acc: 0.406, loss: 1.585,data_loss: 1.585, reg_loss: 0.000 lr: 0.000980\n",
      " step: 174, acc: 0.406, loss: 1.566,data_loss: 1.566, reg_loss: 0.000 lr: 0.000980\n",
      " step: 175, acc: 0.336, loss: 1.671,data_loss: 1.671, reg_loss: 0.000 lr: 0.000980\n",
      " step: 176, acc: 0.312, loss: 1.737,data_loss: 1.737, reg_loss: 0.000 lr: 0.000980\n",
      " step: 177, acc: 0.344, loss: 1.571,data_loss: 1.571, reg_loss: 0.000 lr: 0.000980\n",
      " step: 178, acc: 0.383, loss: 1.700,data_loss: 1.700, reg_loss: 0.000 lr: 0.000980\n",
      " step: 179, acc: 0.469, loss: 1.459,data_loss: 1.459, reg_loss: 0.000 lr: 0.000980\n",
      " step: 180, acc: 0.430, loss: 1.484,data_loss: 1.484, reg_loss: 0.000 lr: 0.000980\n",
      " step: 181, acc: 0.383, loss: 1.605,data_loss: 1.605, reg_loss: 0.000 lr: 0.000980\n",
      " step: 182, acc: 0.438, loss: 1.530,data_loss: 1.530, reg_loss: 0.000 lr: 0.000980\n",
      " step: 183, acc: 0.383, loss: 1.545,data_loss: 1.545, reg_loss: 0.000 lr: 0.000980\n",
      " step: 184, acc: 0.352, loss: 1.614,data_loss: 1.614, reg_loss: 0.000 lr: 0.000980\n",
      " step: 185, acc: 0.328, loss: 1.721,data_loss: 1.721, reg_loss: 0.000 lr: 0.000980\n",
      " step: 186, acc: 0.352, loss: 1.659,data_loss: 1.659, reg_loss: 0.000 lr: 0.000980\n",
      " step: 187, acc: 0.367, loss: 1.581,data_loss: 1.581, reg_loss: 0.000 lr: 0.000980\n",
      " step: 188, acc: 0.367, loss: 1.602,data_loss: 1.602, reg_loss: 0.000 lr: 0.000980\n",
      " step: 189, acc: 0.414, loss: 1.628,data_loss: 1.628, reg_loss: 0.000 lr: 0.000980\n",
      " step: 190, acc: 0.375, loss: 1.631,data_loss: 1.631, reg_loss: 0.000 lr: 0.000980\n",
      " step: 191, acc: 0.297, loss: 1.683,data_loss: 1.683, reg_loss: 0.000 lr: 0.000980\n",
      " step: 192, acc: 0.367, loss: 1.589,data_loss: 1.589, reg_loss: 0.000 lr: 0.000980\n",
      " step: 193, acc: 0.383, loss: 1.628,data_loss: 1.628, reg_loss: 0.000 lr: 0.000980\n",
      " step: 194, acc: 0.367, loss: 1.631,data_loss: 1.631, reg_loss: 0.000 lr: 0.000979\n",
      " step: 195, acc: 0.406, loss: 1.531,data_loss: 1.531, reg_loss: 0.000 lr: 0.000979\n",
      " step: 196, acc: 0.344, loss: 1.616,data_loss: 1.616, reg_loss: 0.000 lr: 0.000979\n",
      " step: 197, acc: 0.398, loss: 1.546,data_loss: 1.546, reg_loss: 0.000 lr: 0.000979\n",
      " step: 198, acc: 0.453, loss: 1.475,data_loss: 1.475, reg_loss: 0.000 lr: 0.000979\n",
      " step: 199, acc: 0.359, loss: 1.622,data_loss: 1.622, reg_loss: 0.000 lr: 0.000979\n",
      " step: 200, acc: 0.352, loss: 1.660,data_loss: 1.660, reg_loss: 0.000 lr: 0.000979\n",
      " step: 201, acc: 0.391, loss: 1.593,data_loss: 1.593, reg_loss: 0.000 lr: 0.000979\n",
      " step: 202, acc: 0.352, loss: 1.631,data_loss: 1.631, reg_loss: 0.000 lr: 0.000979\n",
      " step: 203, acc: 0.344, loss: 1.709,data_loss: 1.709, reg_loss: 0.000 lr: 0.000979\n",
      " step: 204, acc: 0.336, loss: 1.576,data_loss: 1.576, reg_loss: 0.000 lr: 0.000979\n",
      " step: 205, acc: 0.375, loss: 1.632,data_loss: 1.632, reg_loss: 0.000 lr: 0.000979\n",
      " step: 206, acc: 0.367, loss: 1.502,data_loss: 1.502, reg_loss: 0.000 lr: 0.000979\n",
      " step: 207, acc: 0.414, loss: 1.550,data_loss: 1.550, reg_loss: 0.000 lr: 0.000979\n",
      " step: 208, acc: 0.438, loss: 1.581,data_loss: 1.581, reg_loss: 0.000 lr: 0.000979\n",
      " step: 209, acc: 0.320, loss: 1.675,data_loss: 1.675, reg_loss: 0.000 lr: 0.000979\n",
      " step: 210, acc: 0.406, loss: 1.508,data_loss: 1.508, reg_loss: 0.000 lr: 0.000979\n",
      " step: 211, acc: 0.336, loss: 1.576,data_loss: 1.576, reg_loss: 0.000 lr: 0.000979\n",
      " step: 212, acc: 0.359, loss: 1.595,data_loss: 1.595, reg_loss: 0.000 lr: 0.000979\n",
      " step: 213, acc: 0.352, loss: 1.612,data_loss: 1.612, reg_loss: 0.000 lr: 0.000979\n",
      " step: 214, acc: 0.375, loss: 1.591,data_loss: 1.591, reg_loss: 0.000 lr: 0.000979\n",
      " step: 215, acc: 0.406, loss: 1.581,data_loss: 1.581, reg_loss: 0.000 lr: 0.000978\n",
      " step: 216, acc: 0.367, loss: 1.595,data_loss: 1.595, reg_loss: 0.000 lr: 0.000978\n",
      " step: 217, acc: 0.336, loss: 1.593,data_loss: 1.593, reg_loss: 0.000 lr: 0.000978\n",
      " step: 218, acc: 0.398, loss: 1.574,data_loss: 1.574, reg_loss: 0.000 lr: 0.000978\n",
      " step: 219, acc: 0.367, loss: 1.593,data_loss: 1.593, reg_loss: 0.000 lr: 0.000978\n",
      " step: 220, acc: 0.359, loss: 1.645,data_loss: 1.645, reg_loss: 0.000 lr: 0.000978\n",
      " step: 221, acc: 0.328, loss: 1.652,data_loss: 1.652, reg_loss: 0.000 lr: 0.000978\n",
      " step: 222, acc: 0.336, loss: 1.662,data_loss: 1.662, reg_loss: 0.000 lr: 0.000978\n",
      " step: 223, acc: 0.305, loss: 1.626,data_loss: 1.626, reg_loss: 0.000 lr: 0.000978\n",
      " step: 224, acc: 0.216, loss: 1.788,data_loss: 1.788, reg_loss: 0.000 lr: 0.000978\n",
      "training , acc: 0.359, loss: 1.627,data_loss: 1.627, reg_loss: 0.000 lr: 0.000978\n",
      "validation, acc:0.378 ,loss: 1.599 \n",
      "epoch: 3\n",
      " step: 0, acc: 0.281, loss: 1.759,data_loss: 1.759, reg_loss: 0.000 lr: 0.000978\n",
      " step: 1, acc: 0.383, loss: 1.640,data_loss: 1.640, reg_loss: 0.000 lr: 0.000978\n",
      " step: 2, acc: 0.375, loss: 1.575,data_loss: 1.575, reg_loss: 0.000 lr: 0.000978\n",
      " step: 3, acc: 0.430, loss: 1.624,data_loss: 1.624, reg_loss: 0.000 lr: 0.000978\n",
      " step: 4, acc: 0.422, loss: 1.524,data_loss: 1.524, reg_loss: 0.000 lr: 0.000978\n",
      " step: 5, acc: 0.383, loss: 1.644,data_loss: 1.644, reg_loss: 0.000 lr: 0.000978\n",
      " step: 6, acc: 0.352, loss: 1.584,data_loss: 1.584, reg_loss: 0.000 lr: 0.000978\n",
      " step: 7, acc: 0.297, loss: 1.694,data_loss: 1.694, reg_loss: 0.000 lr: 0.000978\n",
      " step: 8, acc: 0.383, loss: 1.573,data_loss: 1.573, reg_loss: 0.000 lr: 0.000978\n",
      " step: 9, acc: 0.320, loss: 1.629,data_loss: 1.629, reg_loss: 0.000 lr: 0.000978\n",
      " step: 10, acc: 0.406, loss: 1.634,data_loss: 1.634, reg_loss: 0.000 lr: 0.000978\n",
      " step: 11, acc: 0.477, loss: 1.473,data_loss: 1.473, reg_loss: 0.000 lr: 0.000977\n",
      " step: 12, acc: 0.383, loss: 1.540,data_loss: 1.540, reg_loss: 0.000 lr: 0.000977\n",
      " step: 13, acc: 0.398, loss: 1.597,data_loss: 1.597, reg_loss: 0.000 lr: 0.000977\n",
      " step: 14, acc: 0.297, loss: 1.604,data_loss: 1.604, reg_loss: 0.000 lr: 0.000977\n",
      " step: 15, acc: 0.344, loss: 1.614,data_loss: 1.614, reg_loss: 0.000 lr: 0.000977\n",
      " step: 16, acc: 0.367, loss: 1.571,data_loss: 1.571, reg_loss: 0.000 lr: 0.000977\n",
      " step: 17, acc: 0.375, loss: 1.607,data_loss: 1.607, reg_loss: 0.000 lr: 0.000977\n",
      " step: 18, acc: 0.336, loss: 1.667,data_loss: 1.667, reg_loss: 0.000 lr: 0.000977\n",
      " step: 19, acc: 0.406, loss: 1.518,data_loss: 1.518, reg_loss: 0.000 lr: 0.000977\n",
      " step: 20, acc: 0.328, loss: 1.646,data_loss: 1.646, reg_loss: 0.000 lr: 0.000977\n",
      " step: 21, acc: 0.383, loss: 1.556,data_loss: 1.556, reg_loss: 0.000 lr: 0.000977\n",
      " step: 22, acc: 0.414, loss: 1.535,data_loss: 1.535, reg_loss: 0.000 lr: 0.000977\n",
      " step: 23, acc: 0.445, loss: 1.551,data_loss: 1.551, reg_loss: 0.000 lr: 0.000977\n",
      " step: 24, acc: 0.367, loss: 1.528,data_loss: 1.528, reg_loss: 0.000 lr: 0.000977\n",
      " step: 25, acc: 0.422, loss: 1.484,data_loss: 1.484, reg_loss: 0.000 lr: 0.000977\n",
      " step: 26, acc: 0.383, loss: 1.607,data_loss: 1.607, reg_loss: 0.000 lr: 0.000977\n",
      " step: 27, acc: 0.438, loss: 1.577,data_loss: 1.577, reg_loss: 0.000 lr: 0.000977\n",
      " step: 28, acc: 0.328, loss: 1.743,data_loss: 1.743, reg_loss: 0.000 lr: 0.000977\n",
      " step: 29, acc: 0.367, loss: 1.573,data_loss: 1.573, reg_loss: 0.000 lr: 0.000977\n",
      " step: 30, acc: 0.266, loss: 1.801,data_loss: 1.801, reg_loss: 0.000 lr: 0.000977\n",
      " step: 31, acc: 0.375, loss: 1.571,data_loss: 1.571, reg_loss: 0.000 lr: 0.000977\n",
      " step: 32, acc: 0.406, loss: 1.597,data_loss: 1.597, reg_loss: 0.000 lr: 0.000976\n",
      " step: 33, acc: 0.352, loss: 1.629,data_loss: 1.629, reg_loss: 0.000 lr: 0.000976\n",
      " step: 34, acc: 0.336, loss: 1.692,data_loss: 1.692, reg_loss: 0.000 lr: 0.000976\n",
      " step: 35, acc: 0.383, loss: 1.582,data_loss: 1.582, reg_loss: 0.000 lr: 0.000976\n",
      " step: 36, acc: 0.430, loss: 1.483,data_loss: 1.483, reg_loss: 0.000 lr: 0.000976\n",
      " step: 37, acc: 0.328, loss: 1.682,data_loss: 1.682, reg_loss: 0.000 lr: 0.000976\n",
      " step: 38, acc: 0.312, loss: 1.638,data_loss: 1.638, reg_loss: 0.000 lr: 0.000976\n",
      " step: 39, acc: 0.328, loss: 1.595,data_loss: 1.595, reg_loss: 0.000 lr: 0.000976\n",
      " step: 40, acc: 0.352, loss: 1.651,data_loss: 1.651, reg_loss: 0.000 lr: 0.000976\n",
      " step: 41, acc: 0.328, loss: 1.683,data_loss: 1.683, reg_loss: 0.000 lr: 0.000976\n",
      " step: 42, acc: 0.391, loss: 1.686,data_loss: 1.686, reg_loss: 0.000 lr: 0.000976\n",
      " step: 43, acc: 0.398, loss: 1.545,data_loss: 1.545, reg_loss: 0.000 lr: 0.000976\n",
      " step: 44, acc: 0.359, loss: 1.625,data_loss: 1.625, reg_loss: 0.000 lr: 0.000976\n",
      " step: 45, acc: 0.359, loss: 1.551,data_loss: 1.551, reg_loss: 0.000 lr: 0.000976\n",
      " step: 46, acc: 0.375, loss: 1.564,data_loss: 1.564, reg_loss: 0.000 lr: 0.000976\n",
      " step: 47, acc: 0.352, loss: 1.627,data_loss: 1.627, reg_loss: 0.000 lr: 0.000976\n",
      " step: 48, acc: 0.469, loss: 1.474,data_loss: 1.474, reg_loss: 0.000 lr: 0.000976\n",
      " step: 49, acc: 0.398, loss: 1.557,data_loss: 1.557, reg_loss: 0.000 lr: 0.000976\n",
      " step: 50, acc: 0.398, loss: 1.507,data_loss: 1.507, reg_loss: 0.000 lr: 0.000976\n",
      " step: 51, acc: 0.297, loss: 1.633,data_loss: 1.633, reg_loss: 0.000 lr: 0.000976\n",
      " step: 52, acc: 0.375, loss: 1.677,data_loss: 1.677, reg_loss: 0.000 lr: 0.000976\n",
      " step: 53, acc: 0.391, loss: 1.616,data_loss: 1.616, reg_loss: 0.000 lr: 0.000975\n",
      " step: 54, acc: 0.352, loss: 1.676,data_loss: 1.676, reg_loss: 0.000 lr: 0.000975\n",
      " step: 55, acc: 0.320, loss: 1.622,data_loss: 1.622, reg_loss: 0.000 lr: 0.000975\n",
      " step: 56, acc: 0.375, loss: 1.587,data_loss: 1.587, reg_loss: 0.000 lr: 0.000975\n",
      " step: 57, acc: 0.430, loss: 1.577,data_loss: 1.577, reg_loss: 0.000 lr: 0.000975\n",
      " step: 58, acc: 0.305, loss: 1.598,data_loss: 1.598, reg_loss: 0.000 lr: 0.000975\n",
      " step: 59, acc: 0.445, loss: 1.533,data_loss: 1.533, reg_loss: 0.000 lr: 0.000975\n",
      " step: 60, acc: 0.398, loss: 1.589,data_loss: 1.589, reg_loss: 0.000 lr: 0.000975\n",
      " step: 61, acc: 0.312, loss: 1.585,data_loss: 1.585, reg_loss: 0.000 lr: 0.000975\n",
      " step: 62, acc: 0.414, loss: 1.542,data_loss: 1.542, reg_loss: 0.000 lr: 0.000975\n",
      " step: 63, acc: 0.375, loss: 1.573,data_loss: 1.573, reg_loss: 0.000 lr: 0.000975\n",
      " step: 64, acc: 0.367, loss: 1.572,data_loss: 1.572, reg_loss: 0.000 lr: 0.000975\n",
      " step: 65, acc: 0.359, loss: 1.576,data_loss: 1.576, reg_loss: 0.000 lr: 0.000975\n",
      " step: 66, acc: 0.398, loss: 1.535,data_loss: 1.535, reg_loss: 0.000 lr: 0.000975\n",
      " step: 67, acc: 0.383, loss: 1.561,data_loss: 1.561, reg_loss: 0.000 lr: 0.000975\n",
      " step: 68, acc: 0.352, loss: 1.618,data_loss: 1.618, reg_loss: 0.000 lr: 0.000975\n",
      " step: 69, acc: 0.297, loss: 1.605,data_loss: 1.605, reg_loss: 0.000 lr: 0.000975\n",
      " step: 70, acc: 0.406, loss: 1.517,data_loss: 1.517, reg_loss: 0.000 lr: 0.000975\n",
      " step: 71, acc: 0.508, loss: 1.484,data_loss: 1.484, reg_loss: 0.000 lr: 0.000975\n",
      " step: 72, acc: 0.336, loss: 1.538,data_loss: 1.538, reg_loss: 0.000 lr: 0.000975\n",
      " step: 73, acc: 0.398, loss: 1.504,data_loss: 1.504, reg_loss: 0.000 lr: 0.000975\n",
      " step: 74, acc: 0.352, loss: 1.573,data_loss: 1.573, reg_loss: 0.000 lr: 0.000974\n",
      " step: 75, acc: 0.438, loss: 1.461,data_loss: 1.461, reg_loss: 0.000 lr: 0.000974\n",
      " step: 76, acc: 0.414, loss: 1.574,data_loss: 1.574, reg_loss: 0.000 lr: 0.000974\n",
      " step: 77, acc: 0.367, loss: 1.588,data_loss: 1.588, reg_loss: 0.000 lr: 0.000974\n",
      " step: 78, acc: 0.367, loss: 1.482,data_loss: 1.482, reg_loss: 0.000 lr: 0.000974\n",
      " step: 79, acc: 0.352, loss: 1.559,data_loss: 1.559, reg_loss: 0.000 lr: 0.000974\n",
      " step: 80, acc: 0.359, loss: 1.536,data_loss: 1.536, reg_loss: 0.000 lr: 0.000974\n",
      " step: 81, acc: 0.312, loss: 1.686,data_loss: 1.686, reg_loss: 0.000 lr: 0.000974\n",
      " step: 82, acc: 0.469, loss: 1.576,data_loss: 1.576, reg_loss: 0.000 lr: 0.000974\n",
      " step: 83, acc: 0.336, loss: 1.607,data_loss: 1.607, reg_loss: 0.000 lr: 0.000974\n",
      " step: 84, acc: 0.352, loss: 1.622,data_loss: 1.622, reg_loss: 0.000 lr: 0.000974\n",
      " step: 85, acc: 0.461, loss: 1.556,data_loss: 1.556, reg_loss: 0.000 lr: 0.000974\n",
      " step: 86, acc: 0.328, loss: 1.570,data_loss: 1.570, reg_loss: 0.000 lr: 0.000974\n",
      " step: 87, acc: 0.383, loss: 1.552,data_loss: 1.552, reg_loss: 0.000 lr: 0.000974\n",
      " step: 88, acc: 0.367, loss: 1.590,data_loss: 1.590, reg_loss: 0.000 lr: 0.000974\n",
      " step: 89, acc: 0.438, loss: 1.548,data_loss: 1.548, reg_loss: 0.000 lr: 0.000974\n",
      " step: 90, acc: 0.367, loss: 1.585,data_loss: 1.585, reg_loss: 0.000 lr: 0.000974\n",
      " step: 91, acc: 0.352, loss: 1.607,data_loss: 1.607, reg_loss: 0.000 lr: 0.000974\n",
      " step: 92, acc: 0.344, loss: 1.609,data_loss: 1.609, reg_loss: 0.000 lr: 0.000974\n",
      " step: 93, acc: 0.398, loss: 1.484,data_loss: 1.484, reg_loss: 0.000 lr: 0.000974\n",
      " step: 94, acc: 0.289, loss: 1.650,data_loss: 1.650, reg_loss: 0.000 lr: 0.000974\n",
      " step: 95, acc: 0.398, loss: 1.558,data_loss: 1.558, reg_loss: 0.000 lr: 0.000973\n",
      " step: 96, acc: 0.367, loss: 1.560,data_loss: 1.560, reg_loss: 0.000 lr: 0.000973\n",
      " step: 97, acc: 0.406, loss: 1.606,data_loss: 1.606, reg_loss: 0.000 lr: 0.000973\n",
      " step: 98, acc: 0.438, loss: 1.520,data_loss: 1.520, reg_loss: 0.000 lr: 0.000973\n",
      " step: 99, acc: 0.445, loss: 1.481,data_loss: 1.481, reg_loss: 0.000 lr: 0.000973\n",
      " step: 100, acc: 0.344, loss: 1.595,data_loss: 1.595, reg_loss: 0.000 lr: 0.000973\n",
      " step: 101, acc: 0.406, loss: 1.573,data_loss: 1.573, reg_loss: 0.000 lr: 0.000973\n",
      " step: 102, acc: 0.383, loss: 1.596,data_loss: 1.596, reg_loss: 0.000 lr: 0.000973\n",
      " step: 103, acc: 0.398, loss: 1.549,data_loss: 1.549, reg_loss: 0.000 lr: 0.000973\n",
      " step: 104, acc: 0.438, loss: 1.508,data_loss: 1.508, reg_loss: 0.000 lr: 0.000973\n",
      " step: 105, acc: 0.367, loss: 1.638,data_loss: 1.638, reg_loss: 0.000 lr: 0.000973\n",
      " step: 106, acc: 0.414, loss: 1.548,data_loss: 1.548, reg_loss: 0.000 lr: 0.000973\n",
      " step: 107, acc: 0.367, loss: 1.641,data_loss: 1.641, reg_loss: 0.000 lr: 0.000973\n",
      " step: 108, acc: 0.344, loss: 1.646,data_loss: 1.646, reg_loss: 0.000 lr: 0.000973\n",
      " step: 109, acc: 0.383, loss: 1.603,data_loss: 1.603, reg_loss: 0.000 lr: 0.000973\n",
      " step: 110, acc: 0.422, loss: 1.501,data_loss: 1.501, reg_loss: 0.000 lr: 0.000973\n",
      " step: 111, acc: 0.359, loss: 1.631,data_loss: 1.631, reg_loss: 0.000 lr: 0.000973\n",
      " step: 112, acc: 0.383, loss: 1.581,data_loss: 1.581, reg_loss: 0.000 lr: 0.000973\n",
      " step: 113, acc: 0.352, loss: 1.567,data_loss: 1.567, reg_loss: 0.000 lr: 0.000973\n",
      " step: 114, acc: 0.352, loss: 1.606,data_loss: 1.606, reg_loss: 0.000 lr: 0.000973\n",
      " step: 115, acc: 0.273, loss: 1.644,data_loss: 1.644, reg_loss: 0.000 lr: 0.000973\n",
      " step: 116, acc: 0.391, loss: 1.671,data_loss: 1.671, reg_loss: 0.000 lr: 0.000972\n",
      " step: 117, acc: 0.391, loss: 1.550,data_loss: 1.550, reg_loss: 0.000 lr: 0.000972\n",
      " step: 118, acc: 0.406, loss: 1.621,data_loss: 1.621, reg_loss: 0.000 lr: 0.000972\n",
      " step: 119, acc: 0.344, loss: 1.545,data_loss: 1.545, reg_loss: 0.000 lr: 0.000972\n",
      " step: 120, acc: 0.375, loss: 1.630,data_loss: 1.630, reg_loss: 0.000 lr: 0.000972\n",
      " step: 121, acc: 0.359, loss: 1.616,data_loss: 1.616, reg_loss: 0.000 lr: 0.000972\n",
      " step: 122, acc: 0.383, loss: 1.584,data_loss: 1.584, reg_loss: 0.000 lr: 0.000972\n",
      " step: 123, acc: 0.367, loss: 1.588,data_loss: 1.588, reg_loss: 0.000 lr: 0.000972\n",
      " step: 124, acc: 0.352, loss: 1.680,data_loss: 1.680, reg_loss: 0.000 lr: 0.000972\n",
      " step: 125, acc: 0.391, loss: 1.541,data_loss: 1.541, reg_loss: 0.000 lr: 0.000972\n",
      " step: 126, acc: 0.367, loss: 1.590,data_loss: 1.590, reg_loss: 0.000 lr: 0.000972\n",
      " step: 127, acc: 0.328, loss: 1.639,data_loss: 1.639, reg_loss: 0.000 lr: 0.000972\n",
      " step: 128, acc: 0.359, loss: 1.538,data_loss: 1.538, reg_loss: 0.000 lr: 0.000972\n",
      " step: 129, acc: 0.336, loss: 1.672,data_loss: 1.672, reg_loss: 0.000 lr: 0.000972\n",
      " step: 130, acc: 0.273, loss: 1.785,data_loss: 1.785, reg_loss: 0.000 lr: 0.000972\n",
      " step: 131, acc: 0.359, loss: 1.580,data_loss: 1.580, reg_loss: 0.000 lr: 0.000972\n",
      " step: 132, acc: 0.367, loss: 1.515,data_loss: 1.515, reg_loss: 0.000 lr: 0.000972\n",
      " step: 133, acc: 0.414, loss: 1.600,data_loss: 1.600, reg_loss: 0.000 lr: 0.000972\n",
      " step: 134, acc: 0.414, loss: 1.412,data_loss: 1.412, reg_loss: 0.000 lr: 0.000972\n",
      " step: 135, acc: 0.414, loss: 1.527,data_loss: 1.527, reg_loss: 0.000 lr: 0.000972\n",
      " step: 136, acc: 0.383, loss: 1.636,data_loss: 1.636, reg_loss: 0.000 lr: 0.000972\n",
      " step: 137, acc: 0.430, loss: 1.568,data_loss: 1.568, reg_loss: 0.000 lr: 0.000971\n",
      " step: 138, acc: 0.438, loss: 1.499,data_loss: 1.499, reg_loss: 0.000 lr: 0.000971\n",
      " step: 139, acc: 0.469, loss: 1.430,data_loss: 1.430, reg_loss: 0.000 lr: 0.000971\n",
      " step: 140, acc: 0.438, loss: 1.471,data_loss: 1.471, reg_loss: 0.000 lr: 0.000971\n",
      " step: 141, acc: 0.367, loss: 1.609,data_loss: 1.609, reg_loss: 0.000 lr: 0.000971\n",
      " step: 142, acc: 0.359, loss: 1.588,data_loss: 1.588, reg_loss: 0.000 lr: 0.000971\n",
      " step: 143, acc: 0.375, loss: 1.606,data_loss: 1.606, reg_loss: 0.000 lr: 0.000971\n",
      " step: 144, acc: 0.320, loss: 1.661,data_loss: 1.661, reg_loss: 0.000 lr: 0.000971\n",
      " step: 145, acc: 0.438, loss: 1.616,data_loss: 1.616, reg_loss: 0.000 lr: 0.000971\n",
      " step: 146, acc: 0.344, loss: 1.536,data_loss: 1.536, reg_loss: 0.000 lr: 0.000971\n",
      " step: 147, acc: 0.336, loss: 1.600,data_loss: 1.600, reg_loss: 0.000 lr: 0.000971\n",
      " step: 148, acc: 0.461, loss: 1.470,data_loss: 1.470, reg_loss: 0.000 lr: 0.000971\n",
      " step: 149, acc: 0.359, loss: 1.607,data_loss: 1.607, reg_loss: 0.000 lr: 0.000971\n",
      " step: 150, acc: 0.477, loss: 1.417,data_loss: 1.417, reg_loss: 0.000 lr: 0.000971\n",
      " step: 151, acc: 0.391, loss: 1.538,data_loss: 1.538, reg_loss: 0.000 lr: 0.000971\n",
      " step: 152, acc: 0.492, loss: 1.499,data_loss: 1.499, reg_loss: 0.000 lr: 0.000971\n",
      " step: 153, acc: 0.367, loss: 1.461,data_loss: 1.461, reg_loss: 0.000 lr: 0.000971\n",
      " step: 154, acc: 0.336, loss: 1.673,data_loss: 1.673, reg_loss: 0.000 lr: 0.000971\n",
      " step: 155, acc: 0.469, loss: 1.455,data_loss: 1.455, reg_loss: 0.000 lr: 0.000971\n",
      " step: 156, acc: 0.398, loss: 1.520,data_loss: 1.520, reg_loss: 0.000 lr: 0.000971\n",
      " step: 157, acc: 0.445, loss: 1.501,data_loss: 1.501, reg_loss: 0.000 lr: 0.000971\n",
      " step: 158, acc: 0.430, loss: 1.594,data_loss: 1.594, reg_loss: 0.000 lr: 0.000970\n",
      " step: 159, acc: 0.414, loss: 1.552,data_loss: 1.552, reg_loss: 0.000 lr: 0.000970\n",
      " step: 160, acc: 0.359, loss: 1.702,data_loss: 1.702, reg_loss: 0.000 lr: 0.000970\n",
      " step: 161, acc: 0.375, loss: 1.645,data_loss: 1.645, reg_loss: 0.000 lr: 0.000970\n",
      " step: 162, acc: 0.383, loss: 1.615,data_loss: 1.615, reg_loss: 0.000 lr: 0.000970\n",
      " step: 163, acc: 0.383, loss: 1.533,data_loss: 1.533, reg_loss: 0.000 lr: 0.000970\n",
      " step: 164, acc: 0.477, loss: 1.455,data_loss: 1.455, reg_loss: 0.000 lr: 0.000970\n",
      " step: 165, acc: 0.359, loss: 1.713,data_loss: 1.713, reg_loss: 0.000 lr: 0.000970\n",
      " step: 166, acc: 0.398, loss: 1.546,data_loss: 1.546, reg_loss: 0.000 lr: 0.000970\n",
      " step: 167, acc: 0.391, loss: 1.570,data_loss: 1.570, reg_loss: 0.000 lr: 0.000970\n",
      " step: 168, acc: 0.391, loss: 1.513,data_loss: 1.513, reg_loss: 0.000 lr: 0.000970\n",
      " step: 169, acc: 0.414, loss: 1.499,data_loss: 1.499, reg_loss: 0.000 lr: 0.000970\n",
      " step: 170, acc: 0.305, loss: 1.700,data_loss: 1.700, reg_loss: 0.000 lr: 0.000970\n",
      " step: 171, acc: 0.453, loss: 1.488,data_loss: 1.488, reg_loss: 0.000 lr: 0.000970\n",
      " step: 172, acc: 0.398, loss: 1.552,data_loss: 1.552, reg_loss: 0.000 lr: 0.000970\n",
      " step: 173, acc: 0.453, loss: 1.493,data_loss: 1.493, reg_loss: 0.000 lr: 0.000970\n",
      " step: 174, acc: 0.422, loss: 1.491,data_loss: 1.491, reg_loss: 0.000 lr: 0.000970\n",
      " step: 175, acc: 0.414, loss: 1.575,data_loss: 1.575, reg_loss: 0.000 lr: 0.000970\n",
      " step: 176, acc: 0.383, loss: 1.676,data_loss: 1.676, reg_loss: 0.000 lr: 0.000970\n",
      " step: 177, acc: 0.352, loss: 1.546,data_loss: 1.546, reg_loss: 0.000 lr: 0.000970\n",
      " step: 178, acc: 0.375, loss: 1.697,data_loss: 1.697, reg_loss: 0.000 lr: 0.000970\n",
      " step: 179, acc: 0.508, loss: 1.382,data_loss: 1.382, reg_loss: 0.000 lr: 0.000970\n",
      " step: 180, acc: 0.484, loss: 1.383,data_loss: 1.383, reg_loss: 0.000 lr: 0.000969\n",
      " step: 181, acc: 0.422, loss: 1.538,data_loss: 1.538, reg_loss: 0.000 lr: 0.000969\n",
      " step: 182, acc: 0.438, loss: 1.457,data_loss: 1.457, reg_loss: 0.000 lr: 0.000969\n",
      " step: 183, acc: 0.391, loss: 1.528,data_loss: 1.528, reg_loss: 0.000 lr: 0.000969\n",
      " step: 184, acc: 0.367, loss: 1.556,data_loss: 1.556, reg_loss: 0.000 lr: 0.000969\n",
      " step: 185, acc: 0.336, loss: 1.584,data_loss: 1.584, reg_loss: 0.000 lr: 0.000969\n",
      " step: 186, acc: 0.367, loss: 1.558,data_loss: 1.558, reg_loss: 0.000 lr: 0.000969\n",
      " step: 187, acc: 0.352, loss: 1.557,data_loss: 1.557, reg_loss: 0.000 lr: 0.000969\n",
      " step: 188, acc: 0.344, loss: 1.553,data_loss: 1.553, reg_loss: 0.000 lr: 0.000969\n",
      " step: 189, acc: 0.430, loss: 1.563,data_loss: 1.563, reg_loss: 0.000 lr: 0.000969\n",
      " step: 190, acc: 0.367, loss: 1.580,data_loss: 1.580, reg_loss: 0.000 lr: 0.000969\n",
      " step: 191, acc: 0.336, loss: 1.674,data_loss: 1.674, reg_loss: 0.000 lr: 0.000969\n",
      " step: 192, acc: 0.406, loss: 1.503,data_loss: 1.503, reg_loss: 0.000 lr: 0.000969\n",
      " step: 193, acc: 0.367, loss: 1.598,data_loss: 1.598, reg_loss: 0.000 lr: 0.000969\n",
      " step: 194, acc: 0.406, loss: 1.580,data_loss: 1.580, reg_loss: 0.000 lr: 0.000969\n",
      " step: 195, acc: 0.469, loss: 1.434,data_loss: 1.434, reg_loss: 0.000 lr: 0.000969\n",
      " step: 196, acc: 0.391, loss: 1.543,data_loss: 1.543, reg_loss: 0.000 lr: 0.000969\n",
      " step: 197, acc: 0.398, loss: 1.527,data_loss: 1.527, reg_loss: 0.000 lr: 0.000969\n",
      " step: 198, acc: 0.461, loss: 1.400,data_loss: 1.400, reg_loss: 0.000 lr: 0.000969\n",
      " step: 199, acc: 0.414, loss: 1.581,data_loss: 1.581, reg_loss: 0.000 lr: 0.000969\n",
      " step: 200, acc: 0.367, loss: 1.595,data_loss: 1.595, reg_loss: 0.000 lr: 0.000969\n",
      " step: 201, acc: 0.430, loss: 1.531,data_loss: 1.531, reg_loss: 0.000 lr: 0.000968\n",
      " step: 202, acc: 0.375, loss: 1.561,data_loss: 1.561, reg_loss: 0.000 lr: 0.000968\n",
      " step: 203, acc: 0.336, loss: 1.687,data_loss: 1.687, reg_loss: 0.000 lr: 0.000968\n",
      " step: 204, acc: 0.391, loss: 1.510,data_loss: 1.510, reg_loss: 0.000 lr: 0.000968\n",
      " step: 205, acc: 0.344, loss: 1.547,data_loss: 1.547, reg_loss: 0.000 lr: 0.000968\n",
      " step: 206, acc: 0.391, loss: 1.539,data_loss: 1.539, reg_loss: 0.000 lr: 0.000968\n",
      " step: 207, acc: 0.367, loss: 1.552,data_loss: 1.552, reg_loss: 0.000 lr: 0.000968\n",
      " step: 208, acc: 0.414, loss: 1.577,data_loss: 1.577, reg_loss: 0.000 lr: 0.000968\n",
      " step: 209, acc: 0.375, loss: 1.589,data_loss: 1.589, reg_loss: 0.000 lr: 0.000968\n",
      " step: 210, acc: 0.453, loss: 1.453,data_loss: 1.453, reg_loss: 0.000 lr: 0.000968\n",
      " step: 211, acc: 0.406, loss: 1.514,data_loss: 1.514, reg_loss: 0.000 lr: 0.000968\n",
      " step: 212, acc: 0.367, loss: 1.576,data_loss: 1.576, reg_loss: 0.000 lr: 0.000968\n",
      " step: 213, acc: 0.344, loss: 1.558,data_loss: 1.558, reg_loss: 0.000 lr: 0.000968\n",
      " step: 214, acc: 0.383, loss: 1.565,data_loss: 1.565, reg_loss: 0.000 lr: 0.000968\n",
      " step: 215, acc: 0.438, loss: 1.536,data_loss: 1.536, reg_loss: 0.000 lr: 0.000968\n",
      " step: 216, acc: 0.414, loss: 1.534,data_loss: 1.534, reg_loss: 0.000 lr: 0.000968\n",
      " step: 217, acc: 0.352, loss: 1.532,data_loss: 1.532, reg_loss: 0.000 lr: 0.000968\n",
      " step: 218, acc: 0.445, loss: 1.493,data_loss: 1.493, reg_loss: 0.000 lr: 0.000968\n",
      " step: 219, acc: 0.414, loss: 1.502,data_loss: 1.502, reg_loss: 0.000 lr: 0.000968\n",
      " step: 220, acc: 0.367, loss: 1.564,data_loss: 1.564, reg_loss: 0.000 lr: 0.000968\n",
      " step: 221, acc: 0.344, loss: 1.620,data_loss: 1.620, reg_loss: 0.000 lr: 0.000968\n",
      " step: 222, acc: 0.367, loss: 1.645,data_loss: 1.645, reg_loss: 0.000 lr: 0.000967\n",
      " step: 223, acc: 0.367, loss: 1.626,data_loss: 1.626, reg_loss: 0.000 lr: 0.000967\n",
      " step: 224, acc: 0.351, loss: 1.600,data_loss: 1.600, reg_loss: 0.000 lr: 0.000967\n",
      "training , acc: 0.383, loss: 1.574,data_loss: 1.574, reg_loss: 0.000 lr: 0.000967\n",
      "validation, acc:0.387 ,loss: 1.566 \n",
      "epoch: 4\n",
      " step: 0, acc: 0.344, loss: 1.714,data_loss: 1.714, reg_loss: 0.000 lr: 0.000967\n",
      " step: 1, acc: 0.352, loss: 1.612,data_loss: 1.612, reg_loss: 0.000 lr: 0.000967\n",
      " step: 2, acc: 0.383, loss: 1.529,data_loss: 1.529, reg_loss: 0.000 lr: 0.000967\n",
      " step: 3, acc: 0.414, loss: 1.559,data_loss: 1.559, reg_loss: 0.000 lr: 0.000967\n",
      " step: 4, acc: 0.422, loss: 1.506,data_loss: 1.506, reg_loss: 0.000 lr: 0.000967\n",
      " step: 5, acc: 0.359, loss: 1.578,data_loss: 1.578, reg_loss: 0.000 lr: 0.000967\n",
      " step: 6, acc: 0.383, loss: 1.551,data_loss: 1.551, reg_loss: 0.000 lr: 0.000967\n",
      " step: 7, acc: 0.289, loss: 1.692,data_loss: 1.692, reg_loss: 0.000 lr: 0.000967\n",
      " step: 8, acc: 0.414, loss: 1.548,data_loss: 1.548, reg_loss: 0.000 lr: 0.000967\n",
      " step: 9, acc: 0.359, loss: 1.554,data_loss: 1.554, reg_loss: 0.000 lr: 0.000967\n",
      " step: 10, acc: 0.398, loss: 1.579,data_loss: 1.579, reg_loss: 0.000 lr: 0.000967\n",
      " step: 11, acc: 0.477, loss: 1.425,data_loss: 1.425, reg_loss: 0.000 lr: 0.000967\n",
      " step: 12, acc: 0.367, loss: 1.557,data_loss: 1.557, reg_loss: 0.000 lr: 0.000967\n",
      " step: 13, acc: 0.375, loss: 1.523,data_loss: 1.523, reg_loss: 0.000 lr: 0.000967\n",
      " step: 14, acc: 0.312, loss: 1.628,data_loss: 1.628, reg_loss: 0.000 lr: 0.000967\n",
      " step: 15, acc: 0.367, loss: 1.572,data_loss: 1.572, reg_loss: 0.000 lr: 0.000967\n",
      " step: 16, acc: 0.375, loss: 1.556,data_loss: 1.556, reg_loss: 0.000 lr: 0.000967\n",
      " step: 17, acc: 0.375, loss: 1.605,data_loss: 1.605, reg_loss: 0.000 lr: 0.000967\n",
      " step: 18, acc: 0.391, loss: 1.608,data_loss: 1.608, reg_loss: 0.000 lr: 0.000967\n",
      " step: 19, acc: 0.383, loss: 1.515,data_loss: 1.515, reg_loss: 0.000 lr: 0.000966\n",
      " step: 20, acc: 0.344, loss: 1.527,data_loss: 1.527, reg_loss: 0.000 lr: 0.000966\n",
      " step: 21, acc: 0.430, loss: 1.548,data_loss: 1.548, reg_loss: 0.000 lr: 0.000966\n",
      " step: 22, acc: 0.430, loss: 1.485,data_loss: 1.485, reg_loss: 0.000 lr: 0.000966\n",
      " step: 23, acc: 0.398, loss: 1.558,data_loss: 1.558, reg_loss: 0.000 lr: 0.000966\n",
      " step: 24, acc: 0.398, loss: 1.539,data_loss: 1.539, reg_loss: 0.000 lr: 0.000966\n",
      " step: 25, acc: 0.453, loss: 1.477,data_loss: 1.477, reg_loss: 0.000 lr: 0.000966\n",
      " step: 26, acc: 0.422, loss: 1.567,data_loss: 1.567, reg_loss: 0.000 lr: 0.000966\n",
      " step: 27, acc: 0.453, loss: 1.547,data_loss: 1.547, reg_loss: 0.000 lr: 0.000966\n",
      " step: 28, acc: 0.328, loss: 1.652,data_loss: 1.652, reg_loss: 0.000 lr: 0.000966\n",
      " step: 29, acc: 0.359, loss: 1.582,data_loss: 1.582, reg_loss: 0.000 lr: 0.000966\n",
      " step: 30, acc: 0.312, loss: 1.770,data_loss: 1.770, reg_loss: 0.000 lr: 0.000966\n",
      " step: 31, acc: 0.438, loss: 1.491,data_loss: 1.491, reg_loss: 0.000 lr: 0.000966\n",
      " step: 32, acc: 0.406, loss: 1.610,data_loss: 1.610, reg_loss: 0.000 lr: 0.000966\n",
      " step: 33, acc: 0.406, loss: 1.612,data_loss: 1.612, reg_loss: 0.000 lr: 0.000966\n",
      " step: 34, acc: 0.367, loss: 1.600,data_loss: 1.600, reg_loss: 0.000 lr: 0.000966\n",
      " step: 35, acc: 0.492, loss: 1.483,data_loss: 1.483, reg_loss: 0.000 lr: 0.000966\n",
      " step: 36, acc: 0.406, loss: 1.440,data_loss: 1.440, reg_loss: 0.000 lr: 0.000966\n",
      " step: 37, acc: 0.367, loss: 1.577,data_loss: 1.577, reg_loss: 0.000 lr: 0.000966\n",
      " step: 38, acc: 0.383, loss: 1.628,data_loss: 1.628, reg_loss: 0.000 lr: 0.000966\n",
      " step: 39, acc: 0.383, loss: 1.550,data_loss: 1.550, reg_loss: 0.000 lr: 0.000966\n",
      " step: 40, acc: 0.375, loss: 1.632,data_loss: 1.632, reg_loss: 0.000 lr: 0.000965\n",
      " step: 41, acc: 0.367, loss: 1.626,data_loss: 1.626, reg_loss: 0.000 lr: 0.000965\n",
      " step: 42, acc: 0.375, loss: 1.663,data_loss: 1.663, reg_loss: 0.000 lr: 0.000965\n",
      " step: 43, acc: 0.344, loss: 1.535,data_loss: 1.535, reg_loss: 0.000 lr: 0.000965\n",
      " step: 44, acc: 0.352, loss: 1.642,data_loss: 1.642, reg_loss: 0.000 lr: 0.000965\n",
      " step: 45, acc: 0.398, loss: 1.459,data_loss: 1.459, reg_loss: 0.000 lr: 0.000965\n",
      " step: 46, acc: 0.438, loss: 1.532,data_loss: 1.532, reg_loss: 0.000 lr: 0.000965\n",
      " step: 47, acc: 0.391, loss: 1.566,data_loss: 1.566, reg_loss: 0.000 lr: 0.000965\n",
      " step: 48, acc: 0.453, loss: 1.405,data_loss: 1.405, reg_loss: 0.000 lr: 0.000965\n",
      " step: 49, acc: 0.430, loss: 1.485,data_loss: 1.485, reg_loss: 0.000 lr: 0.000965\n",
      " step: 50, acc: 0.469, loss: 1.457,data_loss: 1.457, reg_loss: 0.000 lr: 0.000965\n",
      " step: 51, acc: 0.359, loss: 1.638,data_loss: 1.638, reg_loss: 0.000 lr: 0.000965\n",
      " step: 52, acc: 0.352, loss: 1.598,data_loss: 1.598, reg_loss: 0.000 lr: 0.000965\n",
      " step: 53, acc: 0.391, loss: 1.546,data_loss: 1.546, reg_loss: 0.000 lr: 0.000965\n",
      " step: 54, acc: 0.367, loss: 1.595,data_loss: 1.595, reg_loss: 0.000 lr: 0.000965\n",
      " step: 55, acc: 0.414, loss: 1.578,data_loss: 1.578, reg_loss: 0.000 lr: 0.000965\n",
      " step: 56, acc: 0.406, loss: 1.554,data_loss: 1.554, reg_loss: 0.000 lr: 0.000965\n",
      " step: 57, acc: 0.461, loss: 1.546,data_loss: 1.546, reg_loss: 0.000 lr: 0.000965\n",
      " step: 58, acc: 0.391, loss: 1.534,data_loss: 1.534, reg_loss: 0.000 lr: 0.000965\n",
      " step: 59, acc: 0.453, loss: 1.509,data_loss: 1.509, reg_loss: 0.000 lr: 0.000965\n",
      " step: 60, acc: 0.375, loss: 1.558,data_loss: 1.558, reg_loss: 0.000 lr: 0.000965\n",
      " step: 61, acc: 0.414, loss: 1.554,data_loss: 1.554, reg_loss: 0.000 lr: 0.000965\n",
      " step: 62, acc: 0.414, loss: 1.512,data_loss: 1.512, reg_loss: 0.000 lr: 0.000964\n",
      " step: 63, acc: 0.422, loss: 1.526,data_loss: 1.526, reg_loss: 0.000 lr: 0.000964\n",
      " step: 64, acc: 0.469, loss: 1.517,data_loss: 1.517, reg_loss: 0.000 lr: 0.000964\n",
      " step: 65, acc: 0.422, loss: 1.531,data_loss: 1.531, reg_loss: 0.000 lr: 0.000964\n",
      " step: 66, acc: 0.453, loss: 1.460,data_loss: 1.460, reg_loss: 0.000 lr: 0.000964\n",
      " step: 67, acc: 0.430, loss: 1.480,data_loss: 1.480, reg_loss: 0.000 lr: 0.000964\n",
      " step: 68, acc: 0.391, loss: 1.530,data_loss: 1.530, reg_loss: 0.000 lr: 0.000964\n",
      " step: 69, acc: 0.320, loss: 1.571,data_loss: 1.571, reg_loss: 0.000 lr: 0.000964\n",
      " step: 70, acc: 0.375, loss: 1.524,data_loss: 1.524, reg_loss: 0.000 lr: 0.000964\n",
      " step: 71, acc: 0.469, loss: 1.469,data_loss: 1.469, reg_loss: 0.000 lr: 0.000964\n",
      " step: 72, acc: 0.406, loss: 1.515,data_loss: 1.515, reg_loss: 0.000 lr: 0.000964\n",
      " step: 73, acc: 0.391, loss: 1.511,data_loss: 1.511, reg_loss: 0.000 lr: 0.000964\n",
      " step: 74, acc: 0.367, loss: 1.610,data_loss: 1.610, reg_loss: 0.000 lr: 0.000964\n",
      " step: 75, acc: 0.461, loss: 1.422,data_loss: 1.422, reg_loss: 0.000 lr: 0.000964\n",
      " step: 76, acc: 0.414, loss: 1.515,data_loss: 1.515, reg_loss: 0.000 lr: 0.000964\n",
      " step: 77, acc: 0.375, loss: 1.522,data_loss: 1.522, reg_loss: 0.000 lr: 0.000964\n",
      " step: 78, acc: 0.422, loss: 1.421,data_loss: 1.421, reg_loss: 0.000 lr: 0.000964\n",
      " step: 79, acc: 0.383, loss: 1.482,data_loss: 1.482, reg_loss: 0.000 lr: 0.000964\n",
      " step: 80, acc: 0.367, loss: 1.549,data_loss: 1.549, reg_loss: 0.000 lr: 0.000964\n",
      " step: 81, acc: 0.352, loss: 1.644,data_loss: 1.644, reg_loss: 0.000 lr: 0.000964\n",
      " step: 82, acc: 0.477, loss: 1.497,data_loss: 1.497, reg_loss: 0.000 lr: 0.000964\n",
      " step: 83, acc: 0.375, loss: 1.507,data_loss: 1.507, reg_loss: 0.000 lr: 0.000963\n",
      " step: 84, acc: 0.375, loss: 1.608,data_loss: 1.608, reg_loss: 0.000 lr: 0.000963\n",
      " step: 85, acc: 0.406, loss: 1.483,data_loss: 1.483, reg_loss: 0.000 lr: 0.000963\n",
      " step: 86, acc: 0.375, loss: 1.465,data_loss: 1.465, reg_loss: 0.000 lr: 0.000963\n",
      " step: 87, acc: 0.430, loss: 1.495,data_loss: 1.495, reg_loss: 0.000 lr: 0.000963\n",
      " step: 88, acc: 0.391, loss: 1.608,data_loss: 1.608, reg_loss: 0.000 lr: 0.000963\n",
      " step: 89, acc: 0.422, loss: 1.544,data_loss: 1.544, reg_loss: 0.000 lr: 0.000963\n",
      " step: 90, acc: 0.438, loss: 1.510,data_loss: 1.510, reg_loss: 0.000 lr: 0.000963\n",
      " step: 91, acc: 0.375, loss: 1.520,data_loss: 1.520, reg_loss: 0.000 lr: 0.000963\n",
      " step: 92, acc: 0.430, loss: 1.566,data_loss: 1.566, reg_loss: 0.000 lr: 0.000963\n",
      " step: 93, acc: 0.406, loss: 1.460,data_loss: 1.460, reg_loss: 0.000 lr: 0.000963\n",
      " step: 94, acc: 0.344, loss: 1.603,data_loss: 1.603, reg_loss: 0.000 lr: 0.000963\n",
      " step: 95, acc: 0.461, loss: 1.509,data_loss: 1.509, reg_loss: 0.000 lr: 0.000963\n",
      " step: 96, acc: 0.375, loss: 1.513,data_loss: 1.513, reg_loss: 0.000 lr: 0.000963\n",
      " step: 97, acc: 0.336, loss: 1.559,data_loss: 1.559, reg_loss: 0.000 lr: 0.000963\n",
      " step: 98, acc: 0.445, loss: 1.495,data_loss: 1.495, reg_loss: 0.000 lr: 0.000963\n",
      " step: 99, acc: 0.406, loss: 1.424,data_loss: 1.424, reg_loss: 0.000 lr: 0.000963\n",
      " step: 100, acc: 0.367, loss: 1.537,data_loss: 1.537, reg_loss: 0.000 lr: 0.000963\n",
      " step: 101, acc: 0.391, loss: 1.515,data_loss: 1.515, reg_loss: 0.000 lr: 0.000963\n",
      " step: 102, acc: 0.430, loss: 1.543,data_loss: 1.543, reg_loss: 0.000 lr: 0.000963\n",
      " step: 103, acc: 0.398, loss: 1.483,data_loss: 1.483, reg_loss: 0.000 lr: 0.000963\n",
      " step: 104, acc: 0.461, loss: 1.451,data_loss: 1.451, reg_loss: 0.000 lr: 0.000963\n",
      " step: 105, acc: 0.383, loss: 1.512,data_loss: 1.512, reg_loss: 0.000 lr: 0.000962\n",
      " step: 106, acc: 0.500, loss: 1.456,data_loss: 1.456, reg_loss: 0.000 lr: 0.000962\n",
      " step: 107, acc: 0.398, loss: 1.626,data_loss: 1.626, reg_loss: 0.000 lr: 0.000962\n",
      " step: 108, acc: 0.344, loss: 1.605,data_loss: 1.605, reg_loss: 0.000 lr: 0.000962\n",
      " step: 109, acc: 0.406, loss: 1.572,data_loss: 1.572, reg_loss: 0.000 lr: 0.000962\n",
      " step: 110, acc: 0.406, loss: 1.519,data_loss: 1.519, reg_loss: 0.000 lr: 0.000962\n",
      " step: 111, acc: 0.414, loss: 1.505,data_loss: 1.505, reg_loss: 0.000 lr: 0.000962\n",
      " step: 112, acc: 0.414, loss: 1.549,data_loss: 1.549, reg_loss: 0.000 lr: 0.000962\n",
      " step: 113, acc: 0.422, loss: 1.540,data_loss: 1.540, reg_loss: 0.000 lr: 0.000962\n",
      " step: 114, acc: 0.383, loss: 1.562,data_loss: 1.562, reg_loss: 0.000 lr: 0.000962\n",
      " step: 115, acc: 0.422, loss: 1.527,data_loss: 1.527, reg_loss: 0.000 lr: 0.000962\n",
      " step: 116, acc: 0.414, loss: 1.585,data_loss: 1.585, reg_loss: 0.000 lr: 0.000962\n",
      " step: 117, acc: 0.438, loss: 1.469,data_loss: 1.469, reg_loss: 0.000 lr: 0.000962\n",
      " step: 118, acc: 0.383, loss: 1.614,data_loss: 1.614, reg_loss: 0.000 lr: 0.000962\n",
      " step: 119, acc: 0.344, loss: 1.520,data_loss: 1.520, reg_loss: 0.000 lr: 0.000962\n",
      " step: 120, acc: 0.414, loss: 1.558,data_loss: 1.558, reg_loss: 0.000 lr: 0.000962\n",
      " step: 121, acc: 0.359, loss: 1.647,data_loss: 1.647, reg_loss: 0.000 lr: 0.000962\n",
      " step: 122, acc: 0.398, loss: 1.531,data_loss: 1.531, reg_loss: 0.000 lr: 0.000962\n",
      " step: 123, acc: 0.352, loss: 1.561,data_loss: 1.561, reg_loss: 0.000 lr: 0.000962\n",
      " step: 124, acc: 0.359, loss: 1.616,data_loss: 1.616, reg_loss: 0.000 lr: 0.000962\n",
      " step: 125, acc: 0.422, loss: 1.453,data_loss: 1.453, reg_loss: 0.000 lr: 0.000962\n",
      " step: 126, acc: 0.430, loss: 1.585,data_loss: 1.585, reg_loss: 0.000 lr: 0.000961\n",
      " step: 127, acc: 0.344, loss: 1.568,data_loss: 1.568, reg_loss: 0.000 lr: 0.000961\n",
      " step: 128, acc: 0.391, loss: 1.493,data_loss: 1.493, reg_loss: 0.000 lr: 0.000961\n",
      " step: 129, acc: 0.375, loss: 1.630,data_loss: 1.630, reg_loss: 0.000 lr: 0.000961\n",
      " step: 130, acc: 0.312, loss: 1.705,data_loss: 1.705, reg_loss: 0.000 lr: 0.000961\n",
      " step: 131, acc: 0.453, loss: 1.475,data_loss: 1.475, reg_loss: 0.000 lr: 0.000961\n",
      " step: 132, acc: 0.391, loss: 1.497,data_loss: 1.497, reg_loss: 0.000 lr: 0.000961\n",
      " step: 133, acc: 0.414, loss: 1.568,data_loss: 1.568, reg_loss: 0.000 lr: 0.000961\n",
      " step: 134, acc: 0.516, loss: 1.342,data_loss: 1.342, reg_loss: 0.000 lr: 0.000961\n",
      " step: 135, acc: 0.438, loss: 1.513,data_loss: 1.513, reg_loss: 0.000 lr: 0.000961\n",
      " step: 136, acc: 0.422, loss: 1.602,data_loss: 1.602, reg_loss: 0.000 lr: 0.000961\n",
      " step: 137, acc: 0.422, loss: 1.477,data_loss: 1.477, reg_loss: 0.000 lr: 0.000961\n",
      " step: 138, acc: 0.461, loss: 1.444,data_loss: 1.444, reg_loss: 0.000 lr: 0.000961\n",
      " step: 139, acc: 0.445, loss: 1.414,data_loss: 1.414, reg_loss: 0.000 lr: 0.000961\n",
      " step: 140, acc: 0.453, loss: 1.411,data_loss: 1.411, reg_loss: 0.000 lr: 0.000961\n",
      " step: 141, acc: 0.359, loss: 1.511,data_loss: 1.511, reg_loss: 0.000 lr: 0.000961\n",
      " step: 142, acc: 0.414, loss: 1.448,data_loss: 1.448, reg_loss: 0.000 lr: 0.000961\n",
      " step: 143, acc: 0.398, loss: 1.623,data_loss: 1.623, reg_loss: 0.000 lr: 0.000961\n",
      " step: 144, acc: 0.375, loss: 1.629,data_loss: 1.629, reg_loss: 0.000 lr: 0.000961\n",
      " step: 145, acc: 0.445, loss: 1.560,data_loss: 1.560, reg_loss: 0.000 lr: 0.000961\n",
      " step: 146, acc: 0.383, loss: 1.541,data_loss: 1.541, reg_loss: 0.000 lr: 0.000961\n",
      " step: 147, acc: 0.336, loss: 1.500,data_loss: 1.500, reg_loss: 0.000 lr: 0.000961\n",
      " step: 148, acc: 0.414, loss: 1.482,data_loss: 1.482, reg_loss: 0.000 lr: 0.000960\n",
      " step: 149, acc: 0.391, loss: 1.554,data_loss: 1.554, reg_loss: 0.000 lr: 0.000960\n",
      " step: 150, acc: 0.430, loss: 1.397,data_loss: 1.397, reg_loss: 0.000 lr: 0.000960\n",
      " step: 151, acc: 0.414, loss: 1.476,data_loss: 1.476, reg_loss: 0.000 lr: 0.000960\n",
      " step: 152, acc: 0.516, loss: 1.429,data_loss: 1.429, reg_loss: 0.000 lr: 0.000960\n",
      " step: 153, acc: 0.406, loss: 1.432,data_loss: 1.432, reg_loss: 0.000 lr: 0.000960\n",
      " step: 154, acc: 0.320, loss: 1.626,data_loss: 1.626, reg_loss: 0.000 lr: 0.000960\n",
      " step: 155, acc: 0.469, loss: 1.453,data_loss: 1.453, reg_loss: 0.000 lr: 0.000960\n",
      " step: 156, acc: 0.406, loss: 1.460,data_loss: 1.460, reg_loss: 0.000 lr: 0.000960\n",
      " step: 157, acc: 0.445, loss: 1.472,data_loss: 1.472, reg_loss: 0.000 lr: 0.000960\n",
      " step: 158, acc: 0.406, loss: 1.537,data_loss: 1.537, reg_loss: 0.000 lr: 0.000960\n",
      " step: 159, acc: 0.391, loss: 1.571,data_loss: 1.571, reg_loss: 0.000 lr: 0.000960\n",
      " step: 160, acc: 0.367, loss: 1.703,data_loss: 1.703, reg_loss: 0.000 lr: 0.000960\n",
      " step: 161, acc: 0.430, loss: 1.576,data_loss: 1.576, reg_loss: 0.000 lr: 0.000960\n",
      " step: 162, acc: 0.438, loss: 1.567,data_loss: 1.567, reg_loss: 0.000 lr: 0.000960\n",
      " step: 163, acc: 0.367, loss: 1.499,data_loss: 1.499, reg_loss: 0.000 lr: 0.000960\n",
      " step: 164, acc: 0.477, loss: 1.452,data_loss: 1.452, reg_loss: 0.000 lr: 0.000960\n",
      " step: 165, acc: 0.312, loss: 1.682,data_loss: 1.682, reg_loss: 0.000 lr: 0.000960\n",
      " step: 166, acc: 0.414, loss: 1.512,data_loss: 1.512, reg_loss: 0.000 lr: 0.000960\n",
      " step: 167, acc: 0.367, loss: 1.531,data_loss: 1.531, reg_loss: 0.000 lr: 0.000960\n",
      " step: 168, acc: 0.430, loss: 1.480,data_loss: 1.480, reg_loss: 0.000 lr: 0.000960\n",
      " step: 169, acc: 0.406, loss: 1.473,data_loss: 1.473, reg_loss: 0.000 lr: 0.000960\n",
      " step: 170, acc: 0.352, loss: 1.660,data_loss: 1.660, reg_loss: 0.000 lr: 0.000959\n",
      " step: 171, acc: 0.461, loss: 1.483,data_loss: 1.483, reg_loss: 0.000 lr: 0.000959\n",
      " step: 172, acc: 0.398, loss: 1.490,data_loss: 1.490, reg_loss: 0.000 lr: 0.000959\n",
      " step: 173, acc: 0.453, loss: 1.459,data_loss: 1.459, reg_loss: 0.000 lr: 0.000959\n",
      " step: 174, acc: 0.438, loss: 1.501,data_loss: 1.501, reg_loss: 0.000 lr: 0.000959\n",
      " step: 175, acc: 0.406, loss: 1.538,data_loss: 1.538, reg_loss: 0.000 lr: 0.000959\n",
      " step: 176, acc: 0.336, loss: 1.669,data_loss: 1.669, reg_loss: 0.000 lr: 0.000959\n",
      " step: 177, acc: 0.383, loss: 1.486,data_loss: 1.486, reg_loss: 0.000 lr: 0.000959\n",
      " step: 178, acc: 0.398, loss: 1.655,data_loss: 1.655, reg_loss: 0.000 lr: 0.000959\n",
      " step: 179, acc: 0.484, loss: 1.349,data_loss: 1.349, reg_loss: 0.000 lr: 0.000959\n",
      " step: 180, acc: 0.492, loss: 1.364,data_loss: 1.364, reg_loss: 0.000 lr: 0.000959\n",
      " step: 181, acc: 0.461, loss: 1.466,data_loss: 1.466, reg_loss: 0.000 lr: 0.000959\n",
      " step: 182, acc: 0.477, loss: 1.405,data_loss: 1.405, reg_loss: 0.000 lr: 0.000959\n",
      " step: 183, acc: 0.414, loss: 1.454,data_loss: 1.454, reg_loss: 0.000 lr: 0.000959\n",
      " step: 184, acc: 0.359, loss: 1.559,data_loss: 1.559, reg_loss: 0.000 lr: 0.000959\n",
      " step: 185, acc: 0.352, loss: 1.567,data_loss: 1.567, reg_loss: 0.000 lr: 0.000959\n",
      " step: 186, acc: 0.414, loss: 1.508,data_loss: 1.508, reg_loss: 0.000 lr: 0.000959\n",
      " step: 187, acc: 0.461, loss: 1.461,data_loss: 1.461, reg_loss: 0.000 lr: 0.000959\n",
      " step: 188, acc: 0.352, loss: 1.551,data_loss: 1.551, reg_loss: 0.000 lr: 0.000959\n",
      " step: 189, acc: 0.391, loss: 1.513,data_loss: 1.513, reg_loss: 0.000 lr: 0.000959\n",
      " step: 190, acc: 0.398, loss: 1.578,data_loss: 1.578, reg_loss: 0.000 lr: 0.000959\n",
      " step: 191, acc: 0.367, loss: 1.601,data_loss: 1.601, reg_loss: 0.000 lr: 0.000958\n",
      " step: 192, acc: 0.469, loss: 1.473,data_loss: 1.473, reg_loss: 0.000 lr: 0.000958\n",
      " step: 193, acc: 0.430, loss: 1.505,data_loss: 1.505, reg_loss: 0.000 lr: 0.000958\n",
      " step: 194, acc: 0.461, loss: 1.532,data_loss: 1.532, reg_loss: 0.000 lr: 0.000958\n",
      " step: 195, acc: 0.469, loss: 1.392,data_loss: 1.392, reg_loss: 0.000 lr: 0.000958\n",
      " step: 196, acc: 0.406, loss: 1.485,data_loss: 1.485, reg_loss: 0.000 lr: 0.000958\n",
      " step: 197, acc: 0.461, loss: 1.441,data_loss: 1.441, reg_loss: 0.000 lr: 0.000958\n",
      " step: 198, acc: 0.453, loss: 1.447,data_loss: 1.447, reg_loss: 0.000 lr: 0.000958\n",
      " step: 199, acc: 0.422, loss: 1.544,data_loss: 1.544, reg_loss: 0.000 lr: 0.000958\n",
      " step: 200, acc: 0.383, loss: 1.548,data_loss: 1.548, reg_loss: 0.000 lr: 0.000958\n",
      " step: 201, acc: 0.422, loss: 1.502,data_loss: 1.502, reg_loss: 0.000 lr: 0.000958\n",
      " step: 202, acc: 0.383, loss: 1.513,data_loss: 1.513, reg_loss: 0.000 lr: 0.000958\n",
      " step: 203, acc: 0.297, loss: 1.659,data_loss: 1.659, reg_loss: 0.000 lr: 0.000958\n",
      " step: 204, acc: 0.438, loss: 1.486,data_loss: 1.486, reg_loss: 0.000 lr: 0.000958\n",
      " step: 205, acc: 0.375, loss: 1.511,data_loss: 1.511, reg_loss: 0.000 lr: 0.000958\n",
      " step: 206, acc: 0.414, loss: 1.467,data_loss: 1.467, reg_loss: 0.000 lr: 0.000958\n",
      " step: 207, acc: 0.422, loss: 1.503,data_loss: 1.503, reg_loss: 0.000 lr: 0.000958\n",
      " step: 208, acc: 0.469, loss: 1.492,data_loss: 1.492, reg_loss: 0.000 lr: 0.000958\n",
      " step: 209, acc: 0.336, loss: 1.563,data_loss: 1.563, reg_loss: 0.000 lr: 0.000958\n",
      " step: 210, acc: 0.461, loss: 1.451,data_loss: 1.451, reg_loss: 0.000 lr: 0.000958\n",
      " step: 211, acc: 0.469, loss: 1.482,data_loss: 1.482, reg_loss: 0.000 lr: 0.000958\n",
      " step: 212, acc: 0.438, loss: 1.472,data_loss: 1.472, reg_loss: 0.000 lr: 0.000958\n",
      " step: 213, acc: 0.398, loss: 1.530,data_loss: 1.530, reg_loss: 0.000 lr: 0.000957\n",
      " step: 214, acc: 0.344, loss: 1.610,data_loss: 1.610, reg_loss: 0.000 lr: 0.000957\n",
      " step: 215, acc: 0.469, loss: 1.477,data_loss: 1.477, reg_loss: 0.000 lr: 0.000957\n",
      " step: 216, acc: 0.445, loss: 1.466,data_loss: 1.466, reg_loss: 0.000 lr: 0.000957\n",
      " step: 217, acc: 0.414, loss: 1.486,data_loss: 1.486, reg_loss: 0.000 lr: 0.000957\n",
      " step: 218, acc: 0.469, loss: 1.464,data_loss: 1.464, reg_loss: 0.000 lr: 0.000957\n",
      " step: 219, acc: 0.383, loss: 1.475,data_loss: 1.475, reg_loss: 0.000 lr: 0.000957\n",
      " step: 220, acc: 0.414, loss: 1.494,data_loss: 1.494, reg_loss: 0.000 lr: 0.000957\n",
      " step: 221, acc: 0.406, loss: 1.545,data_loss: 1.545, reg_loss: 0.000 lr: 0.000957\n",
      " step: 222, acc: 0.430, loss: 1.592,data_loss: 1.592, reg_loss: 0.000 lr: 0.000957\n",
      " step: 223, acc: 0.391, loss: 1.565,data_loss: 1.565, reg_loss: 0.000 lr: 0.000957\n",
      " step: 224, acc: 0.378, loss: 1.526,data_loss: 1.526, reg_loss: 0.000 lr: 0.000957\n",
      "training , acc: 0.404, loss: 1.531,data_loss: 1.531, reg_loss: 0.000 lr: 0.000957\n",
      "validation, acc:0.404 ,loss: 1.532 \n",
      "epoch: 5\n",
      " step: 0, acc: 0.344, loss: 1.710,data_loss: 1.710, reg_loss: 0.000 lr: 0.000957\n",
      " step: 1, acc: 0.398, loss: 1.527,data_loss: 1.527, reg_loss: 0.000 lr: 0.000957\n",
      " step: 2, acc: 0.469, loss: 1.470,data_loss: 1.470, reg_loss: 0.000 lr: 0.000957\n",
      " step: 3, acc: 0.430, loss: 1.508,data_loss: 1.508, reg_loss: 0.000 lr: 0.000957\n",
      " step: 4, acc: 0.445, loss: 1.462,data_loss: 1.462, reg_loss: 0.000 lr: 0.000957\n",
      " step: 5, acc: 0.375, loss: 1.548,data_loss: 1.548, reg_loss: 0.000 lr: 0.000957\n",
      " step: 6, acc: 0.398, loss: 1.511,data_loss: 1.511, reg_loss: 0.000 lr: 0.000957\n",
      " step: 7, acc: 0.359, loss: 1.647,data_loss: 1.647, reg_loss: 0.000 lr: 0.000957\n",
      " step: 8, acc: 0.398, loss: 1.513,data_loss: 1.513, reg_loss: 0.000 lr: 0.000957\n",
      " step: 9, acc: 0.391, loss: 1.504,data_loss: 1.504, reg_loss: 0.000 lr: 0.000957\n",
      " step: 10, acc: 0.453, loss: 1.491,data_loss: 1.491, reg_loss: 0.000 lr: 0.000956\n",
      " step: 11, acc: 0.438, loss: 1.409,data_loss: 1.409, reg_loss: 0.000 lr: 0.000956\n",
      " step: 12, acc: 0.414, loss: 1.506,data_loss: 1.506, reg_loss: 0.000 lr: 0.000956\n",
      " step: 13, acc: 0.430, loss: 1.472,data_loss: 1.472, reg_loss: 0.000 lr: 0.000956\n",
      " step: 14, acc: 0.359, loss: 1.548,data_loss: 1.548, reg_loss: 0.000 lr: 0.000956\n",
      " step: 15, acc: 0.430, loss: 1.469,data_loss: 1.469, reg_loss: 0.000 lr: 0.000956\n",
      " step: 16, acc: 0.398, loss: 1.479,data_loss: 1.479, reg_loss: 0.000 lr: 0.000956\n",
      " step: 17, acc: 0.406, loss: 1.609,data_loss: 1.609, reg_loss: 0.000 lr: 0.000956\n",
      " step: 18, acc: 0.367, loss: 1.618,data_loss: 1.618, reg_loss: 0.000 lr: 0.000956\n",
      " step: 19, acc: 0.406, loss: 1.425,data_loss: 1.425, reg_loss: 0.000 lr: 0.000956\n",
      " step: 20, acc: 0.352, loss: 1.585,data_loss: 1.585, reg_loss: 0.000 lr: 0.000956\n",
      " step: 21, acc: 0.453, loss: 1.403,data_loss: 1.403, reg_loss: 0.000 lr: 0.000956\n",
      " step: 22, acc: 0.469, loss: 1.420,data_loss: 1.420, reg_loss: 0.000 lr: 0.000956\n",
      " step: 23, acc: 0.391, loss: 1.499,data_loss: 1.499, reg_loss: 0.000 lr: 0.000956\n",
      " step: 24, acc: 0.430, loss: 1.492,data_loss: 1.492, reg_loss: 0.000 lr: 0.000956\n",
      " step: 25, acc: 0.453, loss: 1.406,data_loss: 1.406, reg_loss: 0.000 lr: 0.000956\n",
      " step: 26, acc: 0.336, loss: 1.549,data_loss: 1.549, reg_loss: 0.000 lr: 0.000956\n",
      " step: 27, acc: 0.398, loss: 1.479,data_loss: 1.479, reg_loss: 0.000 lr: 0.000956\n",
      " step: 28, acc: 0.344, loss: 1.541,data_loss: 1.541, reg_loss: 0.000 lr: 0.000956\n",
      " step: 29, acc: 0.375, loss: 1.551,data_loss: 1.551, reg_loss: 0.000 lr: 0.000956\n",
      " step: 30, acc: 0.383, loss: 1.680,data_loss: 1.680, reg_loss: 0.000 lr: 0.000956\n",
      " step: 31, acc: 0.445, loss: 1.427,data_loss: 1.427, reg_loss: 0.000 lr: 0.000956\n",
      " step: 32, acc: 0.461, loss: 1.502,data_loss: 1.502, reg_loss: 0.000 lr: 0.000955\n",
      " step: 33, acc: 0.406, loss: 1.611,data_loss: 1.611, reg_loss: 0.000 lr: 0.000955\n",
      " step: 34, acc: 0.422, loss: 1.532,data_loss: 1.532, reg_loss: 0.000 lr: 0.000955\n",
      " step: 35, acc: 0.492, loss: 1.443,data_loss: 1.443, reg_loss: 0.000 lr: 0.000955\n",
      " step: 36, acc: 0.438, loss: 1.426,data_loss: 1.426, reg_loss: 0.000 lr: 0.000955\n",
      " step: 37, acc: 0.406, loss: 1.553,data_loss: 1.553, reg_loss: 0.000 lr: 0.000955\n",
      " step: 38, acc: 0.406, loss: 1.554,data_loss: 1.554, reg_loss: 0.000 lr: 0.000955\n",
      " step: 39, acc: 0.391, loss: 1.531,data_loss: 1.531, reg_loss: 0.000 lr: 0.000955\n",
      " step: 40, acc: 0.453, loss: 1.563,data_loss: 1.563, reg_loss: 0.000 lr: 0.000955\n",
      " step: 41, acc: 0.336, loss: 1.633,data_loss: 1.633, reg_loss: 0.000 lr: 0.000955\n",
      " step: 42, acc: 0.414, loss: 1.546,data_loss: 1.546, reg_loss: 0.000 lr: 0.000955\n",
      " step: 43, acc: 0.312, loss: 1.542,data_loss: 1.542, reg_loss: 0.000 lr: 0.000955\n",
      " step: 44, acc: 0.383, loss: 1.570,data_loss: 1.570, reg_loss: 0.000 lr: 0.000955\n",
      " step: 45, acc: 0.453, loss: 1.415,data_loss: 1.415, reg_loss: 0.000 lr: 0.000955\n",
      " step: 46, acc: 0.453, loss: 1.453,data_loss: 1.453, reg_loss: 0.000 lr: 0.000955\n",
      " step: 47, acc: 0.383, loss: 1.636,data_loss: 1.636, reg_loss: 0.000 lr: 0.000955\n",
      " step: 48, acc: 0.469, loss: 1.384,data_loss: 1.384, reg_loss: 0.000 lr: 0.000955\n",
      " step: 49, acc: 0.469, loss: 1.487,data_loss: 1.487, reg_loss: 0.000 lr: 0.000955\n",
      " step: 50, acc: 0.453, loss: 1.464,data_loss: 1.464, reg_loss: 0.000 lr: 0.000955\n",
      " step: 51, acc: 0.344, loss: 1.604,data_loss: 1.604, reg_loss: 0.000 lr: 0.000955\n",
      " step: 52, acc: 0.352, loss: 1.594,data_loss: 1.594, reg_loss: 0.000 lr: 0.000955\n",
      " step: 53, acc: 0.461, loss: 1.466,data_loss: 1.466, reg_loss: 0.000 lr: 0.000955\n",
      " step: 54, acc: 0.391, loss: 1.587,data_loss: 1.587, reg_loss: 0.000 lr: 0.000954\n",
      " step: 55, acc: 0.438, loss: 1.520,data_loss: 1.520, reg_loss: 0.000 lr: 0.000954\n",
      " step: 56, acc: 0.406, loss: 1.483,data_loss: 1.483, reg_loss: 0.000 lr: 0.000954\n",
      " step: 57, acc: 0.523, loss: 1.416,data_loss: 1.416, reg_loss: 0.000 lr: 0.000954\n",
      " step: 58, acc: 0.477, loss: 1.445,data_loss: 1.445, reg_loss: 0.000 lr: 0.000954\n",
      " step: 59, acc: 0.453, loss: 1.397,data_loss: 1.397, reg_loss: 0.000 lr: 0.000954\n",
      " step: 60, acc: 0.406, loss: 1.480,data_loss: 1.480, reg_loss: 0.000 lr: 0.000954\n",
      " step: 61, acc: 0.414, loss: 1.540,data_loss: 1.540, reg_loss: 0.000 lr: 0.000954\n",
      " step: 62, acc: 0.484, loss: 1.485,data_loss: 1.485, reg_loss: 0.000 lr: 0.000954\n",
      " step: 63, acc: 0.414, loss: 1.535,data_loss: 1.535, reg_loss: 0.000 lr: 0.000954\n",
      " step: 64, acc: 0.461, loss: 1.503,data_loss: 1.503, reg_loss: 0.000 lr: 0.000954\n",
      " step: 65, acc: 0.406, loss: 1.558,data_loss: 1.558, reg_loss: 0.000 lr: 0.000954\n",
      " step: 66, acc: 0.453, loss: 1.424,data_loss: 1.424, reg_loss: 0.000 lr: 0.000954\n",
      " step: 67, acc: 0.438, loss: 1.474,data_loss: 1.474, reg_loss: 0.000 lr: 0.000954\n",
      " step: 68, acc: 0.367, loss: 1.522,data_loss: 1.522, reg_loss: 0.000 lr: 0.000954\n",
      " step: 69, acc: 0.359, loss: 1.547,data_loss: 1.547, reg_loss: 0.000 lr: 0.000954\n",
      " step: 70, acc: 0.438, loss: 1.460,data_loss: 1.460, reg_loss: 0.000 lr: 0.000954\n",
      " step: 71, acc: 0.523, loss: 1.392,data_loss: 1.392, reg_loss: 0.000 lr: 0.000954\n",
      " step: 72, acc: 0.406, loss: 1.527,data_loss: 1.527, reg_loss: 0.000 lr: 0.000954\n",
      " step: 73, acc: 0.406, loss: 1.489,data_loss: 1.489, reg_loss: 0.000 lr: 0.000954\n",
      " step: 74, acc: 0.367, loss: 1.540,data_loss: 1.540, reg_loss: 0.000 lr: 0.000954\n",
      " step: 75, acc: 0.461, loss: 1.383,data_loss: 1.383, reg_loss: 0.000 lr: 0.000954\n",
      " step: 76, acc: 0.430, loss: 1.482,data_loss: 1.482, reg_loss: 0.000 lr: 0.000953\n",
      " step: 77, acc: 0.406, loss: 1.498,data_loss: 1.498, reg_loss: 0.000 lr: 0.000953\n",
      " step: 78, acc: 0.445, loss: 1.375,data_loss: 1.375, reg_loss: 0.000 lr: 0.000953\n",
      " step: 79, acc: 0.398, loss: 1.458,data_loss: 1.458, reg_loss: 0.000 lr: 0.000953\n",
      " step: 80, acc: 0.383, loss: 1.502,data_loss: 1.502, reg_loss: 0.000 lr: 0.000953\n",
      " step: 81, acc: 0.422, loss: 1.572,data_loss: 1.572, reg_loss: 0.000 lr: 0.000953\n",
      " step: 82, acc: 0.492, loss: 1.478,data_loss: 1.478, reg_loss: 0.000 lr: 0.000953\n",
      " step: 83, acc: 0.406, loss: 1.444,data_loss: 1.444, reg_loss: 0.000 lr: 0.000953\n",
      " step: 84, acc: 0.359, loss: 1.534,data_loss: 1.534, reg_loss: 0.000 lr: 0.000953\n",
      " step: 85, acc: 0.430, loss: 1.433,data_loss: 1.433, reg_loss: 0.000 lr: 0.000953\n",
      " step: 86, acc: 0.391, loss: 1.464,data_loss: 1.464, reg_loss: 0.000 lr: 0.000953\n",
      " step: 87, acc: 0.430, loss: 1.470,data_loss: 1.470, reg_loss: 0.000 lr: 0.000953\n",
      " step: 88, acc: 0.430, loss: 1.525,data_loss: 1.525, reg_loss: 0.000 lr: 0.000953\n",
      " step: 89, acc: 0.453, loss: 1.484,data_loss: 1.484, reg_loss: 0.000 lr: 0.000953\n",
      " step: 90, acc: 0.461, loss: 1.531,data_loss: 1.531, reg_loss: 0.000 lr: 0.000953\n",
      " step: 91, acc: 0.438, loss: 1.541,data_loss: 1.541, reg_loss: 0.000 lr: 0.000953\n",
      " step: 92, acc: 0.406, loss: 1.493,data_loss: 1.493, reg_loss: 0.000 lr: 0.000953\n",
      " step: 93, acc: 0.461, loss: 1.443,data_loss: 1.443, reg_loss: 0.000 lr: 0.000953\n",
      " step: 94, acc: 0.336, loss: 1.597,data_loss: 1.597, reg_loss: 0.000 lr: 0.000953\n",
      " step: 95, acc: 0.445, loss: 1.451,data_loss: 1.451, reg_loss: 0.000 lr: 0.000953\n",
      " step: 96, acc: 0.438, loss: 1.463,data_loss: 1.463, reg_loss: 0.000 lr: 0.000953\n",
      " step: 97, acc: 0.383, loss: 1.560,data_loss: 1.560, reg_loss: 0.000 lr: 0.000953\n",
      " step: 98, acc: 0.500, loss: 1.355,data_loss: 1.355, reg_loss: 0.000 lr: 0.000952\n",
      " step: 99, acc: 0.492, loss: 1.337,data_loss: 1.337, reg_loss: 0.000 lr: 0.000952\n",
      " step: 100, acc: 0.406, loss: 1.456,data_loss: 1.456, reg_loss: 0.000 lr: 0.000952\n",
      " step: 101, acc: 0.406, loss: 1.465,data_loss: 1.465, reg_loss: 0.000 lr: 0.000952\n",
      " step: 102, acc: 0.383, loss: 1.515,data_loss: 1.515, reg_loss: 0.000 lr: 0.000952\n",
      " step: 103, acc: 0.469, loss: 1.429,data_loss: 1.429, reg_loss: 0.000 lr: 0.000952\n",
      " step: 104, acc: 0.508, loss: 1.414,data_loss: 1.414, reg_loss: 0.000 lr: 0.000952\n",
      " step: 105, acc: 0.398, loss: 1.477,data_loss: 1.477, reg_loss: 0.000 lr: 0.000952\n",
      " step: 106, acc: 0.461, loss: 1.419,data_loss: 1.419, reg_loss: 0.000 lr: 0.000952\n",
      " step: 107, acc: 0.352, loss: 1.513,data_loss: 1.513, reg_loss: 0.000 lr: 0.000952\n",
      " step: 108, acc: 0.367, loss: 1.567,data_loss: 1.567, reg_loss: 0.000 lr: 0.000952\n",
      " step: 109, acc: 0.422, loss: 1.571,data_loss: 1.571, reg_loss: 0.000 lr: 0.000952\n",
      " step: 110, acc: 0.383, loss: 1.523,data_loss: 1.523, reg_loss: 0.000 lr: 0.000952\n",
      " step: 111, acc: 0.445, loss: 1.462,data_loss: 1.462, reg_loss: 0.000 lr: 0.000952\n",
      " step: 112, acc: 0.398, loss: 1.496,data_loss: 1.496, reg_loss: 0.000 lr: 0.000952\n",
      " step: 113, acc: 0.391, loss: 1.468,data_loss: 1.468, reg_loss: 0.000 lr: 0.000952\n",
      " step: 114, acc: 0.391, loss: 1.535,data_loss: 1.535, reg_loss: 0.000 lr: 0.000952\n",
      " step: 115, acc: 0.398, loss: 1.512,data_loss: 1.512, reg_loss: 0.000 lr: 0.000952\n",
      " step: 116, acc: 0.430, loss: 1.600,data_loss: 1.600, reg_loss: 0.000 lr: 0.000952\n",
      " step: 117, acc: 0.430, loss: 1.445,data_loss: 1.445, reg_loss: 0.000 lr: 0.000952\n",
      " step: 118, acc: 0.492, loss: 1.472,data_loss: 1.472, reg_loss: 0.000 lr: 0.000952\n",
      " step: 119, acc: 0.461, loss: 1.460,data_loss: 1.460, reg_loss: 0.000 lr: 0.000952\n",
      " step: 120, acc: 0.375, loss: 1.487,data_loss: 1.487, reg_loss: 0.000 lr: 0.000951\n",
      " step: 121, acc: 0.367, loss: 1.575,data_loss: 1.575, reg_loss: 0.000 lr: 0.000951\n",
      " step: 122, acc: 0.422, loss: 1.486,data_loss: 1.486, reg_loss: 0.000 lr: 0.000951\n",
      " step: 123, acc: 0.422, loss: 1.523,data_loss: 1.523, reg_loss: 0.000 lr: 0.000951\n",
      " step: 124, acc: 0.398, loss: 1.531,data_loss: 1.531, reg_loss: 0.000 lr: 0.000951\n",
      " step: 125, acc: 0.383, loss: 1.479,data_loss: 1.479, reg_loss: 0.000 lr: 0.000951\n",
      " step: 126, acc: 0.414, loss: 1.542,data_loss: 1.542, reg_loss: 0.000 lr: 0.000951\n",
      " step: 127, acc: 0.359, loss: 1.566,data_loss: 1.566, reg_loss: 0.000 lr: 0.000951\n",
      " step: 128, acc: 0.406, loss: 1.493,data_loss: 1.493, reg_loss: 0.000 lr: 0.000951\n",
      " step: 129, acc: 0.414, loss: 1.535,data_loss: 1.535, reg_loss: 0.000 lr: 0.000951\n",
      " step: 130, acc: 0.312, loss: 1.700,data_loss: 1.700, reg_loss: 0.000 lr: 0.000951\n",
      " step: 131, acc: 0.477, loss: 1.470,data_loss: 1.470, reg_loss: 0.000 lr: 0.000951\n",
      " step: 132, acc: 0.438, loss: 1.453,data_loss: 1.453, reg_loss: 0.000 lr: 0.000951\n",
      " step: 133, acc: 0.445, loss: 1.519,data_loss: 1.519, reg_loss: 0.000 lr: 0.000951\n",
      " step: 134, acc: 0.500, loss: 1.314,data_loss: 1.314, reg_loss: 0.000 lr: 0.000951\n",
      " step: 135, acc: 0.438, loss: 1.454,data_loss: 1.454, reg_loss: 0.000 lr: 0.000951\n",
      " step: 136, acc: 0.391, loss: 1.577,data_loss: 1.577, reg_loss: 0.000 lr: 0.000951\n",
      " step: 137, acc: 0.508, loss: 1.442,data_loss: 1.442, reg_loss: 0.000 lr: 0.000951\n",
      " step: 138, acc: 0.453, loss: 1.407,data_loss: 1.407, reg_loss: 0.000 lr: 0.000951\n",
      " step: 139, acc: 0.492, loss: 1.354,data_loss: 1.354, reg_loss: 0.000 lr: 0.000951\n",
      " step: 140, acc: 0.500, loss: 1.362,data_loss: 1.362, reg_loss: 0.000 lr: 0.000951\n",
      " step: 141, acc: 0.391, loss: 1.480,data_loss: 1.480, reg_loss: 0.000 lr: 0.000951\n",
      " step: 142, acc: 0.469, loss: 1.403,data_loss: 1.403, reg_loss: 0.000 lr: 0.000950\n",
      " step: 143, acc: 0.430, loss: 1.479,data_loss: 1.479, reg_loss: 0.000 lr: 0.000950\n",
      " step: 144, acc: 0.383, loss: 1.598,data_loss: 1.598, reg_loss: 0.000 lr: 0.000950\n",
      " step: 145, acc: 0.461, loss: 1.475,data_loss: 1.475, reg_loss: 0.000 lr: 0.000950\n",
      " step: 146, acc: 0.398, loss: 1.507,data_loss: 1.507, reg_loss: 0.000 lr: 0.000950\n",
      " step: 147, acc: 0.391, loss: 1.475,data_loss: 1.475, reg_loss: 0.000 lr: 0.000950\n",
      " step: 148, acc: 0.445, loss: 1.382,data_loss: 1.382, reg_loss: 0.000 lr: 0.000950\n",
      " step: 149, acc: 0.344, loss: 1.554,data_loss: 1.554, reg_loss: 0.000 lr: 0.000950\n",
      " step: 150, acc: 0.484, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000950\n",
      " step: 151, acc: 0.375, loss: 1.547,data_loss: 1.547, reg_loss: 0.000 lr: 0.000950\n",
      " step: 152, acc: 0.484, loss: 1.428,data_loss: 1.428, reg_loss: 0.000 lr: 0.000950\n",
      " step: 153, acc: 0.477, loss: 1.386,data_loss: 1.386, reg_loss: 0.000 lr: 0.000950\n",
      " step: 154, acc: 0.375, loss: 1.598,data_loss: 1.598, reg_loss: 0.000 lr: 0.000950\n",
      " step: 155, acc: 0.461, loss: 1.443,data_loss: 1.443, reg_loss: 0.000 lr: 0.000950\n",
      " step: 156, acc: 0.477, loss: 1.483,data_loss: 1.483, reg_loss: 0.000 lr: 0.000950\n",
      " step: 157, acc: 0.469, loss: 1.408,data_loss: 1.408, reg_loss: 0.000 lr: 0.000950\n",
      " step: 158, acc: 0.383, loss: 1.560,data_loss: 1.560, reg_loss: 0.000 lr: 0.000950\n",
      " step: 159, acc: 0.391, loss: 1.565,data_loss: 1.565, reg_loss: 0.000 lr: 0.000950\n",
      " step: 160, acc: 0.375, loss: 1.637,data_loss: 1.637, reg_loss: 0.000 lr: 0.000950\n",
      " step: 161, acc: 0.438, loss: 1.471,data_loss: 1.471, reg_loss: 0.000 lr: 0.000950\n",
      " step: 162, acc: 0.430, loss: 1.505,data_loss: 1.505, reg_loss: 0.000 lr: 0.000950\n",
      " step: 163, acc: 0.391, loss: 1.457,data_loss: 1.457, reg_loss: 0.000 lr: 0.000950\n",
      " step: 164, acc: 0.484, loss: 1.415,data_loss: 1.415, reg_loss: 0.000 lr: 0.000949\n",
      " step: 165, acc: 0.344, loss: 1.616,data_loss: 1.616, reg_loss: 0.000 lr: 0.000949\n",
      " step: 166, acc: 0.430, loss: 1.490,data_loss: 1.490, reg_loss: 0.000 lr: 0.000949\n",
      " step: 167, acc: 0.359, loss: 1.512,data_loss: 1.512, reg_loss: 0.000 lr: 0.000949\n",
      " step: 168, acc: 0.422, loss: 1.467,data_loss: 1.467, reg_loss: 0.000 lr: 0.000949\n",
      " step: 169, acc: 0.438, loss: 1.413,data_loss: 1.413, reg_loss: 0.000 lr: 0.000949\n",
      " step: 170, acc: 0.359, loss: 1.618,data_loss: 1.618, reg_loss: 0.000 lr: 0.000949\n",
      " step: 171, acc: 0.469, loss: 1.434,data_loss: 1.434, reg_loss: 0.000 lr: 0.000949\n",
      " step: 172, acc: 0.430, loss: 1.415,data_loss: 1.415, reg_loss: 0.000 lr: 0.000949\n",
      " step: 173, acc: 0.484, loss: 1.439,data_loss: 1.439, reg_loss: 0.000 lr: 0.000949\n",
      " step: 174, acc: 0.445, loss: 1.409,data_loss: 1.409, reg_loss: 0.000 lr: 0.000949\n",
      " step: 175, acc: 0.336, loss: 1.579,data_loss: 1.579, reg_loss: 0.000 lr: 0.000949\n",
      " step: 176, acc: 0.344, loss: 1.614,data_loss: 1.614, reg_loss: 0.000 lr: 0.000949\n",
      " step: 177, acc: 0.391, loss: 1.455,data_loss: 1.455, reg_loss: 0.000 lr: 0.000949\n",
      " step: 178, acc: 0.352, loss: 1.594,data_loss: 1.594, reg_loss: 0.000 lr: 0.000949\n",
      " step: 179, acc: 0.508, loss: 1.288,data_loss: 1.288, reg_loss: 0.000 lr: 0.000949\n",
      " step: 180, acc: 0.508, loss: 1.341,data_loss: 1.341, reg_loss: 0.000 lr: 0.000949\n",
      " step: 181, acc: 0.469, loss: 1.422,data_loss: 1.422, reg_loss: 0.000 lr: 0.000949\n",
      " step: 182, acc: 0.453, loss: 1.427,data_loss: 1.427, reg_loss: 0.000 lr: 0.000949\n",
      " step: 183, acc: 0.422, loss: 1.463,data_loss: 1.463, reg_loss: 0.000 lr: 0.000949\n",
      " step: 184, acc: 0.398, loss: 1.494,data_loss: 1.494, reg_loss: 0.000 lr: 0.000949\n",
      " step: 185, acc: 0.445, loss: 1.534,data_loss: 1.534, reg_loss: 0.000 lr: 0.000949\n",
      " step: 186, acc: 0.406, loss: 1.453,data_loss: 1.453, reg_loss: 0.000 lr: 0.000948\n",
      " step: 187, acc: 0.484, loss: 1.396,data_loss: 1.396, reg_loss: 0.000 lr: 0.000948\n",
      " step: 188, acc: 0.367, loss: 1.464,data_loss: 1.464, reg_loss: 0.000 lr: 0.000948\n",
      " step: 189, acc: 0.391, loss: 1.526,data_loss: 1.526, reg_loss: 0.000 lr: 0.000948\n",
      " step: 190, acc: 0.414, loss: 1.502,data_loss: 1.502, reg_loss: 0.000 lr: 0.000948\n",
      " step: 191, acc: 0.367, loss: 1.588,data_loss: 1.588, reg_loss: 0.000 lr: 0.000948\n",
      " step: 192, acc: 0.445, loss: 1.450,data_loss: 1.450, reg_loss: 0.000 lr: 0.000948\n",
      " step: 193, acc: 0.453, loss: 1.455,data_loss: 1.455, reg_loss: 0.000 lr: 0.000948\n",
      " step: 194, acc: 0.414, loss: 1.495,data_loss: 1.495, reg_loss: 0.000 lr: 0.000948\n",
      " step: 195, acc: 0.469, loss: 1.360,data_loss: 1.360, reg_loss: 0.000 lr: 0.000948\n",
      " step: 196, acc: 0.453, loss: 1.483,data_loss: 1.483, reg_loss: 0.000 lr: 0.000948\n",
      " step: 197, acc: 0.469, loss: 1.438,data_loss: 1.438, reg_loss: 0.000 lr: 0.000948\n",
      " step: 198, acc: 0.508, loss: 1.375,data_loss: 1.375, reg_loss: 0.000 lr: 0.000948\n",
      " step: 199, acc: 0.430, loss: 1.512,data_loss: 1.512, reg_loss: 0.000 lr: 0.000948\n",
      " step: 200, acc: 0.383, loss: 1.508,data_loss: 1.508, reg_loss: 0.000 lr: 0.000948\n",
      " step: 201, acc: 0.453, loss: 1.464,data_loss: 1.464, reg_loss: 0.000 lr: 0.000948\n",
      " step: 202, acc: 0.383, loss: 1.456,data_loss: 1.456, reg_loss: 0.000 lr: 0.000948\n",
      " step: 203, acc: 0.414, loss: 1.533,data_loss: 1.533, reg_loss: 0.000 lr: 0.000948\n",
      " step: 204, acc: 0.477, loss: 1.412,data_loss: 1.412, reg_loss: 0.000 lr: 0.000948\n",
      " step: 205, acc: 0.391, loss: 1.529,data_loss: 1.529, reg_loss: 0.000 lr: 0.000948\n",
      " step: 206, acc: 0.414, loss: 1.415,data_loss: 1.415, reg_loss: 0.000 lr: 0.000948\n",
      " step: 207, acc: 0.477, loss: 1.435,data_loss: 1.435, reg_loss: 0.000 lr: 0.000948\n",
      " step: 208, acc: 0.469, loss: 1.409,data_loss: 1.409, reg_loss: 0.000 lr: 0.000948\n",
      " step: 209, acc: 0.391, loss: 1.488,data_loss: 1.488, reg_loss: 0.000 lr: 0.000947\n",
      " step: 210, acc: 0.484, loss: 1.390,data_loss: 1.390, reg_loss: 0.000 lr: 0.000947\n",
      " step: 211, acc: 0.477, loss: 1.438,data_loss: 1.438, reg_loss: 0.000 lr: 0.000947\n",
      " step: 212, acc: 0.422, loss: 1.450,data_loss: 1.450, reg_loss: 0.000 lr: 0.000947\n",
      " step: 213, acc: 0.406, loss: 1.475,data_loss: 1.475, reg_loss: 0.000 lr: 0.000947\n",
      " step: 214, acc: 0.383, loss: 1.569,data_loss: 1.569, reg_loss: 0.000 lr: 0.000947\n",
      " step: 215, acc: 0.477, loss: 1.378,data_loss: 1.378, reg_loss: 0.000 lr: 0.000947\n",
      " step: 216, acc: 0.422, loss: 1.448,data_loss: 1.448, reg_loss: 0.000 lr: 0.000947\n",
      " step: 217, acc: 0.406, loss: 1.401,data_loss: 1.401, reg_loss: 0.000 lr: 0.000947\n",
      " step: 218, acc: 0.469, loss: 1.472,data_loss: 1.472, reg_loss: 0.000 lr: 0.000947\n",
      " step: 219, acc: 0.453, loss: 1.449,data_loss: 1.449, reg_loss: 0.000 lr: 0.000947\n",
      " step: 220, acc: 0.398, loss: 1.509,data_loss: 1.509, reg_loss: 0.000 lr: 0.000947\n",
      " step: 221, acc: 0.414, loss: 1.543,data_loss: 1.543, reg_loss: 0.000 lr: 0.000947\n",
      " step: 222, acc: 0.406, loss: 1.516,data_loss: 1.516, reg_loss: 0.000 lr: 0.000947\n",
      " step: 223, acc: 0.422, loss: 1.480,data_loss: 1.480, reg_loss: 0.000 lr: 0.000947\n",
      " step: 224, acc: 0.432, loss: 1.486,data_loss: 1.486, reg_loss: 0.000 lr: 0.000947\n",
      "training , acc: 0.422, loss: 1.489,data_loss: 1.489, reg_loss: 0.000 lr: 0.000947\n",
      "validation, acc:0.404 ,loss: 1.529 \n",
      "epoch: 6\n",
      " step: 0, acc: 0.359, loss: 1.638,data_loss: 1.638, reg_loss: 0.000 lr: 0.000947\n",
      " step: 1, acc: 0.414, loss: 1.598,data_loss: 1.598, reg_loss: 0.000 lr: 0.000947\n",
      " step: 2, acc: 0.398, loss: 1.431,data_loss: 1.431, reg_loss: 0.000 lr: 0.000947\n",
      " step: 3, acc: 0.422, loss: 1.483,data_loss: 1.483, reg_loss: 0.000 lr: 0.000947\n",
      " step: 4, acc: 0.484, loss: 1.426,data_loss: 1.426, reg_loss: 0.000 lr: 0.000947\n",
      " step: 5, acc: 0.375, loss: 1.623,data_loss: 1.623, reg_loss: 0.000 lr: 0.000947\n",
      " step: 6, acc: 0.438, loss: 1.439,data_loss: 1.439, reg_loss: 0.000 lr: 0.000946\n",
      " step: 7, acc: 0.406, loss: 1.562,data_loss: 1.562, reg_loss: 0.000 lr: 0.000946\n",
      " step: 8, acc: 0.445, loss: 1.468,data_loss: 1.468, reg_loss: 0.000 lr: 0.000946\n",
      " step: 9, acc: 0.477, loss: 1.421,data_loss: 1.421, reg_loss: 0.000 lr: 0.000946\n",
      " step: 10, acc: 0.477, loss: 1.464,data_loss: 1.464, reg_loss: 0.000 lr: 0.000946\n",
      " step: 11, acc: 0.508, loss: 1.366,data_loss: 1.366, reg_loss: 0.000 lr: 0.000946\n",
      " step: 12, acc: 0.445, loss: 1.422,data_loss: 1.422, reg_loss: 0.000 lr: 0.000946\n",
      " step: 13, acc: 0.414, loss: 1.443,data_loss: 1.443, reg_loss: 0.000 lr: 0.000946\n",
      " step: 14, acc: 0.375, loss: 1.476,data_loss: 1.476, reg_loss: 0.000 lr: 0.000946\n",
      " step: 15, acc: 0.438, loss: 1.407,data_loss: 1.407, reg_loss: 0.000 lr: 0.000946\n",
      " step: 16, acc: 0.398, loss: 1.476,data_loss: 1.476, reg_loss: 0.000 lr: 0.000946\n",
      " step: 17, acc: 0.398, loss: 1.516,data_loss: 1.516, reg_loss: 0.000 lr: 0.000946\n",
      " step: 18, acc: 0.430, loss: 1.560,data_loss: 1.560, reg_loss: 0.000 lr: 0.000946\n",
      " step: 19, acc: 0.422, loss: 1.424,data_loss: 1.424, reg_loss: 0.000 lr: 0.000946\n",
      " step: 20, acc: 0.398, loss: 1.505,data_loss: 1.505, reg_loss: 0.000 lr: 0.000946\n",
      " step: 21, acc: 0.500, loss: 1.392,data_loss: 1.392, reg_loss: 0.000 lr: 0.000946\n",
      " step: 22, acc: 0.438, loss: 1.405,data_loss: 1.405, reg_loss: 0.000 lr: 0.000946\n",
      " step: 23, acc: 0.484, loss: 1.389,data_loss: 1.389, reg_loss: 0.000 lr: 0.000946\n",
      " step: 24, acc: 0.398, loss: 1.407,data_loss: 1.407, reg_loss: 0.000 lr: 0.000946\n",
      " step: 25, acc: 0.508, loss: 1.371,data_loss: 1.371, reg_loss: 0.000 lr: 0.000946\n",
      " step: 26, acc: 0.398, loss: 1.500,data_loss: 1.500, reg_loss: 0.000 lr: 0.000946\n",
      " step: 27, acc: 0.430, loss: 1.455,data_loss: 1.455, reg_loss: 0.000 lr: 0.000946\n",
      " step: 28, acc: 0.383, loss: 1.477,data_loss: 1.477, reg_loss: 0.000 lr: 0.000945\n",
      " step: 29, acc: 0.430, loss: 1.517,data_loss: 1.517, reg_loss: 0.000 lr: 0.000945\n",
      " step: 30, acc: 0.359, loss: 1.676,data_loss: 1.676, reg_loss: 0.000 lr: 0.000945\n",
      " step: 31, acc: 0.461, loss: 1.381,data_loss: 1.381, reg_loss: 0.000 lr: 0.000945\n",
      " step: 32, acc: 0.391, loss: 1.448,data_loss: 1.448, reg_loss: 0.000 lr: 0.000945\n",
      " step: 33, acc: 0.406, loss: 1.532,data_loss: 1.532, reg_loss: 0.000 lr: 0.000945\n",
      " step: 34, acc: 0.414, loss: 1.463,data_loss: 1.463, reg_loss: 0.000 lr: 0.000945\n",
      " step: 35, acc: 0.414, loss: 1.434,data_loss: 1.434, reg_loss: 0.000 lr: 0.000945\n",
      " step: 36, acc: 0.484, loss: 1.379,data_loss: 1.379, reg_loss: 0.000 lr: 0.000945\n",
      " step: 37, acc: 0.383, loss: 1.563,data_loss: 1.563, reg_loss: 0.000 lr: 0.000945\n",
      " step: 38, acc: 0.398, loss: 1.510,data_loss: 1.510, reg_loss: 0.000 lr: 0.000945\n",
      " step: 39, acc: 0.398, loss: 1.467,data_loss: 1.467, reg_loss: 0.000 lr: 0.000945\n",
      " step: 40, acc: 0.461, loss: 1.520,data_loss: 1.520, reg_loss: 0.000 lr: 0.000945\n",
      " step: 41, acc: 0.391, loss: 1.526,data_loss: 1.526, reg_loss: 0.000 lr: 0.000945\n",
      " step: 42, acc: 0.406, loss: 1.510,data_loss: 1.510, reg_loss: 0.000 lr: 0.000945\n",
      " step: 43, acc: 0.406, loss: 1.468,data_loss: 1.468, reg_loss: 0.000 lr: 0.000945\n",
      " step: 44, acc: 0.391, loss: 1.540,data_loss: 1.540, reg_loss: 0.000 lr: 0.000945\n",
      " step: 45, acc: 0.445, loss: 1.398,data_loss: 1.398, reg_loss: 0.000 lr: 0.000945\n",
      " step: 46, acc: 0.406, loss: 1.504,data_loss: 1.504, reg_loss: 0.000 lr: 0.000945\n",
      " step: 47, acc: 0.414, loss: 1.517,data_loss: 1.517, reg_loss: 0.000 lr: 0.000945\n",
      " step: 48, acc: 0.500, loss: 1.307,data_loss: 1.307, reg_loss: 0.000 lr: 0.000945\n",
      " step: 49, acc: 0.477, loss: 1.382,data_loss: 1.382, reg_loss: 0.000 lr: 0.000945\n",
      " step: 50, acc: 0.492, loss: 1.368,data_loss: 1.368, reg_loss: 0.000 lr: 0.000945\n",
      " step: 51, acc: 0.375, loss: 1.610,data_loss: 1.610, reg_loss: 0.000 lr: 0.000944\n",
      " step: 52, acc: 0.383, loss: 1.528,data_loss: 1.528, reg_loss: 0.000 lr: 0.000944\n",
      " step: 53, acc: 0.453, loss: 1.488,data_loss: 1.488, reg_loss: 0.000 lr: 0.000944\n",
      " step: 54, acc: 0.406, loss: 1.584,data_loss: 1.584, reg_loss: 0.000 lr: 0.000944\n",
      " step: 55, acc: 0.414, loss: 1.521,data_loss: 1.521, reg_loss: 0.000 lr: 0.000944\n",
      " step: 56, acc: 0.438, loss: 1.417,data_loss: 1.417, reg_loss: 0.000 lr: 0.000944\n",
      " step: 57, acc: 0.523, loss: 1.381,data_loss: 1.381, reg_loss: 0.000 lr: 0.000944\n",
      " step: 58, acc: 0.484, loss: 1.439,data_loss: 1.439, reg_loss: 0.000 lr: 0.000944\n",
      " step: 59, acc: 0.500, loss: 1.428,data_loss: 1.428, reg_loss: 0.000 lr: 0.000944\n",
      " step: 60, acc: 0.430, loss: 1.412,data_loss: 1.412, reg_loss: 0.000 lr: 0.000944\n",
      " step: 61, acc: 0.359, loss: 1.547,data_loss: 1.547, reg_loss: 0.000 lr: 0.000944\n",
      " step: 62, acc: 0.445, loss: 1.459,data_loss: 1.459, reg_loss: 0.000 lr: 0.000944\n",
      " step: 63, acc: 0.398, loss: 1.492,data_loss: 1.492, reg_loss: 0.000 lr: 0.000944\n",
      " step: 64, acc: 0.453, loss: 1.485,data_loss: 1.485, reg_loss: 0.000 lr: 0.000944\n",
      " step: 65, acc: 0.445, loss: 1.461,data_loss: 1.461, reg_loss: 0.000 lr: 0.000944\n",
      " step: 66, acc: 0.508, loss: 1.323,data_loss: 1.323, reg_loss: 0.000 lr: 0.000944\n",
      " step: 67, acc: 0.422, loss: 1.418,data_loss: 1.418, reg_loss: 0.000 lr: 0.000944\n",
      " step: 68, acc: 0.438, loss: 1.457,data_loss: 1.457, reg_loss: 0.000 lr: 0.000944\n",
      " step: 69, acc: 0.328, loss: 1.524,data_loss: 1.524, reg_loss: 0.000 lr: 0.000944\n",
      " step: 70, acc: 0.461, loss: 1.375,data_loss: 1.375, reg_loss: 0.000 lr: 0.000944\n",
      " step: 71, acc: 0.477, loss: 1.428,data_loss: 1.428, reg_loss: 0.000 lr: 0.000944\n",
      " step: 72, acc: 0.391, loss: 1.404,data_loss: 1.404, reg_loss: 0.000 lr: 0.000944\n",
      " step: 73, acc: 0.367, loss: 1.422,data_loss: 1.422, reg_loss: 0.000 lr: 0.000943\n",
      " step: 74, acc: 0.430, loss: 1.480,data_loss: 1.480, reg_loss: 0.000 lr: 0.000943\n",
      " step: 75, acc: 0.500, loss: 1.370,data_loss: 1.370, reg_loss: 0.000 lr: 0.000943\n",
      " step: 76, acc: 0.406, loss: 1.456,data_loss: 1.456, reg_loss: 0.000 lr: 0.000943\n",
      " step: 77, acc: 0.375, loss: 1.427,data_loss: 1.427, reg_loss: 0.000 lr: 0.000943\n",
      " step: 78, acc: 0.469, loss: 1.345,data_loss: 1.345, reg_loss: 0.000 lr: 0.000943\n",
      " step: 79, acc: 0.422, loss: 1.407,data_loss: 1.407, reg_loss: 0.000 lr: 0.000943\n",
      " step: 80, acc: 0.438, loss: 1.490,data_loss: 1.490, reg_loss: 0.000 lr: 0.000943\n",
      " step: 81, acc: 0.430, loss: 1.534,data_loss: 1.534, reg_loss: 0.000 lr: 0.000943\n",
      " step: 82, acc: 0.508, loss: 1.456,data_loss: 1.456, reg_loss: 0.000 lr: 0.000943\n",
      " step: 83, acc: 0.414, loss: 1.426,data_loss: 1.426, reg_loss: 0.000 lr: 0.000943\n",
      " step: 84, acc: 0.398, loss: 1.467,data_loss: 1.467, reg_loss: 0.000 lr: 0.000943\n",
      " step: 85, acc: 0.414, loss: 1.487,data_loss: 1.487, reg_loss: 0.000 lr: 0.000943\n",
      " step: 86, acc: 0.414, loss: 1.446,data_loss: 1.446, reg_loss: 0.000 lr: 0.000943\n",
      " step: 87, acc: 0.484, loss: 1.403,data_loss: 1.403, reg_loss: 0.000 lr: 0.000943\n",
      " step: 88, acc: 0.406, loss: 1.526,data_loss: 1.526, reg_loss: 0.000 lr: 0.000943\n",
      " step: 89, acc: 0.484, loss: 1.438,data_loss: 1.438, reg_loss: 0.000 lr: 0.000943\n",
      " step: 90, acc: 0.469, loss: 1.423,data_loss: 1.423, reg_loss: 0.000 lr: 0.000943\n",
      " step: 91, acc: 0.359, loss: 1.472,data_loss: 1.472, reg_loss: 0.000 lr: 0.000943\n",
      " step: 92, acc: 0.461, loss: 1.455,data_loss: 1.455, reg_loss: 0.000 lr: 0.000943\n",
      " step: 93, acc: 0.422, loss: 1.413,data_loss: 1.413, reg_loss: 0.000 lr: 0.000943\n",
      " step: 94, acc: 0.336, loss: 1.560,data_loss: 1.560, reg_loss: 0.000 lr: 0.000943\n",
      " step: 95, acc: 0.469, loss: 1.431,data_loss: 1.431, reg_loss: 0.000 lr: 0.000943\n",
      " step: 96, acc: 0.414, loss: 1.458,data_loss: 1.458, reg_loss: 0.000 lr: 0.000942\n",
      " step: 97, acc: 0.445, loss: 1.427,data_loss: 1.427, reg_loss: 0.000 lr: 0.000942\n",
      " step: 98, acc: 0.508, loss: 1.319,data_loss: 1.319, reg_loss: 0.000 lr: 0.000942\n",
      " step: 99, acc: 0.477, loss: 1.315,data_loss: 1.315, reg_loss: 0.000 lr: 0.000942\n",
      " step: 100, acc: 0.391, loss: 1.496,data_loss: 1.496, reg_loss: 0.000 lr: 0.000942\n",
      " step: 101, acc: 0.445, loss: 1.383,data_loss: 1.383, reg_loss: 0.000 lr: 0.000942\n",
      " step: 102, acc: 0.406, loss: 1.446,data_loss: 1.446, reg_loss: 0.000 lr: 0.000942\n",
      " step: 103, acc: 0.492, loss: 1.367,data_loss: 1.367, reg_loss: 0.000 lr: 0.000942\n",
      " step: 104, acc: 0.516, loss: 1.373,data_loss: 1.373, reg_loss: 0.000 lr: 0.000942\n",
      " step: 105, acc: 0.414, loss: 1.479,data_loss: 1.479, reg_loss: 0.000 lr: 0.000942\n",
      " step: 106, acc: 0.461, loss: 1.350,data_loss: 1.350, reg_loss: 0.000 lr: 0.000942\n",
      " step: 107, acc: 0.422, loss: 1.509,data_loss: 1.509, reg_loss: 0.000 lr: 0.000942\n",
      " step: 108, acc: 0.398, loss: 1.505,data_loss: 1.505, reg_loss: 0.000 lr: 0.000942\n",
      " step: 109, acc: 0.367, loss: 1.540,data_loss: 1.540, reg_loss: 0.000 lr: 0.000942\n",
      " step: 110, acc: 0.453, loss: 1.451,data_loss: 1.451, reg_loss: 0.000 lr: 0.000942\n",
      " step: 111, acc: 0.469, loss: 1.459,data_loss: 1.459, reg_loss: 0.000 lr: 0.000942\n",
      " step: 112, acc: 0.430, loss: 1.537,data_loss: 1.537, reg_loss: 0.000 lr: 0.000942\n",
      " step: 113, acc: 0.438, loss: 1.406,data_loss: 1.406, reg_loss: 0.000 lr: 0.000942\n",
      " step: 114, acc: 0.383, loss: 1.528,data_loss: 1.528, reg_loss: 0.000 lr: 0.000942\n",
      " step: 115, acc: 0.398, loss: 1.460,data_loss: 1.460, reg_loss: 0.000 lr: 0.000942\n",
      " step: 116, acc: 0.438, loss: 1.523,data_loss: 1.523, reg_loss: 0.000 lr: 0.000942\n",
      " step: 117, acc: 0.508, loss: 1.339,data_loss: 1.339, reg_loss: 0.000 lr: 0.000942\n",
      " step: 118, acc: 0.445, loss: 1.445,data_loss: 1.445, reg_loss: 0.000 lr: 0.000941\n",
      " step: 119, acc: 0.477, loss: 1.384,data_loss: 1.384, reg_loss: 0.000 lr: 0.000941\n",
      " step: 120, acc: 0.406, loss: 1.506,data_loss: 1.506, reg_loss: 0.000 lr: 0.000941\n",
      " step: 121, acc: 0.422, loss: 1.565,data_loss: 1.565, reg_loss: 0.000 lr: 0.000941\n",
      " step: 122, acc: 0.359, loss: 1.529,data_loss: 1.529, reg_loss: 0.000 lr: 0.000941\n",
      " step: 123, acc: 0.406, loss: 1.485,data_loss: 1.485, reg_loss: 0.000 lr: 0.000941\n",
      " step: 124, acc: 0.406, loss: 1.523,data_loss: 1.523, reg_loss: 0.000 lr: 0.000941\n",
      " step: 125, acc: 0.398, loss: 1.412,data_loss: 1.412, reg_loss: 0.000 lr: 0.000941\n",
      " step: 126, acc: 0.469, loss: 1.494,data_loss: 1.494, reg_loss: 0.000 lr: 0.000941\n",
      " step: 127, acc: 0.398, loss: 1.576,data_loss: 1.576, reg_loss: 0.000 lr: 0.000941\n",
      " step: 128, acc: 0.359, loss: 1.448,data_loss: 1.448, reg_loss: 0.000 lr: 0.000941\n",
      " step: 129, acc: 0.383, loss: 1.524,data_loss: 1.524, reg_loss: 0.000 lr: 0.000941\n",
      " step: 130, acc: 0.383, loss: 1.590,data_loss: 1.590, reg_loss: 0.000 lr: 0.000941\n",
      " step: 131, acc: 0.461, loss: 1.471,data_loss: 1.471, reg_loss: 0.000 lr: 0.000941\n",
      " step: 132, acc: 0.398, loss: 1.438,data_loss: 1.438, reg_loss: 0.000 lr: 0.000941\n",
      " step: 133, acc: 0.422, loss: 1.533,data_loss: 1.533, reg_loss: 0.000 lr: 0.000941\n",
      " step: 134, acc: 0.555, loss: 1.289,data_loss: 1.289, reg_loss: 0.000 lr: 0.000941\n",
      " step: 135, acc: 0.484, loss: 1.402,data_loss: 1.402, reg_loss: 0.000 lr: 0.000941\n",
      " step: 136, acc: 0.398, loss: 1.548,data_loss: 1.548, reg_loss: 0.000 lr: 0.000941\n",
      " step: 137, acc: 0.477, loss: 1.431,data_loss: 1.431, reg_loss: 0.000 lr: 0.000941\n",
      " step: 138, acc: 0.469, loss: 1.396,data_loss: 1.396, reg_loss: 0.000 lr: 0.000941\n",
      " step: 139, acc: 0.547, loss: 1.264,data_loss: 1.264, reg_loss: 0.000 lr: 0.000941\n",
      " step: 140, acc: 0.492, loss: 1.373,data_loss: 1.373, reg_loss: 0.000 lr: 0.000941\n",
      " step: 141, acc: 0.430, loss: 1.461,data_loss: 1.461, reg_loss: 0.000 lr: 0.000940\n",
      " step: 142, acc: 0.508, loss: 1.360,data_loss: 1.360, reg_loss: 0.000 lr: 0.000940\n",
      " step: 143, acc: 0.414, loss: 1.509,data_loss: 1.509, reg_loss: 0.000 lr: 0.000940\n",
      " step: 144, acc: 0.375, loss: 1.575,data_loss: 1.575, reg_loss: 0.000 lr: 0.000940\n",
      " step: 145, acc: 0.484, loss: 1.478,data_loss: 1.478, reg_loss: 0.000 lr: 0.000940\n",
      " step: 146, acc: 0.422, loss: 1.485,data_loss: 1.485, reg_loss: 0.000 lr: 0.000940\n",
      " step: 147, acc: 0.406, loss: 1.443,data_loss: 1.443, reg_loss: 0.000 lr: 0.000940\n",
      " step: 148, acc: 0.406, loss: 1.403,data_loss: 1.403, reg_loss: 0.000 lr: 0.000940\n",
      " step: 149, acc: 0.453, loss: 1.469,data_loss: 1.469, reg_loss: 0.000 lr: 0.000940\n",
      " step: 150, acc: 0.516, loss: 1.285,data_loss: 1.285, reg_loss: 0.000 lr: 0.000940\n",
      " step: 151, acc: 0.461, loss: 1.497,data_loss: 1.497, reg_loss: 0.000 lr: 0.000940\n",
      " step: 152, acc: 0.492, loss: 1.353,data_loss: 1.353, reg_loss: 0.000 lr: 0.000940\n",
      " step: 153, acc: 0.453, loss: 1.331,data_loss: 1.331, reg_loss: 0.000 lr: 0.000940\n",
      " step: 154, acc: 0.375, loss: 1.561,data_loss: 1.561, reg_loss: 0.000 lr: 0.000940\n",
      " step: 155, acc: 0.500, loss: 1.369,data_loss: 1.369, reg_loss: 0.000 lr: 0.000940\n",
      " step: 156, acc: 0.375, loss: 1.454,data_loss: 1.454, reg_loss: 0.000 lr: 0.000940\n",
      " step: 157, acc: 0.508, loss: 1.327,data_loss: 1.327, reg_loss: 0.000 lr: 0.000940\n",
      " step: 158, acc: 0.391, loss: 1.488,data_loss: 1.488, reg_loss: 0.000 lr: 0.000940\n",
      " step: 159, acc: 0.406, loss: 1.521,data_loss: 1.521, reg_loss: 0.000 lr: 0.000940\n",
      " step: 160, acc: 0.398, loss: 1.630,data_loss: 1.630, reg_loss: 0.000 lr: 0.000940\n",
      " step: 161, acc: 0.523, loss: 1.484,data_loss: 1.484, reg_loss: 0.000 lr: 0.000940\n",
      " step: 162, acc: 0.484, loss: 1.408,data_loss: 1.408, reg_loss: 0.000 lr: 0.000940\n",
      " step: 163, acc: 0.344, loss: 1.463,data_loss: 1.463, reg_loss: 0.000 lr: 0.000939\n",
      " step: 164, acc: 0.461, loss: 1.422,data_loss: 1.422, reg_loss: 0.000 lr: 0.000939\n",
      " step: 165, acc: 0.375, loss: 1.579,data_loss: 1.579, reg_loss: 0.000 lr: 0.000939\n",
      " step: 166, acc: 0.453, loss: 1.515,data_loss: 1.515, reg_loss: 0.000 lr: 0.000939\n",
      " step: 167, acc: 0.398, loss: 1.455,data_loss: 1.455, reg_loss: 0.000 lr: 0.000939\n",
      " step: 168, acc: 0.414, loss: 1.369,data_loss: 1.369, reg_loss: 0.000 lr: 0.000939\n",
      " step: 169, acc: 0.438, loss: 1.418,data_loss: 1.418, reg_loss: 0.000 lr: 0.000939\n",
      " step: 170, acc: 0.398, loss: 1.561,data_loss: 1.561, reg_loss: 0.000 lr: 0.000939\n",
      " step: 171, acc: 0.508, loss: 1.327,data_loss: 1.327, reg_loss: 0.000 lr: 0.000939\n",
      " step: 172, acc: 0.477, loss: 1.462,data_loss: 1.462, reg_loss: 0.000 lr: 0.000939\n",
      " step: 173, acc: 0.508, loss: 1.352,data_loss: 1.352, reg_loss: 0.000 lr: 0.000939\n",
      " step: 174, acc: 0.461, loss: 1.400,data_loss: 1.400, reg_loss: 0.000 lr: 0.000939\n",
      " step: 175, acc: 0.461, loss: 1.484,data_loss: 1.484, reg_loss: 0.000 lr: 0.000939\n",
      " step: 176, acc: 0.438, loss: 1.539,data_loss: 1.539, reg_loss: 0.000 lr: 0.000939\n",
      " step: 177, acc: 0.438, loss: 1.440,data_loss: 1.440, reg_loss: 0.000 lr: 0.000939\n",
      " step: 178, acc: 0.391, loss: 1.629,data_loss: 1.629, reg_loss: 0.000 lr: 0.000939\n",
      " step: 179, acc: 0.547, loss: 1.278,data_loss: 1.278, reg_loss: 0.000 lr: 0.000939\n",
      " step: 180, acc: 0.531, loss: 1.261,data_loss: 1.261, reg_loss: 0.000 lr: 0.000939\n",
      " step: 181, acc: 0.430, loss: 1.443,data_loss: 1.443, reg_loss: 0.000 lr: 0.000939\n",
      " step: 182, acc: 0.469, loss: 1.328,data_loss: 1.328, reg_loss: 0.000 lr: 0.000939\n",
      " step: 183, acc: 0.492, loss: 1.344,data_loss: 1.344, reg_loss: 0.000 lr: 0.000939\n",
      " step: 184, acc: 0.375, loss: 1.495,data_loss: 1.495, reg_loss: 0.000 lr: 0.000939\n",
      " step: 185, acc: 0.367, loss: 1.496,data_loss: 1.496, reg_loss: 0.000 lr: 0.000939\n",
      " step: 186, acc: 0.367, loss: 1.501,data_loss: 1.501, reg_loss: 0.000 lr: 0.000938\n",
      " step: 187, acc: 0.422, loss: 1.439,data_loss: 1.439, reg_loss: 0.000 lr: 0.000938\n",
      " step: 188, acc: 0.398, loss: 1.488,data_loss: 1.488, reg_loss: 0.000 lr: 0.000938\n",
      " step: 189, acc: 0.492, loss: 1.445,data_loss: 1.445, reg_loss: 0.000 lr: 0.000938\n",
      " step: 190, acc: 0.422, loss: 1.456,data_loss: 1.456, reg_loss: 0.000 lr: 0.000938\n",
      " step: 191, acc: 0.406, loss: 1.554,data_loss: 1.554, reg_loss: 0.000 lr: 0.000938\n",
      " step: 192, acc: 0.453, loss: 1.469,data_loss: 1.469, reg_loss: 0.000 lr: 0.000938\n",
      " step: 193, acc: 0.422, loss: 1.441,data_loss: 1.441, reg_loss: 0.000 lr: 0.000938\n",
      " step: 194, acc: 0.484, loss: 1.434,data_loss: 1.434, reg_loss: 0.000 lr: 0.000938\n",
      " step: 195, acc: 0.500, loss: 1.385,data_loss: 1.385, reg_loss: 0.000 lr: 0.000938\n",
      " step: 196, acc: 0.398, loss: 1.442,data_loss: 1.442, reg_loss: 0.000 lr: 0.000938\n",
      " step: 197, acc: 0.398, loss: 1.448,data_loss: 1.448, reg_loss: 0.000 lr: 0.000938\n",
      " step: 198, acc: 0.453, loss: 1.359,data_loss: 1.359, reg_loss: 0.000 lr: 0.000938\n",
      " step: 199, acc: 0.414, loss: 1.553,data_loss: 1.553, reg_loss: 0.000 lr: 0.000938\n",
      " step: 200, acc: 0.406, loss: 1.441,data_loss: 1.441, reg_loss: 0.000 lr: 0.000938\n",
      " step: 201, acc: 0.469, loss: 1.399,data_loss: 1.399, reg_loss: 0.000 lr: 0.000938\n",
      " step: 202, acc: 0.414, loss: 1.402,data_loss: 1.402, reg_loss: 0.000 lr: 0.000938\n",
      " step: 203, acc: 0.391, loss: 1.483,data_loss: 1.483, reg_loss: 0.000 lr: 0.000938\n",
      " step: 204, acc: 0.445, loss: 1.393,data_loss: 1.393, reg_loss: 0.000 lr: 0.000938\n",
      " step: 205, acc: 0.398, loss: 1.433,data_loss: 1.433, reg_loss: 0.000 lr: 0.000938\n",
      " step: 206, acc: 0.445, loss: 1.433,data_loss: 1.433, reg_loss: 0.000 lr: 0.000938\n",
      " step: 207, acc: 0.430, loss: 1.382,data_loss: 1.382, reg_loss: 0.000 lr: 0.000938\n",
      " step: 208, acc: 0.508, loss: 1.401,data_loss: 1.401, reg_loss: 0.000 lr: 0.000938\n",
      " step: 209, acc: 0.414, loss: 1.478,data_loss: 1.478, reg_loss: 0.000 lr: 0.000937\n",
      " step: 210, acc: 0.453, loss: 1.314,data_loss: 1.314, reg_loss: 0.000 lr: 0.000937\n",
      " step: 211, acc: 0.484, loss: 1.380,data_loss: 1.380, reg_loss: 0.000 lr: 0.000937\n",
      " step: 212, acc: 0.492, loss: 1.341,data_loss: 1.341, reg_loss: 0.000 lr: 0.000937\n",
      " step: 213, acc: 0.438, loss: 1.500,data_loss: 1.500, reg_loss: 0.000 lr: 0.000937\n",
      " step: 214, acc: 0.359, loss: 1.581,data_loss: 1.581, reg_loss: 0.000 lr: 0.000937\n",
      " step: 215, acc: 0.539, loss: 1.344,data_loss: 1.344, reg_loss: 0.000 lr: 0.000937\n",
      " step: 216, acc: 0.406, loss: 1.409,data_loss: 1.409, reg_loss: 0.000 lr: 0.000937\n",
      " step: 217, acc: 0.375, loss: 1.461,data_loss: 1.461, reg_loss: 0.000 lr: 0.000937\n",
      " step: 218, acc: 0.477, loss: 1.426,data_loss: 1.426, reg_loss: 0.000 lr: 0.000937\n",
      " step: 219, acc: 0.469, loss: 1.431,data_loss: 1.431, reg_loss: 0.000 lr: 0.000937\n",
      " step: 220, acc: 0.445, loss: 1.437,data_loss: 1.437, reg_loss: 0.000 lr: 0.000937\n",
      " step: 221, acc: 0.375, loss: 1.512,data_loss: 1.512, reg_loss: 0.000 lr: 0.000937\n",
      " step: 222, acc: 0.414, loss: 1.478,data_loss: 1.478, reg_loss: 0.000 lr: 0.000937\n",
      " step: 223, acc: 0.422, loss: 1.467,data_loss: 1.467, reg_loss: 0.000 lr: 0.000937\n",
      " step: 224, acc: 0.459, loss: 1.428,data_loss: 1.428, reg_loss: 0.000 lr: 0.000937\n",
      "training , acc: 0.435, loss: 1.453,data_loss: 1.453, reg_loss: 0.000 lr: 0.000937\n",
      "validation, acc:0.407 ,loss: 1.520 \n",
      "epoch: 7\n",
      " step: 0, acc: 0.367, loss: 1.668,data_loss: 1.668, reg_loss: 0.000 lr: 0.000937\n",
      " step: 1, acc: 0.422, loss: 1.528,data_loss: 1.528, reg_loss: 0.000 lr: 0.000937\n",
      " step: 2, acc: 0.414, loss: 1.429,data_loss: 1.429, reg_loss: 0.000 lr: 0.000937\n",
      " step: 3, acc: 0.414, loss: 1.457,data_loss: 1.457, reg_loss: 0.000 lr: 0.000937\n",
      " step: 4, acc: 0.555, loss: 1.315,data_loss: 1.315, reg_loss: 0.000 lr: 0.000937\n",
      " step: 5, acc: 0.367, loss: 1.557,data_loss: 1.557, reg_loss: 0.000 lr: 0.000937\n",
      " step: 6, acc: 0.453, loss: 1.359,data_loss: 1.359, reg_loss: 0.000 lr: 0.000937\n",
      " step: 7, acc: 0.422, loss: 1.564,data_loss: 1.564, reg_loss: 0.000 lr: 0.000936\n",
      " step: 8, acc: 0.469, loss: 1.420,data_loss: 1.420, reg_loss: 0.000 lr: 0.000936\n",
      " step: 9, acc: 0.469, loss: 1.407,data_loss: 1.407, reg_loss: 0.000 lr: 0.000936\n",
      " step: 10, acc: 0.531, loss: 1.332,data_loss: 1.332, reg_loss: 0.000 lr: 0.000936\n",
      " step: 11, acc: 0.484, loss: 1.356,data_loss: 1.356, reg_loss: 0.000 lr: 0.000936\n",
      " step: 12, acc: 0.438, loss: 1.427,data_loss: 1.427, reg_loss: 0.000 lr: 0.000936\n",
      " step: 13, acc: 0.453, loss: 1.404,data_loss: 1.404, reg_loss: 0.000 lr: 0.000936\n",
      " step: 14, acc: 0.383, loss: 1.472,data_loss: 1.472, reg_loss: 0.000 lr: 0.000936\n",
      " step: 15, acc: 0.422, loss: 1.364,data_loss: 1.364, reg_loss: 0.000 lr: 0.000936\n",
      " step: 16, acc: 0.453, loss: 1.398,data_loss: 1.398, reg_loss: 0.000 lr: 0.000936\n",
      " step: 17, acc: 0.438, loss: 1.509,data_loss: 1.509, reg_loss: 0.000 lr: 0.000936\n",
      " step: 18, acc: 0.383, loss: 1.465,data_loss: 1.465, reg_loss: 0.000 lr: 0.000936\n",
      " step: 19, acc: 0.469, loss: 1.295,data_loss: 1.295, reg_loss: 0.000 lr: 0.000936\n",
      " step: 20, acc: 0.438, loss: 1.527,data_loss: 1.527, reg_loss: 0.000 lr: 0.000936\n",
      " step: 21, acc: 0.492, loss: 1.391,data_loss: 1.391, reg_loss: 0.000 lr: 0.000936\n",
      " step: 22, acc: 0.477, loss: 1.401,data_loss: 1.401, reg_loss: 0.000 lr: 0.000936\n",
      " step: 23, acc: 0.461, loss: 1.453,data_loss: 1.453, reg_loss: 0.000 lr: 0.000936\n",
      " step: 24, acc: 0.422, loss: 1.374,data_loss: 1.374, reg_loss: 0.000 lr: 0.000936\n",
      " step: 25, acc: 0.492, loss: 1.386,data_loss: 1.386, reg_loss: 0.000 lr: 0.000936\n",
      " step: 26, acc: 0.398, loss: 1.418,data_loss: 1.418, reg_loss: 0.000 lr: 0.000936\n",
      " step: 27, acc: 0.461, loss: 1.428,data_loss: 1.428, reg_loss: 0.000 lr: 0.000936\n",
      " step: 28, acc: 0.359, loss: 1.451,data_loss: 1.451, reg_loss: 0.000 lr: 0.000936\n",
      " step: 29, acc: 0.414, loss: 1.440,data_loss: 1.440, reg_loss: 0.000 lr: 0.000935\n",
      " step: 30, acc: 0.383, loss: 1.566,data_loss: 1.566, reg_loss: 0.000 lr: 0.000935\n",
      " step: 31, acc: 0.469, loss: 1.312,data_loss: 1.312, reg_loss: 0.000 lr: 0.000935\n",
      " step: 32, acc: 0.438, loss: 1.391,data_loss: 1.391, reg_loss: 0.000 lr: 0.000935\n",
      " step: 33, acc: 0.430, loss: 1.527,data_loss: 1.527, reg_loss: 0.000 lr: 0.000935\n",
      " step: 34, acc: 0.461, loss: 1.422,data_loss: 1.422, reg_loss: 0.000 lr: 0.000935\n",
      " step: 35, acc: 0.469, loss: 1.398,data_loss: 1.398, reg_loss: 0.000 lr: 0.000935\n",
      " step: 36, acc: 0.453, loss: 1.324,data_loss: 1.324, reg_loss: 0.000 lr: 0.000935\n",
      " step: 37, acc: 0.406, loss: 1.471,data_loss: 1.471, reg_loss: 0.000 lr: 0.000935\n",
      " step: 38, acc: 0.453, loss: 1.498,data_loss: 1.498, reg_loss: 0.000 lr: 0.000935\n",
      " step: 39, acc: 0.375, loss: 1.476,data_loss: 1.476, reg_loss: 0.000 lr: 0.000935\n",
      " step: 40, acc: 0.477, loss: 1.434,data_loss: 1.434, reg_loss: 0.000 lr: 0.000935\n",
      " step: 41, acc: 0.375, loss: 1.564,data_loss: 1.564, reg_loss: 0.000 lr: 0.000935\n",
      " step: 42, acc: 0.430, loss: 1.488,data_loss: 1.488, reg_loss: 0.000 lr: 0.000935\n",
      " step: 43, acc: 0.422, loss: 1.434,data_loss: 1.434, reg_loss: 0.000 lr: 0.000935\n",
      " step: 44, acc: 0.406, loss: 1.487,data_loss: 1.487, reg_loss: 0.000 lr: 0.000935\n",
      " step: 45, acc: 0.461, loss: 1.374,data_loss: 1.374, reg_loss: 0.000 lr: 0.000935\n",
      " step: 46, acc: 0.453, loss: 1.409,data_loss: 1.409, reg_loss: 0.000 lr: 0.000935\n",
      " step: 47, acc: 0.445, loss: 1.453,data_loss: 1.453, reg_loss: 0.000 lr: 0.000935\n",
      " step: 48, acc: 0.500, loss: 1.332,data_loss: 1.332, reg_loss: 0.000 lr: 0.000935\n",
      " step: 49, acc: 0.484, loss: 1.388,data_loss: 1.388, reg_loss: 0.000 lr: 0.000935\n",
      " step: 50, acc: 0.461, loss: 1.326,data_loss: 1.326, reg_loss: 0.000 lr: 0.000935\n",
      " step: 51, acc: 0.367, loss: 1.627,data_loss: 1.627, reg_loss: 0.000 lr: 0.000935\n",
      " step: 52, acc: 0.383, loss: 1.592,data_loss: 1.592, reg_loss: 0.000 lr: 0.000934\n",
      " step: 53, acc: 0.461, loss: 1.434,data_loss: 1.434, reg_loss: 0.000 lr: 0.000934\n",
      " step: 54, acc: 0.359, loss: 1.521,data_loss: 1.521, reg_loss: 0.000 lr: 0.000934\n",
      " step: 55, acc: 0.422, loss: 1.420,data_loss: 1.420, reg_loss: 0.000 lr: 0.000934\n",
      " step: 56, acc: 0.500, loss: 1.370,data_loss: 1.370, reg_loss: 0.000 lr: 0.000934\n",
      " step: 57, acc: 0.516, loss: 1.314,data_loss: 1.314, reg_loss: 0.000 lr: 0.000934\n",
      " step: 58, acc: 0.492, loss: 1.417,data_loss: 1.417, reg_loss: 0.000 lr: 0.000934\n",
      " step: 59, acc: 0.516, loss: 1.397,data_loss: 1.397, reg_loss: 0.000 lr: 0.000934\n",
      " step: 60, acc: 0.469, loss: 1.412,data_loss: 1.412, reg_loss: 0.000 lr: 0.000934\n",
      " step: 61, acc: 0.438, loss: 1.424,data_loss: 1.424, reg_loss: 0.000 lr: 0.000934\n",
      " step: 62, acc: 0.422, loss: 1.444,data_loss: 1.444, reg_loss: 0.000 lr: 0.000934\n",
      " step: 63, acc: 0.438, loss: 1.422,data_loss: 1.422, reg_loss: 0.000 lr: 0.000934\n",
      " step: 64, acc: 0.430, loss: 1.473,data_loss: 1.473, reg_loss: 0.000 lr: 0.000934\n",
      " step: 65, acc: 0.430, loss: 1.459,data_loss: 1.459, reg_loss: 0.000 lr: 0.000934\n",
      " step: 66, acc: 0.477, loss: 1.325,data_loss: 1.325, reg_loss: 0.000 lr: 0.000934\n",
      " step: 67, acc: 0.453, loss: 1.394,data_loss: 1.394, reg_loss: 0.000 lr: 0.000934\n",
      " step: 68, acc: 0.367, loss: 1.487,data_loss: 1.487, reg_loss: 0.000 lr: 0.000934\n",
      " step: 69, acc: 0.352, loss: 1.498,data_loss: 1.498, reg_loss: 0.000 lr: 0.000934\n",
      " step: 70, acc: 0.430, loss: 1.418,data_loss: 1.418, reg_loss: 0.000 lr: 0.000934\n",
      " step: 71, acc: 0.523, loss: 1.361,data_loss: 1.361, reg_loss: 0.000 lr: 0.000934\n",
      " step: 72, acc: 0.406, loss: 1.367,data_loss: 1.367, reg_loss: 0.000 lr: 0.000934\n",
      " step: 73, acc: 0.445, loss: 1.357,data_loss: 1.357, reg_loss: 0.000 lr: 0.000934\n",
      " step: 74, acc: 0.453, loss: 1.480,data_loss: 1.480, reg_loss: 0.000 lr: 0.000934\n",
      " step: 75, acc: 0.391, loss: 1.412,data_loss: 1.412, reg_loss: 0.000 lr: 0.000933\n",
      " step: 76, acc: 0.508, loss: 1.359,data_loss: 1.359, reg_loss: 0.000 lr: 0.000933\n",
      " step: 77, acc: 0.406, loss: 1.386,data_loss: 1.386, reg_loss: 0.000 lr: 0.000933\n",
      " step: 78, acc: 0.492, loss: 1.332,data_loss: 1.332, reg_loss: 0.000 lr: 0.000933\n",
      " step: 79, acc: 0.422, loss: 1.439,data_loss: 1.439, reg_loss: 0.000 lr: 0.000933\n",
      " step: 80, acc: 0.445, loss: 1.451,data_loss: 1.451, reg_loss: 0.000 lr: 0.000933\n",
      " step: 81, acc: 0.438, loss: 1.468,data_loss: 1.468, reg_loss: 0.000 lr: 0.000933\n",
      " step: 82, acc: 0.516, loss: 1.408,data_loss: 1.408, reg_loss: 0.000 lr: 0.000933\n",
      " step: 83, acc: 0.430, loss: 1.379,data_loss: 1.379, reg_loss: 0.000 lr: 0.000933\n",
      " step: 84, acc: 0.445, loss: 1.418,data_loss: 1.418, reg_loss: 0.000 lr: 0.000933\n",
      " step: 85, acc: 0.445, loss: 1.397,data_loss: 1.397, reg_loss: 0.000 lr: 0.000933\n",
      " step: 86, acc: 0.453, loss: 1.367,data_loss: 1.367, reg_loss: 0.000 lr: 0.000933\n",
      " step: 87, acc: 0.500, loss: 1.417,data_loss: 1.417, reg_loss: 0.000 lr: 0.000933\n",
      " step: 88, acc: 0.430, loss: 1.504,data_loss: 1.504, reg_loss: 0.000 lr: 0.000933\n",
      " step: 89, acc: 0.453, loss: 1.441,data_loss: 1.441, reg_loss: 0.000 lr: 0.000933\n",
      " step: 90, acc: 0.414, loss: 1.397,data_loss: 1.397, reg_loss: 0.000 lr: 0.000933\n",
      " step: 91, acc: 0.375, loss: 1.475,data_loss: 1.475, reg_loss: 0.000 lr: 0.000933\n",
      " step: 92, acc: 0.414, loss: 1.428,data_loss: 1.428, reg_loss: 0.000 lr: 0.000933\n",
      " step: 93, acc: 0.539, loss: 1.352,data_loss: 1.352, reg_loss: 0.000 lr: 0.000933\n",
      " step: 94, acc: 0.344, loss: 1.579,data_loss: 1.579, reg_loss: 0.000 lr: 0.000933\n",
      " step: 95, acc: 0.453, loss: 1.370,data_loss: 1.370, reg_loss: 0.000 lr: 0.000933\n",
      " step: 96, acc: 0.422, loss: 1.448,data_loss: 1.448, reg_loss: 0.000 lr: 0.000933\n",
      " step: 97, acc: 0.422, loss: 1.425,data_loss: 1.425, reg_loss: 0.000 lr: 0.000933\n",
      " step: 98, acc: 0.539, loss: 1.295,data_loss: 1.295, reg_loss: 0.000 lr: 0.000932\n",
      " step: 99, acc: 0.547, loss: 1.280,data_loss: 1.280, reg_loss: 0.000 lr: 0.000932\n",
      " step: 100, acc: 0.445, loss: 1.388,data_loss: 1.388, reg_loss: 0.000 lr: 0.000932\n",
      " step: 101, acc: 0.398, loss: 1.421,data_loss: 1.421, reg_loss: 0.000 lr: 0.000932\n",
      " step: 102, acc: 0.414, loss: 1.435,data_loss: 1.435, reg_loss: 0.000 lr: 0.000932\n",
      " step: 103, acc: 0.500, loss: 1.375,data_loss: 1.375, reg_loss: 0.000 lr: 0.000932\n",
      " step: 104, acc: 0.484, loss: 1.332,data_loss: 1.332, reg_loss: 0.000 lr: 0.000932\n",
      " step: 105, acc: 0.422, loss: 1.375,data_loss: 1.375, reg_loss: 0.000 lr: 0.000932\n",
      " step: 106, acc: 0.453, loss: 1.346,data_loss: 1.346, reg_loss: 0.000 lr: 0.000932\n",
      " step: 107, acc: 0.438, loss: 1.439,data_loss: 1.439, reg_loss: 0.000 lr: 0.000932\n",
      " step: 108, acc: 0.352, loss: 1.455,data_loss: 1.455, reg_loss: 0.000 lr: 0.000932\n",
      " step: 109, acc: 0.461, loss: 1.415,data_loss: 1.415, reg_loss: 0.000 lr: 0.000932\n",
      " step: 110, acc: 0.445, loss: 1.436,data_loss: 1.436, reg_loss: 0.000 lr: 0.000932\n",
      " step: 111, acc: 0.430, loss: 1.390,data_loss: 1.390, reg_loss: 0.000 lr: 0.000932\n",
      " step: 112, acc: 0.422, loss: 1.441,data_loss: 1.441, reg_loss: 0.000 lr: 0.000932\n",
      " step: 113, acc: 0.461, loss: 1.374,data_loss: 1.374, reg_loss: 0.000 lr: 0.000932\n",
      " step: 114, acc: 0.430, loss: 1.462,data_loss: 1.462, reg_loss: 0.000 lr: 0.000932\n",
      " step: 115, acc: 0.391, loss: 1.463,data_loss: 1.463, reg_loss: 0.000 lr: 0.000932\n",
      " step: 116, acc: 0.469, loss: 1.504,data_loss: 1.504, reg_loss: 0.000 lr: 0.000932\n",
      " step: 117, acc: 0.500, loss: 1.369,data_loss: 1.369, reg_loss: 0.000 lr: 0.000932\n",
      " step: 118, acc: 0.430, loss: 1.357,data_loss: 1.357, reg_loss: 0.000 lr: 0.000932\n",
      " step: 119, acc: 0.516, loss: 1.363,data_loss: 1.363, reg_loss: 0.000 lr: 0.000932\n",
      " step: 120, acc: 0.422, loss: 1.457,data_loss: 1.457, reg_loss: 0.000 lr: 0.000932\n",
      " step: 121, acc: 0.445, loss: 1.445,data_loss: 1.445, reg_loss: 0.000 lr: 0.000931\n",
      " step: 122, acc: 0.414, loss: 1.421,data_loss: 1.421, reg_loss: 0.000 lr: 0.000931\n",
      " step: 123, acc: 0.406, loss: 1.442,data_loss: 1.442, reg_loss: 0.000 lr: 0.000931\n",
      " step: 124, acc: 0.383, loss: 1.461,data_loss: 1.461, reg_loss: 0.000 lr: 0.000931\n",
      " step: 125, acc: 0.422, loss: 1.419,data_loss: 1.419, reg_loss: 0.000 lr: 0.000931\n",
      " step: 126, acc: 0.430, loss: 1.432,data_loss: 1.432, reg_loss: 0.000 lr: 0.000931\n",
      " step: 127, acc: 0.422, loss: 1.472,data_loss: 1.472, reg_loss: 0.000 lr: 0.000931\n",
      " step: 128, acc: 0.422, loss: 1.428,data_loss: 1.428, reg_loss: 0.000 lr: 0.000931\n",
      " step: 129, acc: 0.414, loss: 1.427,data_loss: 1.427, reg_loss: 0.000 lr: 0.000931\n",
      " step: 130, acc: 0.344, loss: 1.587,data_loss: 1.587, reg_loss: 0.000 lr: 0.000931\n",
      " step: 131, acc: 0.477, loss: 1.393,data_loss: 1.393, reg_loss: 0.000 lr: 0.000931\n",
      " step: 132, acc: 0.430, loss: 1.405,data_loss: 1.405, reg_loss: 0.000 lr: 0.000931\n",
      " step: 133, acc: 0.461, loss: 1.464,data_loss: 1.464, reg_loss: 0.000 lr: 0.000931\n",
      " step: 134, acc: 0.539, loss: 1.203,data_loss: 1.203, reg_loss: 0.000 lr: 0.000931\n",
      " step: 135, acc: 0.469, loss: 1.386,data_loss: 1.386, reg_loss: 0.000 lr: 0.000931\n",
      " step: 136, acc: 0.422, loss: 1.512,data_loss: 1.512, reg_loss: 0.000 lr: 0.000931\n",
      " step: 137, acc: 0.602, loss: 1.322,data_loss: 1.322, reg_loss: 0.000 lr: 0.000931\n",
      " step: 138, acc: 0.445, loss: 1.347,data_loss: 1.347, reg_loss: 0.000 lr: 0.000931\n",
      " step: 139, acc: 0.500, loss: 1.321,data_loss: 1.321, reg_loss: 0.000 lr: 0.000931\n",
      " step: 140, acc: 0.508, loss: 1.261,data_loss: 1.261, reg_loss: 0.000 lr: 0.000931\n",
      " step: 141, acc: 0.367, loss: 1.493,data_loss: 1.493, reg_loss: 0.000 lr: 0.000931\n",
      " step: 142, acc: 0.500, loss: 1.345,data_loss: 1.345, reg_loss: 0.000 lr: 0.000931\n",
      " step: 143, acc: 0.422, loss: 1.462,data_loss: 1.462, reg_loss: 0.000 lr: 0.000931\n",
      " step: 144, acc: 0.320, loss: 1.550,data_loss: 1.550, reg_loss: 0.000 lr: 0.000930\n",
      " step: 145, acc: 0.508, loss: 1.355,data_loss: 1.355, reg_loss: 0.000 lr: 0.000930\n",
      " step: 146, acc: 0.430, loss: 1.407,data_loss: 1.407, reg_loss: 0.000 lr: 0.000930\n",
      " step: 147, acc: 0.445, loss: 1.347,data_loss: 1.347, reg_loss: 0.000 lr: 0.000930\n",
      " step: 148, acc: 0.438, loss: 1.332,data_loss: 1.332, reg_loss: 0.000 lr: 0.000930\n",
      " step: 149, acc: 0.430, loss: 1.536,data_loss: 1.536, reg_loss: 0.000 lr: 0.000930\n",
      " step: 150, acc: 0.531, loss: 1.240,data_loss: 1.240, reg_loss: 0.000 lr: 0.000930\n",
      " step: 151, acc: 0.430, loss: 1.381,data_loss: 1.381, reg_loss: 0.000 lr: 0.000930\n",
      " step: 152, acc: 0.500, loss: 1.349,data_loss: 1.349, reg_loss: 0.000 lr: 0.000930\n",
      " step: 153, acc: 0.539, loss: 1.288,data_loss: 1.288, reg_loss: 0.000 lr: 0.000930\n",
      " step: 154, acc: 0.367, loss: 1.579,data_loss: 1.579, reg_loss: 0.000 lr: 0.000930\n",
      " step: 155, acc: 0.523, loss: 1.391,data_loss: 1.391, reg_loss: 0.000 lr: 0.000930\n",
      " step: 156, acc: 0.430, loss: 1.443,data_loss: 1.443, reg_loss: 0.000 lr: 0.000930\n",
      " step: 157, acc: 0.547, loss: 1.275,data_loss: 1.275, reg_loss: 0.000 lr: 0.000930\n",
      " step: 158, acc: 0.445, loss: 1.462,data_loss: 1.462, reg_loss: 0.000 lr: 0.000930\n",
      " step: 159, acc: 0.477, loss: 1.420,data_loss: 1.420, reg_loss: 0.000 lr: 0.000930\n",
      " step: 160, acc: 0.398, loss: 1.668,data_loss: 1.668, reg_loss: 0.000 lr: 0.000930\n",
      " step: 161, acc: 0.445, loss: 1.458,data_loss: 1.458, reg_loss: 0.000 lr: 0.000930\n",
      " step: 162, acc: 0.461, loss: 1.437,data_loss: 1.437, reg_loss: 0.000 lr: 0.000930\n",
      " step: 163, acc: 0.414, loss: 1.437,data_loss: 1.437, reg_loss: 0.000 lr: 0.000930\n",
      " step: 164, acc: 0.461, loss: 1.355,data_loss: 1.355, reg_loss: 0.000 lr: 0.000930\n",
      " step: 165, acc: 0.422, loss: 1.472,data_loss: 1.472, reg_loss: 0.000 lr: 0.000930\n",
      " step: 166, acc: 0.422, loss: 1.404,data_loss: 1.404, reg_loss: 0.000 lr: 0.000930\n",
      " step: 167, acc: 0.398, loss: 1.474,data_loss: 1.474, reg_loss: 0.000 lr: 0.000929\n",
      " step: 168, acc: 0.461, loss: 1.343,data_loss: 1.343, reg_loss: 0.000 lr: 0.000929\n",
      " step: 169, acc: 0.445, loss: 1.362,data_loss: 1.362, reg_loss: 0.000 lr: 0.000929\n",
      " step: 170, acc: 0.430, loss: 1.573,data_loss: 1.573, reg_loss: 0.000 lr: 0.000929\n",
      " step: 171, acc: 0.500, loss: 1.358,data_loss: 1.358, reg_loss: 0.000 lr: 0.000929\n",
      " step: 172, acc: 0.445, loss: 1.394,data_loss: 1.394, reg_loss: 0.000 lr: 0.000929\n",
      " step: 173, acc: 0.508, loss: 1.407,data_loss: 1.407, reg_loss: 0.000 lr: 0.000929\n",
      " step: 174, acc: 0.414, loss: 1.438,data_loss: 1.438, reg_loss: 0.000 lr: 0.000929\n",
      " step: 175, acc: 0.438, loss: 1.423,data_loss: 1.423, reg_loss: 0.000 lr: 0.000929\n",
      " step: 176, acc: 0.398, loss: 1.520,data_loss: 1.520, reg_loss: 0.000 lr: 0.000929\n",
      " step: 177, acc: 0.438, loss: 1.383,data_loss: 1.383, reg_loss: 0.000 lr: 0.000929\n",
      " step: 178, acc: 0.414, loss: 1.547,data_loss: 1.547, reg_loss: 0.000 lr: 0.000929\n",
      " step: 179, acc: 0.555, loss: 1.243,data_loss: 1.243, reg_loss: 0.000 lr: 0.000929\n",
      " step: 180, acc: 0.508, loss: 1.225,data_loss: 1.225, reg_loss: 0.000 lr: 0.000929\n",
      " step: 181, acc: 0.430, loss: 1.386,data_loss: 1.386, reg_loss: 0.000 lr: 0.000929\n",
      " step: 182, acc: 0.516, loss: 1.318,data_loss: 1.318, reg_loss: 0.000 lr: 0.000929\n",
      " step: 183, acc: 0.445, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000929\n",
      " step: 184, acc: 0.445, loss: 1.427,data_loss: 1.427, reg_loss: 0.000 lr: 0.000929\n",
      " step: 185, acc: 0.375, loss: 1.531,data_loss: 1.531, reg_loss: 0.000 lr: 0.000929\n",
      " step: 186, acc: 0.438, loss: 1.395,data_loss: 1.395, reg_loss: 0.000 lr: 0.000929\n",
      " step: 187, acc: 0.477, loss: 1.365,data_loss: 1.365, reg_loss: 0.000 lr: 0.000929\n",
      " step: 188, acc: 0.391, loss: 1.482,data_loss: 1.482, reg_loss: 0.000 lr: 0.000929\n",
      " step: 189, acc: 0.469, loss: 1.450,data_loss: 1.450, reg_loss: 0.000 lr: 0.000929\n",
      " step: 190, acc: 0.453, loss: 1.428,data_loss: 1.428, reg_loss: 0.000 lr: 0.000929\n",
      " step: 191, acc: 0.422, loss: 1.491,data_loss: 1.491, reg_loss: 0.000 lr: 0.000928\n",
      " step: 192, acc: 0.508, loss: 1.286,data_loss: 1.286, reg_loss: 0.000 lr: 0.000928\n",
      " step: 193, acc: 0.453, loss: 1.375,data_loss: 1.375, reg_loss: 0.000 lr: 0.000928\n",
      " step: 194, acc: 0.484, loss: 1.424,data_loss: 1.424, reg_loss: 0.000 lr: 0.000928\n",
      " step: 195, acc: 0.531, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000928\n",
      " step: 196, acc: 0.453, loss: 1.442,data_loss: 1.442, reg_loss: 0.000 lr: 0.000928\n",
      " step: 197, acc: 0.430, loss: 1.348,data_loss: 1.348, reg_loss: 0.000 lr: 0.000928\n",
      " step: 198, acc: 0.508, loss: 1.263,data_loss: 1.263, reg_loss: 0.000 lr: 0.000928\n",
      " step: 199, acc: 0.445, loss: 1.514,data_loss: 1.514, reg_loss: 0.000 lr: 0.000928\n",
      " step: 200, acc: 0.406, loss: 1.397,data_loss: 1.397, reg_loss: 0.000 lr: 0.000928\n",
      " step: 201, acc: 0.531, loss: 1.398,data_loss: 1.398, reg_loss: 0.000 lr: 0.000928\n",
      " step: 202, acc: 0.469, loss: 1.336,data_loss: 1.336, reg_loss: 0.000 lr: 0.000928\n",
      " step: 203, acc: 0.430, loss: 1.474,data_loss: 1.474, reg_loss: 0.000 lr: 0.000928\n",
      " step: 204, acc: 0.461, loss: 1.339,data_loss: 1.339, reg_loss: 0.000 lr: 0.000928\n",
      " step: 205, acc: 0.430, loss: 1.432,data_loss: 1.432, reg_loss: 0.000 lr: 0.000928\n",
      " step: 206, acc: 0.461, loss: 1.357,data_loss: 1.357, reg_loss: 0.000 lr: 0.000928\n",
      " step: 207, acc: 0.445, loss: 1.347,data_loss: 1.347, reg_loss: 0.000 lr: 0.000928\n",
      " step: 208, acc: 0.492, loss: 1.306,data_loss: 1.306, reg_loss: 0.000 lr: 0.000928\n",
      " step: 209, acc: 0.414, loss: 1.399,data_loss: 1.399, reg_loss: 0.000 lr: 0.000928\n",
      " step: 210, acc: 0.484, loss: 1.380,data_loss: 1.380, reg_loss: 0.000 lr: 0.000928\n",
      " step: 211, acc: 0.492, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000928\n",
      " step: 212, acc: 0.461, loss: 1.336,data_loss: 1.336, reg_loss: 0.000 lr: 0.000928\n",
      " step: 213, acc: 0.445, loss: 1.458,data_loss: 1.458, reg_loss: 0.000 lr: 0.000928\n",
      " step: 214, acc: 0.406, loss: 1.562,data_loss: 1.562, reg_loss: 0.000 lr: 0.000927\n",
      " step: 215, acc: 0.508, loss: 1.335,data_loss: 1.335, reg_loss: 0.000 lr: 0.000927\n",
      " step: 216, acc: 0.430, loss: 1.330,data_loss: 1.330, reg_loss: 0.000 lr: 0.000927\n",
      " step: 217, acc: 0.430, loss: 1.388,data_loss: 1.388, reg_loss: 0.000 lr: 0.000927\n",
      " step: 218, acc: 0.500, loss: 1.342,data_loss: 1.342, reg_loss: 0.000 lr: 0.000927\n",
      " step: 219, acc: 0.430, loss: 1.420,data_loss: 1.420, reg_loss: 0.000 lr: 0.000927\n",
      " step: 220, acc: 0.445, loss: 1.363,data_loss: 1.363, reg_loss: 0.000 lr: 0.000927\n",
      " step: 221, acc: 0.406, loss: 1.433,data_loss: 1.433, reg_loss: 0.000 lr: 0.000927\n",
      " step: 222, acc: 0.453, loss: 1.465,data_loss: 1.465, reg_loss: 0.000 lr: 0.000927\n",
      " step: 223, acc: 0.461, loss: 1.415,data_loss: 1.415, reg_loss: 0.000 lr: 0.000927\n",
      " step: 224, acc: 0.432, loss: 1.375,data_loss: 1.375, reg_loss: 0.000 lr: 0.000927\n",
      "training , acc: 0.447, loss: 1.415,data_loss: 1.415, reg_loss: 0.000 lr: 0.000927\n",
      "validation, acc:0.422 ,loss: 1.496 \n",
      "epoch: 8\n",
      " step: 0, acc: 0.375, loss: 1.552,data_loss: 1.552, reg_loss: 0.000 lr: 0.000927\n",
      " step: 1, acc: 0.469, loss: 1.395,data_loss: 1.395, reg_loss: 0.000 lr: 0.000927\n",
      " step: 2, acc: 0.438, loss: 1.363,data_loss: 1.363, reg_loss: 0.000 lr: 0.000927\n",
      " step: 3, acc: 0.484, loss: 1.382,data_loss: 1.382, reg_loss: 0.000 lr: 0.000927\n",
      " step: 4, acc: 0.500, loss: 1.287,data_loss: 1.287, reg_loss: 0.000 lr: 0.000927\n",
      " step: 5, acc: 0.414, loss: 1.518,data_loss: 1.518, reg_loss: 0.000 lr: 0.000927\n",
      " step: 6, acc: 0.469, loss: 1.444,data_loss: 1.444, reg_loss: 0.000 lr: 0.000927\n",
      " step: 7, acc: 0.438, loss: 1.512,data_loss: 1.512, reg_loss: 0.000 lr: 0.000927\n",
      " step: 8, acc: 0.445, loss: 1.353,data_loss: 1.353, reg_loss: 0.000 lr: 0.000927\n",
      " step: 9, acc: 0.469, loss: 1.401,data_loss: 1.401, reg_loss: 0.000 lr: 0.000927\n",
      " step: 10, acc: 0.555, loss: 1.330,data_loss: 1.330, reg_loss: 0.000 lr: 0.000927\n",
      " step: 11, acc: 0.531, loss: 1.307,data_loss: 1.307, reg_loss: 0.000 lr: 0.000927\n",
      " step: 12, acc: 0.492, loss: 1.387,data_loss: 1.387, reg_loss: 0.000 lr: 0.000926\n",
      " step: 13, acc: 0.438, loss: 1.401,data_loss: 1.401, reg_loss: 0.000 lr: 0.000926\n",
      " step: 14, acc: 0.422, loss: 1.427,data_loss: 1.427, reg_loss: 0.000 lr: 0.000926\n",
      " step: 15, acc: 0.492, loss: 1.341,data_loss: 1.341, reg_loss: 0.000 lr: 0.000926\n",
      " step: 16, acc: 0.414, loss: 1.417,data_loss: 1.417, reg_loss: 0.000 lr: 0.000926\n",
      " step: 17, acc: 0.406, loss: 1.462,data_loss: 1.462, reg_loss: 0.000 lr: 0.000926\n",
      " step: 18, acc: 0.430, loss: 1.510,data_loss: 1.510, reg_loss: 0.000 lr: 0.000926\n",
      " step: 19, acc: 0.523, loss: 1.239,data_loss: 1.239, reg_loss: 0.000 lr: 0.000926\n",
      " step: 20, acc: 0.445, loss: 1.413,data_loss: 1.413, reg_loss: 0.000 lr: 0.000926\n",
      " step: 21, acc: 0.516, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000926\n",
      " step: 22, acc: 0.445, loss: 1.337,data_loss: 1.337, reg_loss: 0.000 lr: 0.000926\n",
      " step: 23, acc: 0.484, loss: 1.337,data_loss: 1.337, reg_loss: 0.000 lr: 0.000926\n",
      " step: 24, acc: 0.453, loss: 1.310,data_loss: 1.310, reg_loss: 0.000 lr: 0.000926\n",
      " step: 25, acc: 0.516, loss: 1.296,data_loss: 1.296, reg_loss: 0.000 lr: 0.000926\n",
      " step: 26, acc: 0.406, loss: 1.462,data_loss: 1.462, reg_loss: 0.000 lr: 0.000926\n",
      " step: 27, acc: 0.484, loss: 1.337,data_loss: 1.337, reg_loss: 0.000 lr: 0.000926\n",
      " step: 28, acc: 0.422, loss: 1.374,data_loss: 1.374, reg_loss: 0.000 lr: 0.000926\n",
      " step: 29, acc: 0.406, loss: 1.490,data_loss: 1.490, reg_loss: 0.000 lr: 0.000926\n",
      " step: 30, acc: 0.414, loss: 1.558,data_loss: 1.558, reg_loss: 0.000 lr: 0.000926\n",
      " step: 31, acc: 0.500, loss: 1.293,data_loss: 1.293, reg_loss: 0.000 lr: 0.000926\n",
      " step: 32, acc: 0.461, loss: 1.375,data_loss: 1.375, reg_loss: 0.000 lr: 0.000926\n",
      " step: 33, acc: 0.516, loss: 1.426,data_loss: 1.426, reg_loss: 0.000 lr: 0.000926\n",
      " step: 34, acc: 0.477, loss: 1.394,data_loss: 1.394, reg_loss: 0.000 lr: 0.000926\n",
      " step: 35, acc: 0.477, loss: 1.387,data_loss: 1.387, reg_loss: 0.000 lr: 0.000925\n",
      " step: 36, acc: 0.508, loss: 1.296,data_loss: 1.296, reg_loss: 0.000 lr: 0.000925\n",
      " step: 37, acc: 0.414, loss: 1.448,data_loss: 1.448, reg_loss: 0.000 lr: 0.000925\n",
      " step: 38, acc: 0.406, loss: 1.403,data_loss: 1.403, reg_loss: 0.000 lr: 0.000925\n",
      " step: 39, acc: 0.445, loss: 1.444,data_loss: 1.444, reg_loss: 0.000 lr: 0.000925\n",
      " step: 40, acc: 0.469, loss: 1.404,data_loss: 1.404, reg_loss: 0.000 lr: 0.000925\n",
      " step: 41, acc: 0.398, loss: 1.525,data_loss: 1.525, reg_loss: 0.000 lr: 0.000925\n",
      " step: 42, acc: 0.422, loss: 1.392,data_loss: 1.392, reg_loss: 0.000 lr: 0.000925\n",
      " step: 43, acc: 0.422, loss: 1.416,data_loss: 1.416, reg_loss: 0.000 lr: 0.000925\n",
      " step: 44, acc: 0.469, loss: 1.499,data_loss: 1.499, reg_loss: 0.000 lr: 0.000925\n",
      " step: 45, acc: 0.531, loss: 1.349,data_loss: 1.349, reg_loss: 0.000 lr: 0.000925\n",
      " step: 46, acc: 0.492, loss: 1.342,data_loss: 1.342, reg_loss: 0.000 lr: 0.000925\n",
      " step: 47, acc: 0.453, loss: 1.398,data_loss: 1.398, reg_loss: 0.000 lr: 0.000925\n",
      " step: 48, acc: 0.516, loss: 1.283,data_loss: 1.283, reg_loss: 0.000 lr: 0.000925\n",
      " step: 49, acc: 0.461, loss: 1.379,data_loss: 1.379, reg_loss: 0.000 lr: 0.000925\n",
      " step: 50, acc: 0.508, loss: 1.280,data_loss: 1.280, reg_loss: 0.000 lr: 0.000925\n",
      " step: 51, acc: 0.398, loss: 1.546,data_loss: 1.546, reg_loss: 0.000 lr: 0.000925\n",
      " step: 52, acc: 0.398, loss: 1.457,data_loss: 1.457, reg_loss: 0.000 lr: 0.000925\n",
      " step: 53, acc: 0.477, loss: 1.465,data_loss: 1.465, reg_loss: 0.000 lr: 0.000925\n",
      " step: 54, acc: 0.422, loss: 1.512,data_loss: 1.512, reg_loss: 0.000 lr: 0.000925\n",
      " step: 55, acc: 0.406, loss: 1.452,data_loss: 1.452, reg_loss: 0.000 lr: 0.000925\n",
      " step: 56, acc: 0.445, loss: 1.368,data_loss: 1.368, reg_loss: 0.000 lr: 0.000925\n",
      " step: 57, acc: 0.477, loss: 1.347,data_loss: 1.347, reg_loss: 0.000 lr: 0.000925\n",
      " step: 58, acc: 0.477, loss: 1.444,data_loss: 1.444, reg_loss: 0.000 lr: 0.000925\n",
      " step: 59, acc: 0.555, loss: 1.354,data_loss: 1.354, reg_loss: 0.000 lr: 0.000924\n",
      " step: 60, acc: 0.484, loss: 1.351,data_loss: 1.351, reg_loss: 0.000 lr: 0.000924\n",
      " step: 61, acc: 0.375, loss: 1.462,data_loss: 1.462, reg_loss: 0.000 lr: 0.000924\n",
      " step: 62, acc: 0.477, loss: 1.377,data_loss: 1.377, reg_loss: 0.000 lr: 0.000924\n",
      " step: 63, acc: 0.500, loss: 1.356,data_loss: 1.356, reg_loss: 0.000 lr: 0.000924\n",
      " step: 64, acc: 0.453, loss: 1.416,data_loss: 1.416, reg_loss: 0.000 lr: 0.000924\n",
      " step: 65, acc: 0.438, loss: 1.417,data_loss: 1.417, reg_loss: 0.000 lr: 0.000924\n",
      " step: 66, acc: 0.484, loss: 1.357,data_loss: 1.357, reg_loss: 0.000 lr: 0.000924\n",
      " step: 67, acc: 0.359, loss: 1.403,data_loss: 1.403, reg_loss: 0.000 lr: 0.000924\n",
      " step: 68, acc: 0.398, loss: 1.426,data_loss: 1.426, reg_loss: 0.000 lr: 0.000924\n",
      " step: 69, acc: 0.344, loss: 1.501,data_loss: 1.501, reg_loss: 0.000 lr: 0.000924\n",
      " step: 70, acc: 0.484, loss: 1.289,data_loss: 1.289, reg_loss: 0.000 lr: 0.000924\n",
      " step: 71, acc: 0.484, loss: 1.278,data_loss: 1.278, reg_loss: 0.000 lr: 0.000924\n",
      " step: 72, acc: 0.453, loss: 1.345,data_loss: 1.345, reg_loss: 0.000 lr: 0.000924\n",
      " step: 73, acc: 0.438, loss: 1.395,data_loss: 1.395, reg_loss: 0.000 lr: 0.000924\n",
      " step: 74, acc: 0.414, loss: 1.389,data_loss: 1.389, reg_loss: 0.000 lr: 0.000924\n",
      " step: 75, acc: 0.508, loss: 1.290,data_loss: 1.290, reg_loss: 0.000 lr: 0.000924\n",
      " step: 76, acc: 0.539, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000924\n",
      " step: 77, acc: 0.453, loss: 1.361,data_loss: 1.361, reg_loss: 0.000 lr: 0.000924\n",
      " step: 78, acc: 0.531, loss: 1.258,data_loss: 1.258, reg_loss: 0.000 lr: 0.000924\n",
      " step: 79, acc: 0.477, loss: 1.372,data_loss: 1.372, reg_loss: 0.000 lr: 0.000924\n",
      " step: 80, acc: 0.430, loss: 1.484,data_loss: 1.484, reg_loss: 0.000 lr: 0.000924\n",
      " step: 81, acc: 0.500, loss: 1.465,data_loss: 1.465, reg_loss: 0.000 lr: 0.000924\n",
      " step: 82, acc: 0.500, loss: 1.316,data_loss: 1.316, reg_loss: 0.000 lr: 0.000923\n",
      " step: 83, acc: 0.430, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000923\n",
      " step: 84, acc: 0.391, loss: 1.460,data_loss: 1.460, reg_loss: 0.000 lr: 0.000923\n",
      " step: 85, acc: 0.500, loss: 1.352,data_loss: 1.352, reg_loss: 0.000 lr: 0.000923\n",
      " step: 86, acc: 0.484, loss: 1.340,data_loss: 1.340, reg_loss: 0.000 lr: 0.000923\n",
      " step: 87, acc: 0.508, loss: 1.427,data_loss: 1.427, reg_loss: 0.000 lr: 0.000923\n",
      " step: 88, acc: 0.453, loss: 1.566,data_loss: 1.566, reg_loss: 0.000 lr: 0.000923\n",
      " step: 89, acc: 0.445, loss: 1.404,data_loss: 1.404, reg_loss: 0.000 lr: 0.000923\n",
      " step: 90, acc: 0.445, loss: 1.353,data_loss: 1.353, reg_loss: 0.000 lr: 0.000923\n",
      " step: 91, acc: 0.414, loss: 1.461,data_loss: 1.461, reg_loss: 0.000 lr: 0.000923\n",
      " step: 92, acc: 0.391, loss: 1.465,data_loss: 1.465, reg_loss: 0.000 lr: 0.000923\n",
      " step: 93, acc: 0.508, loss: 1.352,data_loss: 1.352, reg_loss: 0.000 lr: 0.000923\n",
      " step: 94, acc: 0.375, loss: 1.495,data_loss: 1.495, reg_loss: 0.000 lr: 0.000923\n",
      " step: 95, acc: 0.492, loss: 1.343,data_loss: 1.343, reg_loss: 0.000 lr: 0.000923\n",
      " step: 96, acc: 0.391, loss: 1.432,data_loss: 1.432, reg_loss: 0.000 lr: 0.000923\n",
      " step: 97, acc: 0.469, loss: 1.382,data_loss: 1.382, reg_loss: 0.000 lr: 0.000923\n",
      " step: 98, acc: 0.516, loss: 1.268,data_loss: 1.268, reg_loss: 0.000 lr: 0.000923\n",
      " step: 99, acc: 0.500, loss: 1.270,data_loss: 1.270, reg_loss: 0.000 lr: 0.000923\n",
      " step: 100, acc: 0.500, loss: 1.350,data_loss: 1.350, reg_loss: 0.000 lr: 0.000923\n",
      " step: 101, acc: 0.469, loss: 1.362,data_loss: 1.362, reg_loss: 0.000 lr: 0.000923\n",
      " step: 102, acc: 0.477, loss: 1.357,data_loss: 1.357, reg_loss: 0.000 lr: 0.000923\n",
      " step: 103, acc: 0.531, loss: 1.354,data_loss: 1.354, reg_loss: 0.000 lr: 0.000923\n",
      " step: 104, acc: 0.492, loss: 1.323,data_loss: 1.323, reg_loss: 0.000 lr: 0.000923\n",
      " step: 105, acc: 0.469, loss: 1.368,data_loss: 1.368, reg_loss: 0.000 lr: 0.000923\n",
      " step: 106, acc: 0.469, loss: 1.269,data_loss: 1.269, reg_loss: 0.000 lr: 0.000922\n",
      " step: 107, acc: 0.414, loss: 1.394,data_loss: 1.394, reg_loss: 0.000 lr: 0.000922\n",
      " step: 108, acc: 0.477, loss: 1.381,data_loss: 1.381, reg_loss: 0.000 lr: 0.000922\n",
      " step: 109, acc: 0.445, loss: 1.354,data_loss: 1.354, reg_loss: 0.000 lr: 0.000922\n",
      " step: 110, acc: 0.469, loss: 1.420,data_loss: 1.420, reg_loss: 0.000 lr: 0.000922\n",
      " step: 111, acc: 0.484, loss: 1.376,data_loss: 1.376, reg_loss: 0.000 lr: 0.000922\n",
      " step: 112, acc: 0.461, loss: 1.377,data_loss: 1.377, reg_loss: 0.000 lr: 0.000922\n",
      " step: 113, acc: 0.469, loss: 1.314,data_loss: 1.314, reg_loss: 0.000 lr: 0.000922\n",
      " step: 114, acc: 0.430, loss: 1.403,data_loss: 1.403, reg_loss: 0.000 lr: 0.000922\n",
      " step: 115, acc: 0.422, loss: 1.493,data_loss: 1.493, reg_loss: 0.000 lr: 0.000922\n",
      " step: 116, acc: 0.438, loss: 1.516,data_loss: 1.516, reg_loss: 0.000 lr: 0.000922\n",
      " step: 117, acc: 0.469, loss: 1.320,data_loss: 1.320, reg_loss: 0.000 lr: 0.000922\n",
      " step: 118, acc: 0.453, loss: 1.389,data_loss: 1.389, reg_loss: 0.000 lr: 0.000922\n",
      " step: 119, acc: 0.531, loss: 1.320,data_loss: 1.320, reg_loss: 0.000 lr: 0.000922\n",
      " step: 120, acc: 0.430, loss: 1.440,data_loss: 1.440, reg_loss: 0.000 lr: 0.000922\n",
      " step: 121, acc: 0.461, loss: 1.437,data_loss: 1.437, reg_loss: 0.000 lr: 0.000922\n",
      " step: 122, acc: 0.406, loss: 1.402,data_loss: 1.402, reg_loss: 0.000 lr: 0.000922\n",
      " step: 123, acc: 0.492, loss: 1.390,data_loss: 1.390, reg_loss: 0.000 lr: 0.000922\n",
      " step: 124, acc: 0.445, loss: 1.417,data_loss: 1.417, reg_loss: 0.000 lr: 0.000922\n",
      " step: 125, acc: 0.445, loss: 1.363,data_loss: 1.363, reg_loss: 0.000 lr: 0.000922\n",
      " step: 126, acc: 0.469, loss: 1.392,data_loss: 1.392, reg_loss: 0.000 lr: 0.000922\n",
      " step: 127, acc: 0.430, loss: 1.470,data_loss: 1.470, reg_loss: 0.000 lr: 0.000922\n",
      " step: 128, acc: 0.453, loss: 1.366,data_loss: 1.366, reg_loss: 0.000 lr: 0.000922\n",
      " step: 129, acc: 0.453, loss: 1.488,data_loss: 1.488, reg_loss: 0.000 lr: 0.000921\n",
      " step: 130, acc: 0.359, loss: 1.572,data_loss: 1.572, reg_loss: 0.000 lr: 0.000921\n",
      " step: 131, acc: 0.461, loss: 1.395,data_loss: 1.395, reg_loss: 0.000 lr: 0.000921\n",
      " step: 132, acc: 0.422, loss: 1.323,data_loss: 1.323, reg_loss: 0.000 lr: 0.000921\n",
      " step: 133, acc: 0.398, loss: 1.424,data_loss: 1.424, reg_loss: 0.000 lr: 0.000921\n",
      " step: 134, acc: 0.539, loss: 1.181,data_loss: 1.181, reg_loss: 0.000 lr: 0.000921\n",
      " step: 135, acc: 0.500, loss: 1.374,data_loss: 1.374, reg_loss: 0.000 lr: 0.000921\n",
      " step: 136, acc: 0.406, loss: 1.433,data_loss: 1.433, reg_loss: 0.000 lr: 0.000921\n",
      " step: 137, acc: 0.547, loss: 1.257,data_loss: 1.257, reg_loss: 0.000 lr: 0.000921\n",
      " step: 138, acc: 0.539, loss: 1.330,data_loss: 1.330, reg_loss: 0.000 lr: 0.000921\n",
      " step: 139, acc: 0.500, loss: 1.279,data_loss: 1.279, reg_loss: 0.000 lr: 0.000921\n",
      " step: 140, acc: 0.523, loss: 1.260,data_loss: 1.260, reg_loss: 0.000 lr: 0.000921\n",
      " step: 141, acc: 0.414, loss: 1.384,data_loss: 1.384, reg_loss: 0.000 lr: 0.000921\n",
      " step: 142, acc: 0.469, loss: 1.378,data_loss: 1.378, reg_loss: 0.000 lr: 0.000921\n",
      " step: 143, acc: 0.461, loss: 1.420,data_loss: 1.420, reg_loss: 0.000 lr: 0.000921\n",
      " step: 144, acc: 0.414, loss: 1.580,data_loss: 1.580, reg_loss: 0.000 lr: 0.000921\n",
      " step: 145, acc: 0.484, loss: 1.327,data_loss: 1.327, reg_loss: 0.000 lr: 0.000921\n",
      " step: 146, acc: 0.430, loss: 1.354,data_loss: 1.354, reg_loss: 0.000 lr: 0.000921\n",
      " step: 147, acc: 0.469, loss: 1.360,data_loss: 1.360, reg_loss: 0.000 lr: 0.000921\n",
      " step: 148, acc: 0.500, loss: 1.292,data_loss: 1.292, reg_loss: 0.000 lr: 0.000921\n",
      " step: 149, acc: 0.422, loss: 1.402,data_loss: 1.402, reg_loss: 0.000 lr: 0.000921\n",
      " step: 150, acc: 0.500, loss: 1.203,data_loss: 1.203, reg_loss: 0.000 lr: 0.000921\n",
      " step: 151, acc: 0.453, loss: 1.392,data_loss: 1.392, reg_loss: 0.000 lr: 0.000921\n",
      " step: 152, acc: 0.523, loss: 1.372,data_loss: 1.372, reg_loss: 0.000 lr: 0.000921\n",
      " step: 153, acc: 0.492, loss: 1.251,data_loss: 1.251, reg_loss: 0.000 lr: 0.000920\n",
      " step: 154, acc: 0.406, loss: 1.495,data_loss: 1.495, reg_loss: 0.000 lr: 0.000920\n",
      " step: 155, acc: 0.430, loss: 1.405,data_loss: 1.405, reg_loss: 0.000 lr: 0.000920\n",
      " step: 156, acc: 0.492, loss: 1.398,data_loss: 1.398, reg_loss: 0.000 lr: 0.000920\n",
      " step: 157, acc: 0.547, loss: 1.307,data_loss: 1.307, reg_loss: 0.000 lr: 0.000920\n",
      " step: 158, acc: 0.422, loss: 1.420,data_loss: 1.420, reg_loss: 0.000 lr: 0.000920\n",
      " step: 159, acc: 0.422, loss: 1.468,data_loss: 1.468, reg_loss: 0.000 lr: 0.000920\n",
      " step: 160, acc: 0.391, loss: 1.581,data_loss: 1.581, reg_loss: 0.000 lr: 0.000920\n",
      " step: 161, acc: 0.500, loss: 1.415,data_loss: 1.415, reg_loss: 0.000 lr: 0.000920\n",
      " step: 162, acc: 0.492, loss: 1.421,data_loss: 1.421, reg_loss: 0.000 lr: 0.000920\n",
      " step: 163, acc: 0.430, loss: 1.366,data_loss: 1.366, reg_loss: 0.000 lr: 0.000920\n",
      " step: 164, acc: 0.477, loss: 1.312,data_loss: 1.312, reg_loss: 0.000 lr: 0.000920\n",
      " step: 165, acc: 0.398, loss: 1.447,data_loss: 1.447, reg_loss: 0.000 lr: 0.000920\n",
      " step: 166, acc: 0.484, loss: 1.353,data_loss: 1.353, reg_loss: 0.000 lr: 0.000920\n",
      " step: 167, acc: 0.422, loss: 1.459,data_loss: 1.459, reg_loss: 0.000 lr: 0.000920\n",
      " step: 168, acc: 0.461, loss: 1.325,data_loss: 1.325, reg_loss: 0.000 lr: 0.000920\n",
      " step: 169, acc: 0.500, loss: 1.360,data_loss: 1.360, reg_loss: 0.000 lr: 0.000920\n",
      " step: 170, acc: 0.375, loss: 1.580,data_loss: 1.580, reg_loss: 0.000 lr: 0.000920\n",
      " step: 171, acc: 0.516, loss: 1.300,data_loss: 1.300, reg_loss: 0.000 lr: 0.000920\n",
      " step: 172, acc: 0.469, loss: 1.372,data_loss: 1.372, reg_loss: 0.000 lr: 0.000920\n",
      " step: 173, acc: 0.539, loss: 1.334,data_loss: 1.334, reg_loss: 0.000 lr: 0.000920\n",
      " step: 174, acc: 0.438, loss: 1.340,data_loss: 1.340, reg_loss: 0.000 lr: 0.000920\n",
      " step: 175, acc: 0.430, loss: 1.510,data_loss: 1.510, reg_loss: 0.000 lr: 0.000920\n",
      " step: 176, acc: 0.359, loss: 1.532,data_loss: 1.532, reg_loss: 0.000 lr: 0.000919\n",
      " step: 177, acc: 0.461, loss: 1.356,data_loss: 1.356, reg_loss: 0.000 lr: 0.000919\n",
      " step: 178, acc: 0.406, loss: 1.596,data_loss: 1.596, reg_loss: 0.000 lr: 0.000919\n",
      " step: 179, acc: 0.539, loss: 1.158,data_loss: 1.158, reg_loss: 0.000 lr: 0.000919\n",
      " step: 180, acc: 0.555, loss: 1.157,data_loss: 1.157, reg_loss: 0.000 lr: 0.000919\n",
      " step: 181, acc: 0.492, loss: 1.345,data_loss: 1.345, reg_loss: 0.000 lr: 0.000919\n",
      " step: 182, acc: 0.500, loss: 1.294,data_loss: 1.294, reg_loss: 0.000 lr: 0.000919\n",
      " step: 183, acc: 0.484, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000919\n",
      " step: 184, acc: 0.438, loss: 1.451,data_loss: 1.451, reg_loss: 0.000 lr: 0.000919\n",
      " step: 185, acc: 0.492, loss: 1.404,data_loss: 1.404, reg_loss: 0.000 lr: 0.000919\n",
      " step: 186, acc: 0.414, loss: 1.434,data_loss: 1.434, reg_loss: 0.000 lr: 0.000919\n",
      " step: 187, acc: 0.438, loss: 1.363,data_loss: 1.363, reg_loss: 0.000 lr: 0.000919\n",
      " step: 188, acc: 0.438, loss: 1.413,data_loss: 1.413, reg_loss: 0.000 lr: 0.000919\n",
      " step: 189, acc: 0.438, loss: 1.396,data_loss: 1.396, reg_loss: 0.000 lr: 0.000919\n",
      " step: 190, acc: 0.438, loss: 1.485,data_loss: 1.485, reg_loss: 0.000 lr: 0.000919\n",
      " step: 191, acc: 0.453, loss: 1.462,data_loss: 1.462, reg_loss: 0.000 lr: 0.000919\n",
      " step: 192, acc: 0.531, loss: 1.253,data_loss: 1.253, reg_loss: 0.000 lr: 0.000919\n",
      " step: 193, acc: 0.430, loss: 1.361,data_loss: 1.361, reg_loss: 0.000 lr: 0.000919\n",
      " step: 194, acc: 0.523, loss: 1.380,data_loss: 1.380, reg_loss: 0.000 lr: 0.000919\n",
      " step: 195, acc: 0.500, loss: 1.255,data_loss: 1.255, reg_loss: 0.000 lr: 0.000919\n",
      " step: 196, acc: 0.469, loss: 1.371,data_loss: 1.371, reg_loss: 0.000 lr: 0.000919\n",
      " step: 197, acc: 0.469, loss: 1.357,data_loss: 1.357, reg_loss: 0.000 lr: 0.000919\n",
      " step: 198, acc: 0.555, loss: 1.230,data_loss: 1.230, reg_loss: 0.000 lr: 0.000919\n",
      " step: 199, acc: 0.391, loss: 1.553,data_loss: 1.553, reg_loss: 0.000 lr: 0.000919\n",
      " step: 200, acc: 0.492, loss: 1.316,data_loss: 1.316, reg_loss: 0.000 lr: 0.000918\n",
      " step: 201, acc: 0.500, loss: 1.386,data_loss: 1.386, reg_loss: 0.000 lr: 0.000918\n",
      " step: 202, acc: 0.445, loss: 1.305,data_loss: 1.305, reg_loss: 0.000 lr: 0.000918\n",
      " step: 203, acc: 0.445, loss: 1.361,data_loss: 1.361, reg_loss: 0.000 lr: 0.000918\n",
      " step: 204, acc: 0.547, loss: 1.239,data_loss: 1.239, reg_loss: 0.000 lr: 0.000918\n",
      " step: 205, acc: 0.430, loss: 1.427,data_loss: 1.427, reg_loss: 0.000 lr: 0.000918\n",
      " step: 206, acc: 0.492, loss: 1.237,data_loss: 1.237, reg_loss: 0.000 lr: 0.000918\n",
      " step: 207, acc: 0.516, loss: 1.288,data_loss: 1.288, reg_loss: 0.000 lr: 0.000918\n",
      " step: 208, acc: 0.531, loss: 1.289,data_loss: 1.289, reg_loss: 0.000 lr: 0.000918\n",
      " step: 209, acc: 0.461, loss: 1.394,data_loss: 1.394, reg_loss: 0.000 lr: 0.000918\n",
      " step: 210, acc: 0.484, loss: 1.338,data_loss: 1.338, reg_loss: 0.000 lr: 0.000918\n",
      " step: 211, acc: 0.461, loss: 1.367,data_loss: 1.367, reg_loss: 0.000 lr: 0.000918\n",
      " step: 212, acc: 0.523, loss: 1.268,data_loss: 1.268, reg_loss: 0.000 lr: 0.000918\n",
      " step: 213, acc: 0.453, loss: 1.318,data_loss: 1.318, reg_loss: 0.000 lr: 0.000918\n",
      " step: 214, acc: 0.406, loss: 1.434,data_loss: 1.434, reg_loss: 0.000 lr: 0.000918\n",
      " step: 215, acc: 0.516, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000918\n",
      " step: 216, acc: 0.477, loss: 1.335,data_loss: 1.335, reg_loss: 0.000 lr: 0.000918\n",
      " step: 217, acc: 0.484, loss: 1.274,data_loss: 1.274, reg_loss: 0.000 lr: 0.000918\n",
      " step: 218, acc: 0.523, loss: 1.289,data_loss: 1.289, reg_loss: 0.000 lr: 0.000918\n",
      " step: 219, acc: 0.508, loss: 1.354,data_loss: 1.354, reg_loss: 0.000 lr: 0.000918\n",
      " step: 220, acc: 0.469, loss: 1.337,data_loss: 1.337, reg_loss: 0.000 lr: 0.000918\n",
      " step: 221, acc: 0.422, loss: 1.407,data_loss: 1.407, reg_loss: 0.000 lr: 0.000918\n",
      " step: 222, acc: 0.461, loss: 1.405,data_loss: 1.405, reg_loss: 0.000 lr: 0.000918\n",
      " step: 223, acc: 0.398, loss: 1.490,data_loss: 1.490, reg_loss: 0.000 lr: 0.000918\n",
      " step: 224, acc: 0.514, loss: 1.302,data_loss: 1.302, reg_loss: 0.000 lr: 0.000917\n",
      "training , acc: 0.463, loss: 1.381,data_loss: 1.381, reg_loss: 0.000 lr: 0.000917\n",
      "validation, acc:0.427 ,loss: 1.494 \n",
      "epoch: 9\n",
      " step: 0, acc: 0.375, loss: 1.501,data_loss: 1.501, reg_loss: 0.000 lr: 0.000917\n",
      " step: 1, acc: 0.422, loss: 1.432,data_loss: 1.432, reg_loss: 0.000 lr: 0.000917\n",
      " step: 2, acc: 0.500, loss: 1.326,data_loss: 1.326, reg_loss: 0.000 lr: 0.000917\n",
      " step: 3, acc: 0.445, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000917\n",
      " step: 4, acc: 0.539, loss: 1.233,data_loss: 1.233, reg_loss: 0.000 lr: 0.000917\n",
      " step: 5, acc: 0.445, loss: 1.468,data_loss: 1.468, reg_loss: 0.000 lr: 0.000917\n",
      " step: 6, acc: 0.477, loss: 1.295,data_loss: 1.295, reg_loss: 0.000 lr: 0.000917\n",
      " step: 7, acc: 0.492, loss: 1.425,data_loss: 1.425, reg_loss: 0.000 lr: 0.000917\n",
      " step: 8, acc: 0.492, loss: 1.317,data_loss: 1.317, reg_loss: 0.000 lr: 0.000917\n",
      " step: 9, acc: 0.508, loss: 1.290,data_loss: 1.290, reg_loss: 0.000 lr: 0.000917\n",
      " step: 10, acc: 0.477, loss: 1.306,data_loss: 1.306, reg_loss: 0.000 lr: 0.000917\n",
      " step: 11, acc: 0.539, loss: 1.262,data_loss: 1.262, reg_loss: 0.000 lr: 0.000917\n",
      " step: 12, acc: 0.469, loss: 1.330,data_loss: 1.330, reg_loss: 0.000 lr: 0.000917\n",
      " step: 13, acc: 0.453, loss: 1.390,data_loss: 1.390, reg_loss: 0.000 lr: 0.000917\n",
      " step: 14, acc: 0.422, loss: 1.406,data_loss: 1.406, reg_loss: 0.000 lr: 0.000917\n",
      " step: 15, acc: 0.500, loss: 1.257,data_loss: 1.257, reg_loss: 0.000 lr: 0.000917\n",
      " step: 16, acc: 0.461, loss: 1.355,data_loss: 1.355, reg_loss: 0.000 lr: 0.000917\n",
      " step: 17, acc: 0.461, loss: 1.423,data_loss: 1.423, reg_loss: 0.000 lr: 0.000917\n",
      " step: 18, acc: 0.438, loss: 1.424,data_loss: 1.424, reg_loss: 0.000 lr: 0.000917\n",
      " step: 19, acc: 0.516, loss: 1.196,data_loss: 1.196, reg_loss: 0.000 lr: 0.000917\n",
      " step: 20, acc: 0.484, loss: 1.292,data_loss: 1.292, reg_loss: 0.000 lr: 0.000917\n",
      " step: 21, acc: 0.508, loss: 1.291,data_loss: 1.291, reg_loss: 0.000 lr: 0.000917\n",
      " step: 22, acc: 0.508, loss: 1.278,data_loss: 1.278, reg_loss: 0.000 lr: 0.000917\n",
      " step: 23, acc: 0.562, loss: 1.291,data_loss: 1.291, reg_loss: 0.000 lr: 0.000916\n",
      " step: 24, acc: 0.422, loss: 1.367,data_loss: 1.367, reg_loss: 0.000 lr: 0.000916\n",
      " step: 25, acc: 0.492, loss: 1.347,data_loss: 1.347, reg_loss: 0.000 lr: 0.000916\n",
      " step: 26, acc: 0.391, loss: 1.392,data_loss: 1.392, reg_loss: 0.000 lr: 0.000916\n",
      " step: 27, acc: 0.500, loss: 1.292,data_loss: 1.292, reg_loss: 0.000 lr: 0.000916\n",
      " step: 28, acc: 0.492, loss: 1.320,data_loss: 1.320, reg_loss: 0.000 lr: 0.000916\n",
      " step: 29, acc: 0.469, loss: 1.361,data_loss: 1.361, reg_loss: 0.000 lr: 0.000916\n",
      " step: 30, acc: 0.375, loss: 1.639,data_loss: 1.639, reg_loss: 0.000 lr: 0.000916\n",
      " step: 31, acc: 0.531, loss: 1.192,data_loss: 1.192, reg_loss: 0.000 lr: 0.000916\n",
      " step: 32, acc: 0.461, loss: 1.388,data_loss: 1.388, reg_loss: 0.000 lr: 0.000916\n",
      " step: 33, acc: 0.531, loss: 1.431,data_loss: 1.431, reg_loss: 0.000 lr: 0.000916\n",
      " step: 34, acc: 0.453, loss: 1.404,data_loss: 1.404, reg_loss: 0.000 lr: 0.000916\n",
      " step: 35, acc: 0.500, loss: 1.322,data_loss: 1.322, reg_loss: 0.000 lr: 0.000916\n",
      " step: 36, acc: 0.547, loss: 1.288,data_loss: 1.288, reg_loss: 0.000 lr: 0.000916\n",
      " step: 37, acc: 0.461, loss: 1.384,data_loss: 1.384, reg_loss: 0.000 lr: 0.000916\n",
      " step: 38, acc: 0.438, loss: 1.380,data_loss: 1.380, reg_loss: 0.000 lr: 0.000916\n",
      " step: 39, acc: 0.383, loss: 1.446,data_loss: 1.446, reg_loss: 0.000 lr: 0.000916\n",
      " step: 40, acc: 0.516, loss: 1.411,data_loss: 1.411, reg_loss: 0.000 lr: 0.000916\n",
      " step: 41, acc: 0.414, loss: 1.486,data_loss: 1.486, reg_loss: 0.000 lr: 0.000916\n",
      " step: 42, acc: 0.492, loss: 1.379,data_loss: 1.379, reg_loss: 0.000 lr: 0.000916\n",
      " step: 43, acc: 0.430, loss: 1.394,data_loss: 1.394, reg_loss: 0.000 lr: 0.000916\n",
      " step: 44, acc: 0.484, loss: 1.406,data_loss: 1.406, reg_loss: 0.000 lr: 0.000916\n",
      " step: 45, acc: 0.523, loss: 1.380,data_loss: 1.380, reg_loss: 0.000 lr: 0.000916\n",
      " step: 46, acc: 0.500, loss: 1.341,data_loss: 1.341, reg_loss: 0.000 lr: 0.000915\n",
      " step: 47, acc: 0.453, loss: 1.341,data_loss: 1.341, reg_loss: 0.000 lr: 0.000915\n",
      " step: 48, acc: 0.516, loss: 1.245,data_loss: 1.245, reg_loss: 0.000 lr: 0.000915\n",
      " step: 49, acc: 0.508, loss: 1.284,data_loss: 1.284, reg_loss: 0.000 lr: 0.000915\n",
      " step: 50, acc: 0.492, loss: 1.318,data_loss: 1.318, reg_loss: 0.000 lr: 0.000915\n",
      " step: 51, acc: 0.391, loss: 1.499,data_loss: 1.499, reg_loss: 0.000 lr: 0.000915\n",
      " step: 52, acc: 0.398, loss: 1.449,data_loss: 1.449, reg_loss: 0.000 lr: 0.000915\n",
      " step: 53, acc: 0.562, loss: 1.361,data_loss: 1.361, reg_loss: 0.000 lr: 0.000915\n",
      " step: 54, acc: 0.422, loss: 1.479,data_loss: 1.479, reg_loss: 0.000 lr: 0.000915\n",
      " step: 55, acc: 0.453, loss: 1.408,data_loss: 1.408, reg_loss: 0.000 lr: 0.000915\n",
      " step: 56, acc: 0.492, loss: 1.335,data_loss: 1.335, reg_loss: 0.000 lr: 0.000915\n",
      " step: 57, acc: 0.539, loss: 1.266,data_loss: 1.266, reg_loss: 0.000 lr: 0.000915\n",
      " step: 58, acc: 0.547, loss: 1.336,data_loss: 1.336, reg_loss: 0.000 lr: 0.000915\n",
      " step: 59, acc: 0.484, loss: 1.317,data_loss: 1.317, reg_loss: 0.000 lr: 0.000915\n",
      " step: 60, acc: 0.461, loss: 1.291,data_loss: 1.291, reg_loss: 0.000 lr: 0.000915\n",
      " step: 61, acc: 0.484, loss: 1.353,data_loss: 1.353, reg_loss: 0.000 lr: 0.000915\n",
      " step: 62, acc: 0.484, loss: 1.365,data_loss: 1.365, reg_loss: 0.000 lr: 0.000915\n",
      " step: 63, acc: 0.500, loss: 1.325,data_loss: 1.325, reg_loss: 0.000 lr: 0.000915\n",
      " step: 64, acc: 0.438, loss: 1.422,data_loss: 1.422, reg_loss: 0.000 lr: 0.000915\n",
      " step: 65, acc: 0.438, loss: 1.395,data_loss: 1.395, reg_loss: 0.000 lr: 0.000915\n",
      " step: 66, acc: 0.516, loss: 1.266,data_loss: 1.266, reg_loss: 0.000 lr: 0.000915\n",
      " step: 67, acc: 0.477, loss: 1.340,data_loss: 1.340, reg_loss: 0.000 lr: 0.000915\n",
      " step: 68, acc: 0.406, loss: 1.435,data_loss: 1.435, reg_loss: 0.000 lr: 0.000915\n",
      " step: 69, acc: 0.414, loss: 1.461,data_loss: 1.461, reg_loss: 0.000 lr: 0.000915\n",
      " step: 70, acc: 0.453, loss: 1.325,data_loss: 1.325, reg_loss: 0.000 lr: 0.000914\n",
      " step: 71, acc: 0.523, loss: 1.332,data_loss: 1.332, reg_loss: 0.000 lr: 0.000914\n",
      " step: 72, acc: 0.469, loss: 1.362,data_loss: 1.362, reg_loss: 0.000 lr: 0.000914\n",
      " step: 73, acc: 0.469, loss: 1.350,data_loss: 1.350, reg_loss: 0.000 lr: 0.000914\n",
      " step: 74, acc: 0.445, loss: 1.436,data_loss: 1.436, reg_loss: 0.000 lr: 0.000914\n",
      " step: 75, acc: 0.516, loss: 1.245,data_loss: 1.245, reg_loss: 0.000 lr: 0.000914\n",
      " step: 76, acc: 0.500, loss: 1.337,data_loss: 1.337, reg_loss: 0.000 lr: 0.000914\n",
      " step: 77, acc: 0.469, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000914\n",
      " step: 78, acc: 0.531, loss: 1.182,data_loss: 1.182, reg_loss: 0.000 lr: 0.000914\n",
      " step: 79, acc: 0.414, loss: 1.422,data_loss: 1.422, reg_loss: 0.000 lr: 0.000914\n",
      " step: 80, acc: 0.492, loss: 1.391,data_loss: 1.391, reg_loss: 0.000 lr: 0.000914\n",
      " step: 81, acc: 0.492, loss: 1.432,data_loss: 1.432, reg_loss: 0.000 lr: 0.000914\n",
      " step: 82, acc: 0.547, loss: 1.292,data_loss: 1.292, reg_loss: 0.000 lr: 0.000914\n",
      " step: 83, acc: 0.531, loss: 1.239,data_loss: 1.239, reg_loss: 0.000 lr: 0.000914\n",
      " step: 84, acc: 0.469, loss: 1.341,data_loss: 1.341, reg_loss: 0.000 lr: 0.000914\n",
      " step: 85, acc: 0.492, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000914\n",
      " step: 86, acc: 0.438, loss: 1.351,data_loss: 1.351, reg_loss: 0.000 lr: 0.000914\n",
      " step: 87, acc: 0.508, loss: 1.288,data_loss: 1.288, reg_loss: 0.000 lr: 0.000914\n",
      " step: 88, acc: 0.484, loss: 1.427,data_loss: 1.427, reg_loss: 0.000 lr: 0.000914\n",
      " step: 89, acc: 0.484, loss: 1.351,data_loss: 1.351, reg_loss: 0.000 lr: 0.000914\n",
      " step: 90, acc: 0.469, loss: 1.408,data_loss: 1.408, reg_loss: 0.000 lr: 0.000914\n",
      " step: 91, acc: 0.430, loss: 1.457,data_loss: 1.457, reg_loss: 0.000 lr: 0.000914\n",
      " step: 92, acc: 0.477, loss: 1.360,data_loss: 1.360, reg_loss: 0.000 lr: 0.000914\n",
      " step: 93, acc: 0.562, loss: 1.275,data_loss: 1.275, reg_loss: 0.000 lr: 0.000914\n",
      " step: 94, acc: 0.367, loss: 1.471,data_loss: 1.471, reg_loss: 0.000 lr: 0.000913\n",
      " step: 95, acc: 0.500, loss: 1.305,data_loss: 1.305, reg_loss: 0.000 lr: 0.000913\n",
      " step: 96, acc: 0.453, loss: 1.383,data_loss: 1.383, reg_loss: 0.000 lr: 0.000913\n",
      " step: 97, acc: 0.477, loss: 1.368,data_loss: 1.368, reg_loss: 0.000 lr: 0.000913\n",
      " step: 98, acc: 0.508, loss: 1.247,data_loss: 1.247, reg_loss: 0.000 lr: 0.000913\n",
      " step: 99, acc: 0.508, loss: 1.189,data_loss: 1.189, reg_loss: 0.000 lr: 0.000913\n",
      " step: 100, acc: 0.422, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000913\n",
      " step: 101, acc: 0.500, loss: 1.292,data_loss: 1.292, reg_loss: 0.000 lr: 0.000913\n",
      " step: 102, acc: 0.461, loss: 1.376,data_loss: 1.376, reg_loss: 0.000 lr: 0.000913\n",
      " step: 103, acc: 0.547, loss: 1.252,data_loss: 1.252, reg_loss: 0.000 lr: 0.000913\n",
      " step: 104, acc: 0.531, loss: 1.243,data_loss: 1.243, reg_loss: 0.000 lr: 0.000913\n",
      " step: 105, acc: 0.453, loss: 1.406,data_loss: 1.406, reg_loss: 0.000 lr: 0.000913\n",
      " step: 106, acc: 0.531, loss: 1.254,data_loss: 1.254, reg_loss: 0.000 lr: 0.000913\n",
      " step: 107, acc: 0.492, loss: 1.350,data_loss: 1.350, reg_loss: 0.000 lr: 0.000913\n",
      " step: 108, acc: 0.438, loss: 1.396,data_loss: 1.396, reg_loss: 0.000 lr: 0.000913\n",
      " step: 109, acc: 0.484, loss: 1.399,data_loss: 1.399, reg_loss: 0.000 lr: 0.000913\n",
      " step: 110, acc: 0.500, loss: 1.303,data_loss: 1.303, reg_loss: 0.000 lr: 0.000913\n",
      " step: 111, acc: 0.445, loss: 1.341,data_loss: 1.341, reg_loss: 0.000 lr: 0.000913\n",
      " step: 112, acc: 0.445, loss: 1.358,data_loss: 1.358, reg_loss: 0.000 lr: 0.000913\n",
      " step: 113, acc: 0.547, loss: 1.315,data_loss: 1.315, reg_loss: 0.000 lr: 0.000913\n",
      " step: 114, acc: 0.469, loss: 1.385,data_loss: 1.385, reg_loss: 0.000 lr: 0.000913\n",
      " step: 115, acc: 0.453, loss: 1.406,data_loss: 1.406, reg_loss: 0.000 lr: 0.000913\n",
      " step: 116, acc: 0.508, loss: 1.460,data_loss: 1.460, reg_loss: 0.000 lr: 0.000913\n",
      " step: 117, acc: 0.508, loss: 1.298,data_loss: 1.298, reg_loss: 0.000 lr: 0.000913\n",
      " step: 118, acc: 0.414, loss: 1.357,data_loss: 1.357, reg_loss: 0.000 lr: 0.000912\n",
      " step: 119, acc: 0.562, loss: 1.221,data_loss: 1.221, reg_loss: 0.000 lr: 0.000912\n",
      " step: 120, acc: 0.445, loss: 1.392,data_loss: 1.392, reg_loss: 0.000 lr: 0.000912\n",
      " step: 121, acc: 0.438, loss: 1.457,data_loss: 1.457, reg_loss: 0.000 lr: 0.000912\n",
      " step: 122, acc: 0.406, loss: 1.411,data_loss: 1.411, reg_loss: 0.000 lr: 0.000912\n",
      " step: 123, acc: 0.461, loss: 1.452,data_loss: 1.452, reg_loss: 0.000 lr: 0.000912\n",
      " step: 124, acc: 0.469, loss: 1.405,data_loss: 1.405, reg_loss: 0.000 lr: 0.000912\n",
      " step: 125, acc: 0.523, loss: 1.304,data_loss: 1.304, reg_loss: 0.000 lr: 0.000912\n",
      " step: 126, acc: 0.453, loss: 1.377,data_loss: 1.377, reg_loss: 0.000 lr: 0.000912\n",
      " step: 127, acc: 0.422, loss: 1.419,data_loss: 1.419, reg_loss: 0.000 lr: 0.000912\n",
      " step: 128, acc: 0.422, loss: 1.300,data_loss: 1.300, reg_loss: 0.000 lr: 0.000912\n",
      " step: 129, acc: 0.492, loss: 1.370,data_loss: 1.370, reg_loss: 0.000 lr: 0.000912\n",
      " step: 130, acc: 0.406, loss: 1.438,data_loss: 1.438, reg_loss: 0.000 lr: 0.000912\n",
      " step: 131, acc: 0.453, loss: 1.327,data_loss: 1.327, reg_loss: 0.000 lr: 0.000912\n",
      " step: 132, acc: 0.469, loss: 1.349,data_loss: 1.349, reg_loss: 0.000 lr: 0.000912\n",
      " step: 133, acc: 0.469, loss: 1.454,data_loss: 1.454, reg_loss: 0.000 lr: 0.000912\n",
      " step: 134, acc: 0.578, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000912\n",
      " step: 135, acc: 0.477, loss: 1.330,data_loss: 1.330, reg_loss: 0.000 lr: 0.000912\n",
      " step: 136, acc: 0.414, loss: 1.391,data_loss: 1.391, reg_loss: 0.000 lr: 0.000912\n",
      " step: 137, acc: 0.570, loss: 1.293,data_loss: 1.293, reg_loss: 0.000 lr: 0.000912\n",
      " step: 138, acc: 0.484, loss: 1.232,data_loss: 1.232, reg_loss: 0.000 lr: 0.000912\n",
      " step: 139, acc: 0.547, loss: 1.261,data_loss: 1.261, reg_loss: 0.000 lr: 0.000912\n",
      " step: 140, acc: 0.508, loss: 1.255,data_loss: 1.255, reg_loss: 0.000 lr: 0.000912\n",
      " step: 141, acc: 0.430, loss: 1.354,data_loss: 1.354, reg_loss: 0.000 lr: 0.000912\n",
      " step: 142, acc: 0.516, loss: 1.251,data_loss: 1.251, reg_loss: 0.000 lr: 0.000911\n",
      " step: 143, acc: 0.492, loss: 1.360,data_loss: 1.360, reg_loss: 0.000 lr: 0.000911\n",
      " step: 144, acc: 0.445, loss: 1.429,data_loss: 1.429, reg_loss: 0.000 lr: 0.000911\n",
      " step: 145, acc: 0.516, loss: 1.343,data_loss: 1.343, reg_loss: 0.000 lr: 0.000911\n",
      " step: 146, acc: 0.469, loss: 1.361,data_loss: 1.361, reg_loss: 0.000 lr: 0.000911\n",
      " step: 147, acc: 0.477, loss: 1.309,data_loss: 1.309, reg_loss: 0.000 lr: 0.000911\n",
      " step: 148, acc: 0.477, loss: 1.285,data_loss: 1.285, reg_loss: 0.000 lr: 0.000911\n",
      " step: 149, acc: 0.422, loss: 1.437,data_loss: 1.437, reg_loss: 0.000 lr: 0.000911\n",
      " step: 150, acc: 0.523, loss: 1.240,data_loss: 1.240, reg_loss: 0.000 lr: 0.000911\n",
      " step: 151, acc: 0.422, loss: 1.439,data_loss: 1.439, reg_loss: 0.000 lr: 0.000911\n",
      " step: 152, acc: 0.531, loss: 1.246,data_loss: 1.246, reg_loss: 0.000 lr: 0.000911\n",
      " step: 153, acc: 0.492, loss: 1.258,data_loss: 1.258, reg_loss: 0.000 lr: 0.000911\n",
      " step: 154, acc: 0.438, loss: 1.445,data_loss: 1.445, reg_loss: 0.000 lr: 0.000911\n",
      " step: 155, acc: 0.508, loss: 1.264,data_loss: 1.264, reg_loss: 0.000 lr: 0.000911\n",
      " step: 156, acc: 0.469, loss: 1.299,data_loss: 1.299, reg_loss: 0.000 lr: 0.000911\n",
      " step: 157, acc: 0.570, loss: 1.189,data_loss: 1.189, reg_loss: 0.000 lr: 0.000911\n",
      " step: 158, acc: 0.438, loss: 1.392,data_loss: 1.392, reg_loss: 0.000 lr: 0.000911\n",
      " step: 159, acc: 0.469, loss: 1.432,data_loss: 1.432, reg_loss: 0.000 lr: 0.000911\n",
      " step: 160, acc: 0.391, loss: 1.489,data_loss: 1.489, reg_loss: 0.000 lr: 0.000911\n",
      " step: 161, acc: 0.539, loss: 1.328,data_loss: 1.328, reg_loss: 0.000 lr: 0.000911\n",
      " step: 162, acc: 0.445, loss: 1.356,data_loss: 1.356, reg_loss: 0.000 lr: 0.000911\n",
      " step: 163, acc: 0.477, loss: 1.312,data_loss: 1.312, reg_loss: 0.000 lr: 0.000911\n",
      " step: 164, acc: 0.469, loss: 1.281,data_loss: 1.281, reg_loss: 0.000 lr: 0.000911\n",
      " step: 165, acc: 0.414, loss: 1.427,data_loss: 1.427, reg_loss: 0.000 lr: 0.000911\n",
      " step: 166, acc: 0.477, loss: 1.403,data_loss: 1.403, reg_loss: 0.000 lr: 0.000910\n",
      " step: 167, acc: 0.406, loss: 1.375,data_loss: 1.375, reg_loss: 0.000 lr: 0.000910\n",
      " step: 168, acc: 0.469, loss: 1.313,data_loss: 1.313, reg_loss: 0.000 lr: 0.000910\n",
      " step: 169, acc: 0.477, loss: 1.246,data_loss: 1.246, reg_loss: 0.000 lr: 0.000910\n",
      " step: 170, acc: 0.469, loss: 1.391,data_loss: 1.391, reg_loss: 0.000 lr: 0.000910\n",
      " step: 171, acc: 0.523, loss: 1.291,data_loss: 1.291, reg_loss: 0.000 lr: 0.000910\n",
      " step: 172, acc: 0.477, loss: 1.382,data_loss: 1.382, reg_loss: 0.000 lr: 0.000910\n",
      " step: 173, acc: 0.516, loss: 1.393,data_loss: 1.393, reg_loss: 0.000 lr: 0.000910\n",
      " step: 174, acc: 0.477, loss: 1.318,data_loss: 1.318, reg_loss: 0.000 lr: 0.000910\n",
      " step: 175, acc: 0.445, loss: 1.384,data_loss: 1.384, reg_loss: 0.000 lr: 0.000910\n",
      " step: 176, acc: 0.469, loss: 1.421,data_loss: 1.421, reg_loss: 0.000 lr: 0.000910\n",
      " step: 177, acc: 0.453, loss: 1.323,data_loss: 1.323, reg_loss: 0.000 lr: 0.000910\n",
      " step: 178, acc: 0.406, loss: 1.580,data_loss: 1.580, reg_loss: 0.000 lr: 0.000910\n",
      " step: 179, acc: 0.555, loss: 1.116,data_loss: 1.116, reg_loss: 0.000 lr: 0.000910\n",
      " step: 180, acc: 0.570, loss: 1.128,data_loss: 1.128, reg_loss: 0.000 lr: 0.000910\n",
      " step: 181, acc: 0.453, loss: 1.302,data_loss: 1.302, reg_loss: 0.000 lr: 0.000910\n",
      " step: 182, acc: 0.484, loss: 1.264,data_loss: 1.264, reg_loss: 0.000 lr: 0.000910\n",
      " step: 183, acc: 0.531, loss: 1.280,data_loss: 1.280, reg_loss: 0.000 lr: 0.000910\n",
      " step: 184, acc: 0.492, loss: 1.364,data_loss: 1.364, reg_loss: 0.000 lr: 0.000910\n",
      " step: 185, acc: 0.484, loss: 1.406,data_loss: 1.406, reg_loss: 0.000 lr: 0.000910\n",
      " step: 186, acc: 0.484, loss: 1.275,data_loss: 1.275, reg_loss: 0.000 lr: 0.000910\n",
      " step: 187, acc: 0.516, loss: 1.299,data_loss: 1.299, reg_loss: 0.000 lr: 0.000910\n",
      " step: 188, acc: 0.383, loss: 1.505,data_loss: 1.505, reg_loss: 0.000 lr: 0.000910\n",
      " step: 189, acc: 0.445, loss: 1.353,data_loss: 1.353, reg_loss: 0.000 lr: 0.000910\n",
      " step: 190, acc: 0.469, loss: 1.365,data_loss: 1.365, reg_loss: 0.000 lr: 0.000910\n",
      " step: 191, acc: 0.461, loss: 1.401,data_loss: 1.401, reg_loss: 0.000 lr: 0.000909\n",
      " step: 192, acc: 0.586, loss: 1.226,data_loss: 1.226, reg_loss: 0.000 lr: 0.000909\n",
      " step: 193, acc: 0.414, loss: 1.355,data_loss: 1.355, reg_loss: 0.000 lr: 0.000909\n",
      " step: 194, acc: 0.492, loss: 1.367,data_loss: 1.367, reg_loss: 0.000 lr: 0.000909\n",
      " step: 195, acc: 0.516, loss: 1.263,data_loss: 1.263, reg_loss: 0.000 lr: 0.000909\n",
      " step: 196, acc: 0.469, loss: 1.299,data_loss: 1.299, reg_loss: 0.000 lr: 0.000909\n",
      " step: 197, acc: 0.500, loss: 1.288,data_loss: 1.288, reg_loss: 0.000 lr: 0.000909\n",
      " step: 198, acc: 0.586, loss: 1.205,data_loss: 1.205, reg_loss: 0.000 lr: 0.000909\n",
      " step: 199, acc: 0.453, loss: 1.403,data_loss: 1.403, reg_loss: 0.000 lr: 0.000909\n",
      " step: 200, acc: 0.469, loss: 1.318,data_loss: 1.318, reg_loss: 0.000 lr: 0.000909\n",
      " step: 201, acc: 0.531, loss: 1.293,data_loss: 1.293, reg_loss: 0.000 lr: 0.000909\n",
      " step: 202, acc: 0.461, loss: 1.335,data_loss: 1.335, reg_loss: 0.000 lr: 0.000909\n",
      " step: 203, acc: 0.422, loss: 1.367,data_loss: 1.367, reg_loss: 0.000 lr: 0.000909\n",
      " step: 204, acc: 0.539, loss: 1.178,data_loss: 1.178, reg_loss: 0.000 lr: 0.000909\n",
      " step: 205, acc: 0.445, loss: 1.481,data_loss: 1.481, reg_loss: 0.000 lr: 0.000909\n",
      " step: 206, acc: 0.469, loss: 1.255,data_loss: 1.255, reg_loss: 0.000 lr: 0.000909\n",
      " step: 207, acc: 0.531, loss: 1.291,data_loss: 1.291, reg_loss: 0.000 lr: 0.000909\n",
      " step: 208, acc: 0.555, loss: 1.252,data_loss: 1.252, reg_loss: 0.000 lr: 0.000909\n",
      " step: 209, acc: 0.438, loss: 1.345,data_loss: 1.345, reg_loss: 0.000 lr: 0.000909\n",
      " step: 210, acc: 0.555, loss: 1.231,data_loss: 1.231, reg_loss: 0.000 lr: 0.000909\n",
      " step: 211, acc: 0.453, loss: 1.340,data_loss: 1.340, reg_loss: 0.000 lr: 0.000909\n",
      " step: 212, acc: 0.508, loss: 1.271,data_loss: 1.271, reg_loss: 0.000 lr: 0.000909\n",
      " step: 213, acc: 0.492, loss: 1.323,data_loss: 1.323, reg_loss: 0.000 lr: 0.000909\n",
      " step: 214, acc: 0.453, loss: 1.414,data_loss: 1.414, reg_loss: 0.000 lr: 0.000909\n",
      " step: 215, acc: 0.555, loss: 1.234,data_loss: 1.234, reg_loss: 0.000 lr: 0.000908\n",
      " step: 216, acc: 0.516, loss: 1.282,data_loss: 1.282, reg_loss: 0.000 lr: 0.000908\n",
      " step: 217, acc: 0.422, loss: 1.391,data_loss: 1.391, reg_loss: 0.000 lr: 0.000908\n",
      " step: 218, acc: 0.508, loss: 1.363,data_loss: 1.363, reg_loss: 0.000 lr: 0.000908\n",
      " step: 219, acc: 0.492, loss: 1.284,data_loss: 1.284, reg_loss: 0.000 lr: 0.000908\n",
      " step: 220, acc: 0.492, loss: 1.298,data_loss: 1.298, reg_loss: 0.000 lr: 0.000908\n",
      " step: 221, acc: 0.531, loss: 1.302,data_loss: 1.302, reg_loss: 0.000 lr: 0.000908\n",
      " step: 222, acc: 0.484, loss: 1.310,data_loss: 1.310, reg_loss: 0.000 lr: 0.000908\n",
      " step: 223, acc: 0.484, loss: 1.319,data_loss: 1.319, reg_loss: 0.000 lr: 0.000908\n",
      " step: 224, acc: 0.595, loss: 1.164,data_loss: 1.164, reg_loss: 0.000 lr: 0.000908\n",
      "training , acc: 0.479, loss: 1.342,data_loss: 1.342, reg_loss: 0.000 lr: 0.000908\n",
      "validation, acc:0.426 ,loss: 1.493 \n",
      "epoch: 10\n",
      " step: 0, acc: 0.469, loss: 1.424,data_loss: 1.424, reg_loss: 0.000 lr: 0.000908\n",
      " step: 1, acc: 0.469, loss: 1.360,data_loss: 1.360, reg_loss: 0.000 lr: 0.000908\n",
      " step: 2, acc: 0.516, loss: 1.367,data_loss: 1.367, reg_loss: 0.000 lr: 0.000908\n",
      " step: 3, acc: 0.500, loss: 1.319,data_loss: 1.319, reg_loss: 0.000 lr: 0.000908\n",
      " step: 4, acc: 0.555, loss: 1.211,data_loss: 1.211, reg_loss: 0.000 lr: 0.000908\n",
      " step: 5, acc: 0.453, loss: 1.403,data_loss: 1.403, reg_loss: 0.000 lr: 0.000908\n",
      " step: 6, acc: 0.508, loss: 1.300,data_loss: 1.300, reg_loss: 0.000 lr: 0.000908\n",
      " step: 7, acc: 0.469, loss: 1.406,data_loss: 1.406, reg_loss: 0.000 lr: 0.000908\n",
      " step: 8, acc: 0.539, loss: 1.244,data_loss: 1.244, reg_loss: 0.000 lr: 0.000908\n",
      " step: 9, acc: 0.516, loss: 1.277,data_loss: 1.277, reg_loss: 0.000 lr: 0.000908\n",
      " step: 10, acc: 0.562, loss: 1.299,data_loss: 1.299, reg_loss: 0.000 lr: 0.000908\n",
      " step: 11, acc: 0.570, loss: 1.324,data_loss: 1.324, reg_loss: 0.000 lr: 0.000908\n",
      " step: 12, acc: 0.531, loss: 1.261,data_loss: 1.261, reg_loss: 0.000 lr: 0.000908\n",
      " step: 13, acc: 0.508, loss: 1.286,data_loss: 1.286, reg_loss: 0.000 lr: 0.000908\n",
      " step: 14, acc: 0.453, loss: 1.407,data_loss: 1.407, reg_loss: 0.000 lr: 0.000907\n",
      " step: 15, acc: 0.516, loss: 1.249,data_loss: 1.249, reg_loss: 0.000 lr: 0.000907\n",
      " step: 16, acc: 0.469, loss: 1.342,data_loss: 1.342, reg_loss: 0.000 lr: 0.000907\n",
      " step: 17, acc: 0.492, loss: 1.330,data_loss: 1.330, reg_loss: 0.000 lr: 0.000907\n",
      " step: 18, acc: 0.469, loss: 1.395,data_loss: 1.395, reg_loss: 0.000 lr: 0.000907\n",
      " step: 19, acc: 0.547, loss: 1.130,data_loss: 1.130, reg_loss: 0.000 lr: 0.000907\n",
      " step: 20, acc: 0.477, loss: 1.276,data_loss: 1.276, reg_loss: 0.000 lr: 0.000907\n",
      " step: 21, acc: 0.516, loss: 1.341,data_loss: 1.341, reg_loss: 0.000 lr: 0.000907\n",
      " step: 22, acc: 0.453, loss: 1.343,data_loss: 1.343, reg_loss: 0.000 lr: 0.000907\n",
      " step: 23, acc: 0.438, loss: 1.368,data_loss: 1.368, reg_loss: 0.000 lr: 0.000907\n",
      " step: 24, acc: 0.453, loss: 1.330,data_loss: 1.330, reg_loss: 0.000 lr: 0.000907\n",
      " step: 25, acc: 0.516, loss: 1.305,data_loss: 1.305, reg_loss: 0.000 lr: 0.000907\n",
      " step: 26, acc: 0.508, loss: 1.302,data_loss: 1.302, reg_loss: 0.000 lr: 0.000907\n",
      " step: 27, acc: 0.500, loss: 1.285,data_loss: 1.285, reg_loss: 0.000 lr: 0.000907\n",
      " step: 28, acc: 0.461, loss: 1.307,data_loss: 1.307, reg_loss: 0.000 lr: 0.000907\n",
      " step: 29, acc: 0.461, loss: 1.394,data_loss: 1.394, reg_loss: 0.000 lr: 0.000907\n",
      " step: 30, acc: 0.445, loss: 1.463,data_loss: 1.463, reg_loss: 0.000 lr: 0.000907\n",
      " step: 31, acc: 0.547, loss: 1.169,data_loss: 1.169, reg_loss: 0.000 lr: 0.000907\n",
      " step: 32, acc: 0.492, loss: 1.311,data_loss: 1.311, reg_loss: 0.000 lr: 0.000907\n",
      " step: 33, acc: 0.477, loss: 1.388,data_loss: 1.388, reg_loss: 0.000 lr: 0.000907\n",
      " step: 34, acc: 0.492, loss: 1.319,data_loss: 1.319, reg_loss: 0.000 lr: 0.000907\n",
      " step: 35, acc: 0.492, loss: 1.220,data_loss: 1.220, reg_loss: 0.000 lr: 0.000907\n",
      " step: 36, acc: 0.539, loss: 1.215,data_loss: 1.215, reg_loss: 0.000 lr: 0.000907\n",
      " step: 37, acc: 0.492, loss: 1.341,data_loss: 1.341, reg_loss: 0.000 lr: 0.000907\n",
      " step: 38, acc: 0.477, loss: 1.419,data_loss: 1.419, reg_loss: 0.000 lr: 0.000906\n",
      " step: 39, acc: 0.406, loss: 1.449,data_loss: 1.449, reg_loss: 0.000 lr: 0.000906\n",
      " step: 40, acc: 0.508, loss: 1.323,data_loss: 1.323, reg_loss: 0.000 lr: 0.000906\n",
      " step: 41, acc: 0.461, loss: 1.461,data_loss: 1.461, reg_loss: 0.000 lr: 0.000906\n",
      " step: 42, acc: 0.500, loss: 1.346,data_loss: 1.346, reg_loss: 0.000 lr: 0.000906\n",
      " step: 43, acc: 0.477, loss: 1.330,data_loss: 1.330, reg_loss: 0.000 lr: 0.000906\n",
      " step: 44, acc: 0.445, loss: 1.425,data_loss: 1.425, reg_loss: 0.000 lr: 0.000906\n",
      " step: 45, acc: 0.516, loss: 1.275,data_loss: 1.275, reg_loss: 0.000 lr: 0.000906\n",
      " step: 46, acc: 0.484, loss: 1.384,data_loss: 1.384, reg_loss: 0.000 lr: 0.000906\n",
      " step: 47, acc: 0.430, loss: 1.347,data_loss: 1.347, reg_loss: 0.000 lr: 0.000906\n",
      " step: 48, acc: 0.531, loss: 1.239,data_loss: 1.239, reg_loss: 0.000 lr: 0.000906\n",
      " step: 49, acc: 0.492, loss: 1.284,data_loss: 1.284, reg_loss: 0.000 lr: 0.000906\n",
      " step: 50, acc: 0.523, loss: 1.175,data_loss: 1.175, reg_loss: 0.000 lr: 0.000906\n",
      " step: 51, acc: 0.438, loss: 1.392,data_loss: 1.392, reg_loss: 0.000 lr: 0.000906\n",
      " step: 52, acc: 0.438, loss: 1.326,data_loss: 1.326, reg_loss: 0.000 lr: 0.000906\n",
      " step: 53, acc: 0.594, loss: 1.372,data_loss: 1.372, reg_loss: 0.000 lr: 0.000906\n",
      " step: 54, acc: 0.414, loss: 1.567,data_loss: 1.567, reg_loss: 0.000 lr: 0.000906\n",
      " step: 55, acc: 0.477, loss: 1.391,data_loss: 1.391, reg_loss: 0.000 lr: 0.000906\n",
      " step: 56, acc: 0.523, loss: 1.255,data_loss: 1.255, reg_loss: 0.000 lr: 0.000906\n",
      " step: 57, acc: 0.539, loss: 1.230,data_loss: 1.230, reg_loss: 0.000 lr: 0.000906\n",
      " step: 58, acc: 0.508, loss: 1.367,data_loss: 1.367, reg_loss: 0.000 lr: 0.000906\n",
      " step: 59, acc: 0.531, loss: 1.344,data_loss: 1.344, reg_loss: 0.000 lr: 0.000906\n",
      " step: 60, acc: 0.477, loss: 1.254,data_loss: 1.254, reg_loss: 0.000 lr: 0.000906\n",
      " step: 61, acc: 0.461, loss: 1.449,data_loss: 1.449, reg_loss: 0.000 lr: 0.000906\n",
      " step: 62, acc: 0.484, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000906\n",
      " step: 63, acc: 0.492, loss: 1.330,data_loss: 1.330, reg_loss: 0.000 lr: 0.000905\n",
      " step: 64, acc: 0.500, loss: 1.364,data_loss: 1.364, reg_loss: 0.000 lr: 0.000905\n",
      " step: 65, acc: 0.461, loss: 1.354,data_loss: 1.354, reg_loss: 0.000 lr: 0.000905\n",
      " step: 66, acc: 0.531, loss: 1.195,data_loss: 1.195, reg_loss: 0.000 lr: 0.000905\n",
      " step: 67, acc: 0.500, loss: 1.310,data_loss: 1.310, reg_loss: 0.000 lr: 0.000905\n",
      " step: 68, acc: 0.438, loss: 1.359,data_loss: 1.359, reg_loss: 0.000 lr: 0.000905\n",
      " step: 69, acc: 0.445, loss: 1.416,data_loss: 1.416, reg_loss: 0.000 lr: 0.000905\n",
      " step: 70, acc: 0.547, loss: 1.235,data_loss: 1.235, reg_loss: 0.000 lr: 0.000905\n",
      " step: 71, acc: 0.508, loss: 1.243,data_loss: 1.243, reg_loss: 0.000 lr: 0.000905\n",
      " step: 72, acc: 0.477, loss: 1.345,data_loss: 1.345, reg_loss: 0.000 lr: 0.000905\n",
      " step: 73, acc: 0.438, loss: 1.343,data_loss: 1.343, reg_loss: 0.000 lr: 0.000905\n",
      " step: 74, acc: 0.453, loss: 1.389,data_loss: 1.389, reg_loss: 0.000 lr: 0.000905\n",
      " step: 75, acc: 0.531, loss: 1.238,data_loss: 1.238, reg_loss: 0.000 lr: 0.000905\n",
      " step: 76, acc: 0.555, loss: 1.238,data_loss: 1.238, reg_loss: 0.000 lr: 0.000905\n",
      " step: 77, acc: 0.461, loss: 1.287,data_loss: 1.287, reg_loss: 0.000 lr: 0.000905\n",
      " step: 78, acc: 0.492, loss: 1.261,data_loss: 1.261, reg_loss: 0.000 lr: 0.000905\n",
      " step: 79, acc: 0.492, loss: 1.322,data_loss: 1.322, reg_loss: 0.000 lr: 0.000905\n",
      " step: 80, acc: 0.438, loss: 1.344,data_loss: 1.344, reg_loss: 0.000 lr: 0.000905\n",
      " step: 81, acc: 0.453, loss: 1.459,data_loss: 1.459, reg_loss: 0.000 lr: 0.000905\n",
      " step: 82, acc: 0.539, loss: 1.316,data_loss: 1.316, reg_loss: 0.000 lr: 0.000905\n",
      " step: 83, acc: 0.477, loss: 1.290,data_loss: 1.290, reg_loss: 0.000 lr: 0.000905\n",
      " step: 84, acc: 0.477, loss: 1.264,data_loss: 1.264, reg_loss: 0.000 lr: 0.000905\n",
      " step: 85, acc: 0.523, loss: 1.274,data_loss: 1.274, reg_loss: 0.000 lr: 0.000905\n",
      " step: 86, acc: 0.477, loss: 1.338,data_loss: 1.338, reg_loss: 0.000 lr: 0.000905\n",
      " step: 87, acc: 0.477, loss: 1.252,data_loss: 1.252, reg_loss: 0.000 lr: 0.000904\n",
      " step: 88, acc: 0.531, loss: 1.505,data_loss: 1.505, reg_loss: 0.000 lr: 0.000904\n",
      " step: 89, acc: 0.453, loss: 1.421,data_loss: 1.421, reg_loss: 0.000 lr: 0.000904\n",
      " step: 90, acc: 0.453, loss: 1.385,data_loss: 1.385, reg_loss: 0.000 lr: 0.000904\n",
      " step: 91, acc: 0.469, loss: 1.365,data_loss: 1.365, reg_loss: 0.000 lr: 0.000904\n",
      " step: 92, acc: 0.445, loss: 1.364,data_loss: 1.364, reg_loss: 0.000 lr: 0.000904\n",
      " step: 93, acc: 0.492, loss: 1.271,data_loss: 1.271, reg_loss: 0.000 lr: 0.000904\n",
      " step: 94, acc: 0.398, loss: 1.504,data_loss: 1.504, reg_loss: 0.000 lr: 0.000904\n",
      " step: 95, acc: 0.547, loss: 1.287,data_loss: 1.287, reg_loss: 0.000 lr: 0.000904\n",
      " step: 96, acc: 0.477, loss: 1.358,data_loss: 1.358, reg_loss: 0.000 lr: 0.000904\n",
      " step: 97, acc: 0.555, loss: 1.293,data_loss: 1.293, reg_loss: 0.000 lr: 0.000904\n",
      " step: 98, acc: 0.555, loss: 1.226,data_loss: 1.226, reg_loss: 0.000 lr: 0.000904\n",
      " step: 99, acc: 0.516, loss: 1.217,data_loss: 1.217, reg_loss: 0.000 lr: 0.000904\n",
      " step: 100, acc: 0.477, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000904\n",
      " step: 101, acc: 0.453, loss: 1.310,data_loss: 1.310, reg_loss: 0.000 lr: 0.000904\n",
      " step: 102, acc: 0.500, loss: 1.289,data_loss: 1.289, reg_loss: 0.000 lr: 0.000904\n",
      " step: 103, acc: 0.555, loss: 1.304,data_loss: 1.304, reg_loss: 0.000 lr: 0.000904\n",
      " step: 104, acc: 0.523, loss: 1.241,data_loss: 1.241, reg_loss: 0.000 lr: 0.000904\n",
      " step: 105, acc: 0.469, loss: 1.353,data_loss: 1.353, reg_loss: 0.000 lr: 0.000904\n",
      " step: 106, acc: 0.594, loss: 1.218,data_loss: 1.218, reg_loss: 0.000 lr: 0.000904\n",
      " step: 107, acc: 0.484, loss: 1.275,data_loss: 1.275, reg_loss: 0.000 lr: 0.000904\n",
      " step: 108, acc: 0.500, loss: 1.290,data_loss: 1.290, reg_loss: 0.000 lr: 0.000904\n",
      " step: 109, acc: 0.453, loss: 1.411,data_loss: 1.411, reg_loss: 0.000 lr: 0.000904\n",
      " step: 110, acc: 0.500, loss: 1.326,data_loss: 1.326, reg_loss: 0.000 lr: 0.000904\n",
      " step: 111, acc: 0.508, loss: 1.294,data_loss: 1.294, reg_loss: 0.000 lr: 0.000904\n",
      " step: 112, acc: 0.461, loss: 1.408,data_loss: 1.408, reg_loss: 0.000 lr: 0.000903\n",
      " step: 113, acc: 0.602, loss: 1.225,data_loss: 1.225, reg_loss: 0.000 lr: 0.000903\n",
      " step: 114, acc: 0.461, loss: 1.338,data_loss: 1.338, reg_loss: 0.000 lr: 0.000903\n",
      " step: 115, acc: 0.500, loss: 1.336,data_loss: 1.336, reg_loss: 0.000 lr: 0.000903\n",
      " step: 116, acc: 0.484, loss: 1.324,data_loss: 1.324, reg_loss: 0.000 lr: 0.000903\n",
      " step: 117, acc: 0.461, loss: 1.263,data_loss: 1.263, reg_loss: 0.000 lr: 0.000903\n",
      " step: 118, acc: 0.445, loss: 1.270,data_loss: 1.270, reg_loss: 0.000 lr: 0.000903\n",
      " step: 119, acc: 0.555, loss: 1.225,data_loss: 1.225, reg_loss: 0.000 lr: 0.000903\n",
      " step: 120, acc: 0.422, loss: 1.440,data_loss: 1.440, reg_loss: 0.000 lr: 0.000903\n",
      " step: 121, acc: 0.461, loss: 1.389,data_loss: 1.389, reg_loss: 0.000 lr: 0.000903\n",
      " step: 122, acc: 0.445, loss: 1.287,data_loss: 1.287, reg_loss: 0.000 lr: 0.000903\n",
      " step: 123, acc: 0.477, loss: 1.313,data_loss: 1.313, reg_loss: 0.000 lr: 0.000903\n",
      " step: 124, acc: 0.430, loss: 1.319,data_loss: 1.319, reg_loss: 0.000 lr: 0.000903\n",
      " step: 125, acc: 0.438, loss: 1.322,data_loss: 1.322, reg_loss: 0.000 lr: 0.000903\n",
      " step: 126, acc: 0.484, loss: 1.356,data_loss: 1.356, reg_loss: 0.000 lr: 0.000903\n",
      " step: 127, acc: 0.406, loss: 1.459,data_loss: 1.459, reg_loss: 0.000 lr: 0.000903\n",
      " step: 128, acc: 0.438, loss: 1.342,data_loss: 1.342, reg_loss: 0.000 lr: 0.000903\n",
      " step: 129, acc: 0.539, loss: 1.281,data_loss: 1.281, reg_loss: 0.000 lr: 0.000903\n",
      " step: 130, acc: 0.383, loss: 1.497,data_loss: 1.497, reg_loss: 0.000 lr: 0.000903\n",
      " step: 131, acc: 0.500, loss: 1.299,data_loss: 1.299, reg_loss: 0.000 lr: 0.000903\n",
      " step: 132, acc: 0.461, loss: 1.351,data_loss: 1.351, reg_loss: 0.000 lr: 0.000903\n",
      " step: 133, acc: 0.445, loss: 1.394,data_loss: 1.394, reg_loss: 0.000 lr: 0.000903\n",
      " step: 134, acc: 0.578, loss: 1.130,data_loss: 1.130, reg_loss: 0.000 lr: 0.000903\n",
      " step: 135, acc: 0.453, loss: 1.319,data_loss: 1.319, reg_loss: 0.000 lr: 0.000903\n",
      " step: 136, acc: 0.422, loss: 1.421,data_loss: 1.421, reg_loss: 0.000 lr: 0.000902\n",
      " step: 137, acc: 0.531, loss: 1.244,data_loss: 1.244, reg_loss: 0.000 lr: 0.000902\n",
      " step: 138, acc: 0.578, loss: 1.233,data_loss: 1.233, reg_loss: 0.000 lr: 0.000902\n",
      " step: 139, acc: 0.516, loss: 1.266,data_loss: 1.266, reg_loss: 0.000 lr: 0.000902\n",
      " step: 140, acc: 0.523, loss: 1.159,data_loss: 1.159, reg_loss: 0.000 lr: 0.000902\n",
      " step: 141, acc: 0.375, loss: 1.388,data_loss: 1.388, reg_loss: 0.000 lr: 0.000902\n",
      " step: 142, acc: 0.547, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000902\n",
      " step: 143, acc: 0.477, loss: 1.325,data_loss: 1.325, reg_loss: 0.000 lr: 0.000902\n",
      " step: 144, acc: 0.445, loss: 1.451,data_loss: 1.451, reg_loss: 0.000 lr: 0.000902\n",
      " step: 145, acc: 0.492, loss: 1.360,data_loss: 1.360, reg_loss: 0.000 lr: 0.000902\n",
      " step: 146, acc: 0.484, loss: 1.318,data_loss: 1.318, reg_loss: 0.000 lr: 0.000902\n",
      " step: 147, acc: 0.492, loss: 1.324,data_loss: 1.324, reg_loss: 0.000 lr: 0.000902\n",
      " step: 148, acc: 0.492, loss: 1.231,data_loss: 1.231, reg_loss: 0.000 lr: 0.000902\n",
      " step: 149, acc: 0.461, loss: 1.314,data_loss: 1.314, reg_loss: 0.000 lr: 0.000902\n",
      " step: 150, acc: 0.508, loss: 1.221,data_loss: 1.221, reg_loss: 0.000 lr: 0.000902\n",
      " step: 151, acc: 0.438, loss: 1.346,data_loss: 1.346, reg_loss: 0.000 lr: 0.000902\n",
      " step: 152, acc: 0.555, loss: 1.249,data_loss: 1.249, reg_loss: 0.000 lr: 0.000902\n",
      " step: 153, acc: 0.484, loss: 1.161,data_loss: 1.161, reg_loss: 0.000 lr: 0.000902\n",
      " step: 154, acc: 0.438, loss: 1.412,data_loss: 1.412, reg_loss: 0.000 lr: 0.000902\n",
      " step: 155, acc: 0.547, loss: 1.210,data_loss: 1.210, reg_loss: 0.000 lr: 0.000902\n",
      " step: 156, acc: 0.414, loss: 1.353,data_loss: 1.353, reg_loss: 0.000 lr: 0.000902\n",
      " step: 157, acc: 0.539, loss: 1.282,data_loss: 1.282, reg_loss: 0.000 lr: 0.000902\n",
      " step: 158, acc: 0.430, loss: 1.442,data_loss: 1.442, reg_loss: 0.000 lr: 0.000902\n",
      " step: 159, acc: 0.484, loss: 1.363,data_loss: 1.363, reg_loss: 0.000 lr: 0.000902\n",
      " step: 160, acc: 0.406, loss: 1.547,data_loss: 1.547, reg_loss: 0.000 lr: 0.000902\n",
      " step: 161, acc: 0.477, loss: 1.345,data_loss: 1.345, reg_loss: 0.000 lr: 0.000901\n",
      " step: 162, acc: 0.516, loss: 1.304,data_loss: 1.304, reg_loss: 0.000 lr: 0.000901\n",
      " step: 163, acc: 0.422, loss: 1.309,data_loss: 1.309, reg_loss: 0.000 lr: 0.000901\n",
      " step: 164, acc: 0.547, loss: 1.242,data_loss: 1.242, reg_loss: 0.000 lr: 0.000901\n",
      " step: 165, acc: 0.383, loss: 1.423,data_loss: 1.423, reg_loss: 0.000 lr: 0.000901\n",
      " step: 166, acc: 0.508, loss: 1.314,data_loss: 1.314, reg_loss: 0.000 lr: 0.000901\n",
      " step: 167, acc: 0.445, loss: 1.411,data_loss: 1.411, reg_loss: 0.000 lr: 0.000901\n",
      " step: 168, acc: 0.461, loss: 1.282,data_loss: 1.282, reg_loss: 0.000 lr: 0.000901\n",
      " step: 169, acc: 0.500, loss: 1.317,data_loss: 1.317, reg_loss: 0.000 lr: 0.000901\n",
      " step: 170, acc: 0.445, loss: 1.333,data_loss: 1.333, reg_loss: 0.000 lr: 0.000901\n",
      " step: 171, acc: 0.523, loss: 1.180,data_loss: 1.180, reg_loss: 0.000 lr: 0.000901\n",
      " step: 172, acc: 0.461, loss: 1.303,data_loss: 1.303, reg_loss: 0.000 lr: 0.000901\n",
      " step: 173, acc: 0.547, loss: 1.257,data_loss: 1.257, reg_loss: 0.000 lr: 0.000901\n",
      " step: 174, acc: 0.461, loss: 1.357,data_loss: 1.357, reg_loss: 0.000 lr: 0.000901\n",
      " step: 175, acc: 0.492, loss: 1.337,data_loss: 1.337, reg_loss: 0.000 lr: 0.000901\n",
      " step: 176, acc: 0.438, loss: 1.399,data_loss: 1.399, reg_loss: 0.000 lr: 0.000901\n",
      " step: 177, acc: 0.469, loss: 1.242,data_loss: 1.242, reg_loss: 0.000 lr: 0.000901\n",
      " step: 178, acc: 0.453, loss: 1.485,data_loss: 1.485, reg_loss: 0.000 lr: 0.000901\n",
      " step: 179, acc: 0.570, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000901\n",
      " step: 180, acc: 0.594, loss: 1.164,data_loss: 1.164, reg_loss: 0.000 lr: 0.000901\n",
      " step: 181, acc: 0.523, loss: 1.277,data_loss: 1.277, reg_loss: 0.000 lr: 0.000901\n",
      " step: 182, acc: 0.508, loss: 1.242,data_loss: 1.242, reg_loss: 0.000 lr: 0.000901\n",
      " step: 183, acc: 0.500, loss: 1.313,data_loss: 1.313, reg_loss: 0.000 lr: 0.000901\n",
      " step: 184, acc: 0.461, loss: 1.301,data_loss: 1.301, reg_loss: 0.000 lr: 0.000901\n",
      " step: 185, acc: 0.469, loss: 1.340,data_loss: 1.340, reg_loss: 0.000 lr: 0.000900\n",
      " step: 186, acc: 0.484, loss: 1.273,data_loss: 1.273, reg_loss: 0.000 lr: 0.000900\n",
      " step: 187, acc: 0.523, loss: 1.260,data_loss: 1.260, reg_loss: 0.000 lr: 0.000900\n",
      " step: 188, acc: 0.453, loss: 1.371,data_loss: 1.371, reg_loss: 0.000 lr: 0.000900\n",
      " step: 189, acc: 0.516, loss: 1.347,data_loss: 1.347, reg_loss: 0.000 lr: 0.000900\n",
      " step: 190, acc: 0.430, loss: 1.396,data_loss: 1.396, reg_loss: 0.000 lr: 0.000900\n",
      " step: 191, acc: 0.484, loss: 1.359,data_loss: 1.359, reg_loss: 0.000 lr: 0.000900\n",
      " step: 192, acc: 0.570, loss: 1.291,data_loss: 1.291, reg_loss: 0.000 lr: 0.000900\n",
      " step: 193, acc: 0.477, loss: 1.278,data_loss: 1.278, reg_loss: 0.000 lr: 0.000900\n",
      " step: 194, acc: 0.562, loss: 1.264,data_loss: 1.264, reg_loss: 0.000 lr: 0.000900\n",
      " step: 195, acc: 0.531, loss: 1.218,data_loss: 1.218, reg_loss: 0.000 lr: 0.000900\n",
      " step: 196, acc: 0.484, loss: 1.281,data_loss: 1.281, reg_loss: 0.000 lr: 0.000900\n",
      " step: 197, acc: 0.484, loss: 1.258,data_loss: 1.258, reg_loss: 0.000 lr: 0.000900\n",
      " step: 198, acc: 0.523, loss: 1.213,data_loss: 1.213, reg_loss: 0.000 lr: 0.000900\n",
      " step: 199, acc: 0.414, loss: 1.561,data_loss: 1.561, reg_loss: 0.000 lr: 0.000900\n",
      " step: 200, acc: 0.453, loss: 1.313,data_loss: 1.313, reg_loss: 0.000 lr: 0.000900\n",
      " step: 201, acc: 0.484, loss: 1.238,data_loss: 1.238, reg_loss: 0.000 lr: 0.000900\n",
      " step: 202, acc: 0.500, loss: 1.213,data_loss: 1.213, reg_loss: 0.000 lr: 0.000900\n",
      " step: 203, acc: 0.461, loss: 1.338,data_loss: 1.338, reg_loss: 0.000 lr: 0.000900\n",
      " step: 204, acc: 0.492, loss: 1.234,data_loss: 1.234, reg_loss: 0.000 lr: 0.000900\n",
      " step: 205, acc: 0.461, loss: 1.331,data_loss: 1.331, reg_loss: 0.000 lr: 0.000900\n",
      " step: 206, acc: 0.492, loss: 1.211,data_loss: 1.211, reg_loss: 0.000 lr: 0.000900\n",
      " step: 207, acc: 0.492, loss: 1.319,data_loss: 1.319, reg_loss: 0.000 lr: 0.000900\n",
      " step: 208, acc: 0.578, loss: 1.160,data_loss: 1.160, reg_loss: 0.000 lr: 0.000900\n",
      " step: 209, acc: 0.469, loss: 1.359,data_loss: 1.359, reg_loss: 0.000 lr: 0.000900\n",
      " step: 210, acc: 0.453, loss: 1.314,data_loss: 1.314, reg_loss: 0.000 lr: 0.000899\n",
      " step: 211, acc: 0.539, loss: 1.238,data_loss: 1.238, reg_loss: 0.000 lr: 0.000899\n",
      " step: 212, acc: 0.523, loss: 1.265,data_loss: 1.265, reg_loss: 0.000 lr: 0.000899\n",
      " step: 213, acc: 0.516, loss: 1.316,data_loss: 1.316, reg_loss: 0.000 lr: 0.000899\n",
      " step: 214, acc: 0.477, loss: 1.442,data_loss: 1.442, reg_loss: 0.000 lr: 0.000899\n",
      " step: 215, acc: 0.570, loss: 1.162,data_loss: 1.162, reg_loss: 0.000 lr: 0.000899\n",
      " step: 216, acc: 0.523, loss: 1.289,data_loss: 1.289, reg_loss: 0.000 lr: 0.000899\n",
      " step: 217, acc: 0.477, loss: 1.300,data_loss: 1.300, reg_loss: 0.000 lr: 0.000899\n",
      " step: 218, acc: 0.594, loss: 1.215,data_loss: 1.215, reg_loss: 0.000 lr: 0.000899\n",
      " step: 219, acc: 0.500, loss: 1.246,data_loss: 1.246, reg_loss: 0.000 lr: 0.000899\n",
      " step: 220, acc: 0.492, loss: 1.326,data_loss: 1.326, reg_loss: 0.000 lr: 0.000899\n",
      " step: 221, acc: 0.484, loss: 1.323,data_loss: 1.323, reg_loss: 0.000 lr: 0.000899\n",
      " step: 222, acc: 0.523, loss: 1.280,data_loss: 1.280, reg_loss: 0.000 lr: 0.000899\n",
      " step: 223, acc: 0.484, loss: 1.310,data_loss: 1.310, reg_loss: 0.000 lr: 0.000899\n",
      " step: 224, acc: 0.622, loss: 1.193,data_loss: 1.193, reg_loss: 0.000 lr: 0.000899\n",
      "training , acc: 0.490, loss: 1.316,data_loss: 1.316, reg_loss: 0.000 lr: 0.000899\n",
      "validation, acc:0.429 ,loss: 1.500 \n",
      "epoch: 11\n",
      " step: 0, acc: 0.438, loss: 1.395,data_loss: 1.395, reg_loss: 0.000 lr: 0.000899\n",
      " step: 1, acc: 0.500, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000899\n",
      " step: 2, acc: 0.477, loss: 1.331,data_loss: 1.331, reg_loss: 0.000 lr: 0.000899\n",
      " step: 3, acc: 0.492, loss: 1.307,data_loss: 1.307, reg_loss: 0.000 lr: 0.000899\n",
      " step: 4, acc: 0.539, loss: 1.268,data_loss: 1.268, reg_loss: 0.000 lr: 0.000899\n",
      " step: 5, acc: 0.500, loss: 1.288,data_loss: 1.288, reg_loss: 0.000 lr: 0.000899\n",
      " step: 6, acc: 0.539, loss: 1.312,data_loss: 1.312, reg_loss: 0.000 lr: 0.000899\n",
      " step: 7, acc: 0.516, loss: 1.295,data_loss: 1.295, reg_loss: 0.000 lr: 0.000899\n",
      " step: 8, acc: 0.555, loss: 1.217,data_loss: 1.217, reg_loss: 0.000 lr: 0.000899\n",
      " step: 9, acc: 0.547, loss: 1.281,data_loss: 1.281, reg_loss: 0.000 lr: 0.000899\n",
      " step: 10, acc: 0.539, loss: 1.200,data_loss: 1.200, reg_loss: 0.000 lr: 0.000898\n",
      " step: 11, acc: 0.570, loss: 1.246,data_loss: 1.246, reg_loss: 0.000 lr: 0.000898\n",
      " step: 12, acc: 0.516, loss: 1.373,data_loss: 1.373, reg_loss: 0.000 lr: 0.000898\n",
      " step: 13, acc: 0.492, loss: 1.258,data_loss: 1.258, reg_loss: 0.000 lr: 0.000898\n",
      " step: 14, acc: 0.477, loss: 1.374,data_loss: 1.374, reg_loss: 0.000 lr: 0.000898\n",
      " step: 15, acc: 0.570, loss: 1.225,data_loss: 1.225, reg_loss: 0.000 lr: 0.000898\n",
      " step: 16, acc: 0.477, loss: 1.233,data_loss: 1.233, reg_loss: 0.000 lr: 0.000898\n",
      " step: 17, acc: 0.461, loss: 1.336,data_loss: 1.336, reg_loss: 0.000 lr: 0.000898\n",
      " step: 18, acc: 0.445, loss: 1.386,data_loss: 1.386, reg_loss: 0.000 lr: 0.000898\n",
      " step: 19, acc: 0.555, loss: 1.120,data_loss: 1.120, reg_loss: 0.000 lr: 0.000898\n",
      " step: 20, acc: 0.523, loss: 1.263,data_loss: 1.263, reg_loss: 0.000 lr: 0.000898\n",
      " step: 21, acc: 0.484, loss: 1.309,data_loss: 1.309, reg_loss: 0.000 lr: 0.000898\n",
      " step: 22, acc: 0.500, loss: 1.281,data_loss: 1.281, reg_loss: 0.000 lr: 0.000898\n",
      " step: 23, acc: 0.492, loss: 1.288,data_loss: 1.288, reg_loss: 0.000 lr: 0.000898\n",
      " step: 24, acc: 0.508, loss: 1.301,data_loss: 1.301, reg_loss: 0.000 lr: 0.000898\n",
      " step: 25, acc: 0.578, loss: 1.245,data_loss: 1.245, reg_loss: 0.000 lr: 0.000898\n",
      " step: 26, acc: 0.516, loss: 1.370,data_loss: 1.370, reg_loss: 0.000 lr: 0.000898\n",
      " step: 27, acc: 0.484, loss: 1.284,data_loss: 1.284, reg_loss: 0.000 lr: 0.000898\n",
      " step: 28, acc: 0.477, loss: 1.270,data_loss: 1.270, reg_loss: 0.000 lr: 0.000898\n",
      " step: 29, acc: 0.500, loss: 1.314,data_loss: 1.314, reg_loss: 0.000 lr: 0.000898\n",
      " step: 30, acc: 0.391, loss: 1.572,data_loss: 1.572, reg_loss: 0.000 lr: 0.000898\n",
      " step: 31, acc: 0.562, loss: 1.158,data_loss: 1.158, reg_loss: 0.000 lr: 0.000898\n",
      " step: 32, acc: 0.547, loss: 1.300,data_loss: 1.300, reg_loss: 0.000 lr: 0.000898\n",
      " step: 33, acc: 0.539, loss: 1.252,data_loss: 1.252, reg_loss: 0.000 lr: 0.000898\n",
      " step: 34, acc: 0.539, loss: 1.241,data_loss: 1.241, reg_loss: 0.000 lr: 0.000898\n",
      " step: 35, acc: 0.523, loss: 1.245,data_loss: 1.245, reg_loss: 0.000 lr: 0.000897\n",
      " step: 36, acc: 0.492, loss: 1.209,data_loss: 1.209, reg_loss: 0.000 lr: 0.000897\n",
      " step: 37, acc: 0.453, loss: 1.299,data_loss: 1.299, reg_loss: 0.000 lr: 0.000897\n",
      " step: 38, acc: 0.469, loss: 1.289,data_loss: 1.289, reg_loss: 0.000 lr: 0.000897\n",
      " step: 39, acc: 0.469, loss: 1.349,data_loss: 1.349, reg_loss: 0.000 lr: 0.000897\n",
      " step: 40, acc: 0.484, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000897\n",
      " step: 41, acc: 0.445, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000897\n",
      " step: 42, acc: 0.516, loss: 1.291,data_loss: 1.291, reg_loss: 0.000 lr: 0.000897\n",
      " step: 43, acc: 0.461, loss: 1.370,data_loss: 1.370, reg_loss: 0.000 lr: 0.000897\n",
      " step: 44, acc: 0.516, loss: 1.332,data_loss: 1.332, reg_loss: 0.000 lr: 0.000897\n",
      " step: 45, acc: 0.469, loss: 1.333,data_loss: 1.333, reg_loss: 0.000 lr: 0.000897\n",
      " step: 46, acc: 0.492, loss: 1.349,data_loss: 1.349, reg_loss: 0.000 lr: 0.000897\n",
      " step: 47, acc: 0.461, loss: 1.254,data_loss: 1.254, reg_loss: 0.000 lr: 0.000897\n",
      " step: 48, acc: 0.516, loss: 1.214,data_loss: 1.214, reg_loss: 0.000 lr: 0.000897\n",
      " step: 49, acc: 0.508, loss: 1.299,data_loss: 1.299, reg_loss: 0.000 lr: 0.000897\n",
      " step: 50, acc: 0.570, loss: 1.169,data_loss: 1.169, reg_loss: 0.000 lr: 0.000897\n",
      " step: 51, acc: 0.445, loss: 1.560,data_loss: 1.560, reg_loss: 0.000 lr: 0.000897\n",
      " step: 52, acc: 0.484, loss: 1.342,data_loss: 1.342, reg_loss: 0.000 lr: 0.000897\n",
      " step: 53, acc: 0.523, loss: 1.370,data_loss: 1.370, reg_loss: 0.000 lr: 0.000897\n",
      " step: 54, acc: 0.359, loss: 1.501,data_loss: 1.501, reg_loss: 0.000 lr: 0.000897\n",
      " step: 55, acc: 0.508, loss: 1.239,data_loss: 1.239, reg_loss: 0.000 lr: 0.000897\n",
      " step: 56, acc: 0.477, loss: 1.250,data_loss: 1.250, reg_loss: 0.000 lr: 0.000897\n",
      " step: 57, acc: 0.594, loss: 1.192,data_loss: 1.192, reg_loss: 0.000 lr: 0.000897\n",
      " step: 58, acc: 0.531, loss: 1.381,data_loss: 1.381, reg_loss: 0.000 lr: 0.000897\n",
      " step: 59, acc: 0.562, loss: 1.306,data_loss: 1.306, reg_loss: 0.000 lr: 0.000896\n",
      " step: 60, acc: 0.516, loss: 1.224,data_loss: 1.224, reg_loss: 0.000 lr: 0.000896\n",
      " step: 61, acc: 0.461, loss: 1.350,data_loss: 1.350, reg_loss: 0.000 lr: 0.000896\n",
      " step: 62, acc: 0.477, loss: 1.294,data_loss: 1.294, reg_loss: 0.000 lr: 0.000896\n",
      " step: 63, acc: 0.547, loss: 1.245,data_loss: 1.245, reg_loss: 0.000 lr: 0.000896\n",
      " step: 64, acc: 0.445, loss: 1.381,data_loss: 1.381, reg_loss: 0.000 lr: 0.000896\n",
      " step: 65, acc: 0.523, loss: 1.253,data_loss: 1.253, reg_loss: 0.000 lr: 0.000896\n",
      " step: 66, acc: 0.523, loss: 1.182,data_loss: 1.182, reg_loss: 0.000 lr: 0.000896\n",
      " step: 67, acc: 0.492, loss: 1.297,data_loss: 1.297, reg_loss: 0.000 lr: 0.000896\n",
      " step: 68, acc: 0.477, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000896\n",
      " step: 69, acc: 0.438, loss: 1.366,data_loss: 1.366, reg_loss: 0.000 lr: 0.000896\n",
      " step: 70, acc: 0.492, loss: 1.226,data_loss: 1.226, reg_loss: 0.000 lr: 0.000896\n",
      " step: 71, acc: 0.578, loss: 1.200,data_loss: 1.200, reg_loss: 0.000 lr: 0.000896\n",
      " step: 72, acc: 0.477, loss: 1.236,data_loss: 1.236, reg_loss: 0.000 lr: 0.000896\n",
      " step: 73, acc: 0.492, loss: 1.245,data_loss: 1.245, reg_loss: 0.000 lr: 0.000896\n",
      " step: 74, acc: 0.469, loss: 1.449,data_loss: 1.449, reg_loss: 0.000 lr: 0.000896\n",
      " step: 75, acc: 0.516, loss: 1.265,data_loss: 1.265, reg_loss: 0.000 lr: 0.000896\n",
      " step: 76, acc: 0.531, loss: 1.218,data_loss: 1.218, reg_loss: 0.000 lr: 0.000896\n",
      " step: 77, acc: 0.516, loss: 1.244,data_loss: 1.244, reg_loss: 0.000 lr: 0.000896\n",
      " step: 78, acc: 0.484, loss: 1.157,data_loss: 1.157, reg_loss: 0.000 lr: 0.000896\n",
      " step: 79, acc: 0.492, loss: 1.351,data_loss: 1.351, reg_loss: 0.000 lr: 0.000896\n",
      " step: 80, acc: 0.477, loss: 1.324,data_loss: 1.324, reg_loss: 0.000 lr: 0.000896\n",
      " step: 81, acc: 0.445, loss: 1.433,data_loss: 1.433, reg_loss: 0.000 lr: 0.000896\n",
      " step: 82, acc: 0.570, loss: 1.309,data_loss: 1.309, reg_loss: 0.000 lr: 0.000896\n",
      " step: 83, acc: 0.461, loss: 1.201,data_loss: 1.201, reg_loss: 0.000 lr: 0.000896\n",
      " step: 84, acc: 0.461, loss: 1.317,data_loss: 1.317, reg_loss: 0.000 lr: 0.000895\n",
      " step: 85, acc: 0.555, loss: 1.222,data_loss: 1.222, reg_loss: 0.000 lr: 0.000895\n",
      " step: 86, acc: 0.484, loss: 1.254,data_loss: 1.254, reg_loss: 0.000 lr: 0.000895\n",
      " step: 87, acc: 0.516, loss: 1.301,data_loss: 1.301, reg_loss: 0.000 lr: 0.000895\n",
      " step: 88, acc: 0.531, loss: 1.337,data_loss: 1.337, reg_loss: 0.000 lr: 0.000895\n",
      " step: 89, acc: 0.453, loss: 1.384,data_loss: 1.384, reg_loss: 0.000 lr: 0.000895\n",
      " step: 90, acc: 0.469, loss: 1.237,data_loss: 1.237, reg_loss: 0.000 lr: 0.000895\n",
      " step: 91, acc: 0.500, loss: 1.282,data_loss: 1.282, reg_loss: 0.000 lr: 0.000895\n",
      " step: 92, acc: 0.453, loss: 1.287,data_loss: 1.287, reg_loss: 0.000 lr: 0.000895\n",
      " step: 93, acc: 0.531, loss: 1.285,data_loss: 1.285, reg_loss: 0.000 lr: 0.000895\n",
      " step: 94, acc: 0.359, loss: 1.495,data_loss: 1.495, reg_loss: 0.000 lr: 0.000895\n",
      " step: 95, acc: 0.539, loss: 1.258,data_loss: 1.258, reg_loss: 0.000 lr: 0.000895\n",
      " step: 96, acc: 0.484, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000895\n",
      " step: 97, acc: 0.500, loss: 1.256,data_loss: 1.256, reg_loss: 0.000 lr: 0.000895\n",
      " step: 98, acc: 0.555, loss: 1.183,data_loss: 1.183, reg_loss: 0.000 lr: 0.000895\n",
      " step: 99, acc: 0.617, loss: 1.087,data_loss: 1.087, reg_loss: 0.000 lr: 0.000895\n",
      " step: 100, acc: 0.477, loss: 1.281,data_loss: 1.281, reg_loss: 0.000 lr: 0.000895\n",
      " step: 101, acc: 0.492, loss: 1.227,data_loss: 1.227, reg_loss: 0.000 lr: 0.000895\n",
      " step: 102, acc: 0.438, loss: 1.351,data_loss: 1.351, reg_loss: 0.000 lr: 0.000895\n",
      " step: 103, acc: 0.516, loss: 1.237,data_loss: 1.237, reg_loss: 0.000 lr: 0.000895\n",
      " step: 104, acc: 0.578, loss: 1.144,data_loss: 1.144, reg_loss: 0.000 lr: 0.000895\n",
      " step: 105, acc: 0.531, loss: 1.241,data_loss: 1.241, reg_loss: 0.000 lr: 0.000895\n",
      " step: 106, acc: 0.531, loss: 1.215,data_loss: 1.215, reg_loss: 0.000 lr: 0.000895\n",
      " step: 107, acc: 0.508, loss: 1.252,data_loss: 1.252, reg_loss: 0.000 lr: 0.000895\n",
      " step: 108, acc: 0.477, loss: 1.286,data_loss: 1.286, reg_loss: 0.000 lr: 0.000895\n",
      " step: 109, acc: 0.445, loss: 1.299,data_loss: 1.299, reg_loss: 0.000 lr: 0.000894\n",
      " step: 110, acc: 0.492, loss: 1.325,data_loss: 1.325, reg_loss: 0.000 lr: 0.000894\n",
      " step: 111, acc: 0.453, loss: 1.358,data_loss: 1.358, reg_loss: 0.000 lr: 0.000894\n",
      " step: 112, acc: 0.469, loss: 1.370,data_loss: 1.370, reg_loss: 0.000 lr: 0.000894\n",
      " step: 113, acc: 0.516, loss: 1.306,data_loss: 1.306, reg_loss: 0.000 lr: 0.000894\n",
      " step: 114, acc: 0.469, loss: 1.393,data_loss: 1.393, reg_loss: 0.000 lr: 0.000894\n",
      " step: 115, acc: 0.469, loss: 1.278,data_loss: 1.278, reg_loss: 0.000 lr: 0.000894\n",
      " step: 116, acc: 0.484, loss: 1.402,data_loss: 1.402, reg_loss: 0.000 lr: 0.000894\n",
      " step: 117, acc: 0.508, loss: 1.148,data_loss: 1.148, reg_loss: 0.000 lr: 0.000894\n",
      " step: 118, acc: 0.461, loss: 1.315,data_loss: 1.315, reg_loss: 0.000 lr: 0.000894\n",
      " step: 119, acc: 0.547, loss: 1.160,data_loss: 1.160, reg_loss: 0.000 lr: 0.000894\n",
      " step: 120, acc: 0.492, loss: 1.240,data_loss: 1.240, reg_loss: 0.000 lr: 0.000894\n",
      " step: 121, acc: 0.422, loss: 1.423,data_loss: 1.423, reg_loss: 0.000 lr: 0.000894\n",
      " step: 122, acc: 0.438, loss: 1.366,data_loss: 1.366, reg_loss: 0.000 lr: 0.000894\n",
      " step: 123, acc: 0.422, loss: 1.380,data_loss: 1.380, reg_loss: 0.000 lr: 0.000894\n",
      " step: 124, acc: 0.445, loss: 1.336,data_loss: 1.336, reg_loss: 0.000 lr: 0.000894\n",
      " step: 125, acc: 0.453, loss: 1.351,data_loss: 1.351, reg_loss: 0.000 lr: 0.000894\n",
      " step: 126, acc: 0.484, loss: 1.265,data_loss: 1.265, reg_loss: 0.000 lr: 0.000894\n",
      " step: 127, acc: 0.453, loss: 1.369,data_loss: 1.369, reg_loss: 0.000 lr: 0.000894\n",
      " step: 128, acc: 0.461, loss: 1.278,data_loss: 1.278, reg_loss: 0.000 lr: 0.000894\n",
      " step: 129, acc: 0.523, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000894\n",
      " step: 130, acc: 0.391, loss: 1.407,data_loss: 1.407, reg_loss: 0.000 lr: 0.000894\n",
      " step: 131, acc: 0.500, loss: 1.266,data_loss: 1.266, reg_loss: 0.000 lr: 0.000894\n",
      " step: 132, acc: 0.523, loss: 1.197,data_loss: 1.197, reg_loss: 0.000 lr: 0.000894\n",
      " step: 133, acc: 0.453, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000894\n",
      " step: 134, acc: 0.586, loss: 1.030,data_loss: 1.030, reg_loss: 0.000 lr: 0.000893\n",
      " step: 135, acc: 0.453, loss: 1.347,data_loss: 1.347, reg_loss: 0.000 lr: 0.000893\n",
      " step: 136, acc: 0.453, loss: 1.306,data_loss: 1.306, reg_loss: 0.000 lr: 0.000893\n",
      " step: 137, acc: 0.578, loss: 1.156,data_loss: 1.156, reg_loss: 0.000 lr: 0.000893\n",
      " step: 138, acc: 0.547, loss: 1.234,data_loss: 1.234, reg_loss: 0.000 lr: 0.000893\n",
      " step: 139, acc: 0.547, loss: 1.178,data_loss: 1.178, reg_loss: 0.000 lr: 0.000893\n",
      " step: 140, acc: 0.547, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000893\n",
      " step: 141, acc: 0.477, loss: 1.316,data_loss: 1.316, reg_loss: 0.000 lr: 0.000893\n",
      " step: 142, acc: 0.570, loss: 1.226,data_loss: 1.226, reg_loss: 0.000 lr: 0.000893\n",
      " step: 143, acc: 0.508, loss: 1.273,data_loss: 1.273, reg_loss: 0.000 lr: 0.000893\n",
      " step: 144, acc: 0.477, loss: 1.298,data_loss: 1.298, reg_loss: 0.000 lr: 0.000893\n",
      " step: 145, acc: 0.508, loss: 1.334,data_loss: 1.334, reg_loss: 0.000 lr: 0.000893\n",
      " step: 146, acc: 0.461, loss: 1.270,data_loss: 1.270, reg_loss: 0.000 lr: 0.000893\n",
      " step: 147, acc: 0.523, loss: 1.371,data_loss: 1.371, reg_loss: 0.000 lr: 0.000893\n",
      " step: 148, acc: 0.570, loss: 1.197,data_loss: 1.197, reg_loss: 0.000 lr: 0.000893\n",
      " step: 149, acc: 0.500, loss: 1.330,data_loss: 1.330, reg_loss: 0.000 lr: 0.000893\n",
      " step: 150, acc: 0.562, loss: 1.131,data_loss: 1.131, reg_loss: 0.000 lr: 0.000893\n",
      " step: 151, acc: 0.438, loss: 1.318,data_loss: 1.318, reg_loss: 0.000 lr: 0.000893\n",
      " step: 152, acc: 0.547, loss: 1.119,data_loss: 1.119, reg_loss: 0.000 lr: 0.000893\n",
      " step: 153, acc: 0.508, loss: 1.179,data_loss: 1.179, reg_loss: 0.000 lr: 0.000893\n",
      " step: 154, acc: 0.492, loss: 1.343,data_loss: 1.343, reg_loss: 0.000 lr: 0.000893\n",
      " step: 155, acc: 0.508, loss: 1.216,data_loss: 1.216, reg_loss: 0.000 lr: 0.000893\n",
      " step: 156, acc: 0.445, loss: 1.352,data_loss: 1.352, reg_loss: 0.000 lr: 0.000893\n",
      " step: 157, acc: 0.547, loss: 1.204,data_loss: 1.204, reg_loss: 0.000 lr: 0.000893\n",
      " step: 158, acc: 0.461, loss: 1.395,data_loss: 1.395, reg_loss: 0.000 lr: 0.000893\n",
      " step: 159, acc: 0.547, loss: 1.372,data_loss: 1.372, reg_loss: 0.000 lr: 0.000892\n",
      " step: 160, acc: 0.484, loss: 1.596,data_loss: 1.596, reg_loss: 0.000 lr: 0.000892\n",
      " step: 161, acc: 0.492, loss: 1.299,data_loss: 1.299, reg_loss: 0.000 lr: 0.000892\n",
      " step: 162, acc: 0.531, loss: 1.294,data_loss: 1.294, reg_loss: 0.000 lr: 0.000892\n",
      " step: 163, acc: 0.477, loss: 1.288,data_loss: 1.288, reg_loss: 0.000 lr: 0.000892\n",
      " step: 164, acc: 0.523, loss: 1.178,data_loss: 1.178, reg_loss: 0.000 lr: 0.000892\n",
      " step: 165, acc: 0.461, loss: 1.398,data_loss: 1.398, reg_loss: 0.000 lr: 0.000892\n",
      " step: 166, acc: 0.484, loss: 1.283,data_loss: 1.283, reg_loss: 0.000 lr: 0.000892\n",
      " step: 167, acc: 0.430, loss: 1.324,data_loss: 1.324, reg_loss: 0.000 lr: 0.000892\n",
      " step: 168, acc: 0.500, loss: 1.235,data_loss: 1.235, reg_loss: 0.000 lr: 0.000892\n",
      " step: 169, acc: 0.539, loss: 1.235,data_loss: 1.235, reg_loss: 0.000 lr: 0.000892\n",
      " step: 170, acc: 0.500, loss: 1.304,data_loss: 1.304, reg_loss: 0.000 lr: 0.000892\n",
      " step: 171, acc: 0.578, loss: 1.161,data_loss: 1.161, reg_loss: 0.000 lr: 0.000892\n",
      " step: 172, acc: 0.516, loss: 1.252,data_loss: 1.252, reg_loss: 0.000 lr: 0.000892\n",
      " step: 173, acc: 0.570, loss: 1.257,data_loss: 1.257, reg_loss: 0.000 lr: 0.000892\n",
      " step: 174, acc: 0.555, loss: 1.221,data_loss: 1.221, reg_loss: 0.000 lr: 0.000892\n",
      " step: 175, acc: 0.531, loss: 1.281,data_loss: 1.281, reg_loss: 0.000 lr: 0.000892\n",
      " step: 176, acc: 0.477, loss: 1.347,data_loss: 1.347, reg_loss: 0.000 lr: 0.000892\n",
      " step: 177, acc: 0.531, loss: 1.191,data_loss: 1.191, reg_loss: 0.000 lr: 0.000892\n",
      " step: 178, acc: 0.484, loss: 1.372,data_loss: 1.372, reg_loss: 0.000 lr: 0.000892\n",
      " step: 179, acc: 0.539, loss: 1.092,data_loss: 1.092, reg_loss: 0.000 lr: 0.000892\n",
      " step: 180, acc: 0.570, loss: 1.097,data_loss: 1.097, reg_loss: 0.000 lr: 0.000892\n",
      " step: 181, acc: 0.523, loss: 1.221,data_loss: 1.221, reg_loss: 0.000 lr: 0.000892\n",
      " step: 182, acc: 0.555, loss: 1.209,data_loss: 1.209, reg_loss: 0.000 lr: 0.000892\n",
      " step: 183, acc: 0.555, loss: 1.134,data_loss: 1.134, reg_loss: 0.000 lr: 0.000892\n",
      " step: 184, acc: 0.469, loss: 1.316,data_loss: 1.316, reg_loss: 0.000 lr: 0.000892\n",
      " step: 185, acc: 0.500, loss: 1.373,data_loss: 1.373, reg_loss: 0.000 lr: 0.000891\n",
      " step: 186, acc: 0.484, loss: 1.251,data_loss: 1.251, reg_loss: 0.000 lr: 0.000891\n",
      " step: 187, acc: 0.570, loss: 1.202,data_loss: 1.202, reg_loss: 0.000 lr: 0.000891\n",
      " step: 188, acc: 0.508, loss: 1.295,data_loss: 1.295, reg_loss: 0.000 lr: 0.000891\n",
      " step: 189, acc: 0.523, loss: 1.242,data_loss: 1.242, reg_loss: 0.000 lr: 0.000891\n",
      " step: 190, acc: 0.461, loss: 1.312,data_loss: 1.312, reg_loss: 0.000 lr: 0.000891\n",
      " step: 191, acc: 0.461, loss: 1.472,data_loss: 1.472, reg_loss: 0.000 lr: 0.000891\n",
      " step: 192, acc: 0.516, loss: 1.225,data_loss: 1.225, reg_loss: 0.000 lr: 0.000891\n",
      " step: 193, acc: 0.547, loss: 1.188,data_loss: 1.188, reg_loss: 0.000 lr: 0.000891\n",
      " step: 194, acc: 0.484, loss: 1.289,data_loss: 1.289, reg_loss: 0.000 lr: 0.000891\n",
      " step: 195, acc: 0.484, loss: 1.274,data_loss: 1.274, reg_loss: 0.000 lr: 0.000891\n",
      " step: 196, acc: 0.508, loss: 1.239,data_loss: 1.239, reg_loss: 0.000 lr: 0.000891\n",
      " step: 197, acc: 0.539, loss: 1.274,data_loss: 1.274, reg_loss: 0.000 lr: 0.000891\n",
      " step: 198, acc: 0.508, loss: 1.142,data_loss: 1.142, reg_loss: 0.000 lr: 0.000891\n",
      " step: 199, acc: 0.453, loss: 1.466,data_loss: 1.466, reg_loss: 0.000 lr: 0.000891\n",
      " step: 200, acc: 0.453, loss: 1.282,data_loss: 1.282, reg_loss: 0.000 lr: 0.000891\n",
      " step: 201, acc: 0.570, loss: 1.196,data_loss: 1.196, reg_loss: 0.000 lr: 0.000891\n",
      " step: 202, acc: 0.508, loss: 1.271,data_loss: 1.271, reg_loss: 0.000 lr: 0.000891\n",
      " step: 203, acc: 0.453, loss: 1.293,data_loss: 1.293, reg_loss: 0.000 lr: 0.000891\n",
      " step: 204, acc: 0.555, loss: 1.140,data_loss: 1.140, reg_loss: 0.000 lr: 0.000891\n",
      " step: 205, acc: 0.516, loss: 1.256,data_loss: 1.256, reg_loss: 0.000 lr: 0.000891\n",
      " step: 206, acc: 0.492, loss: 1.192,data_loss: 1.192, reg_loss: 0.000 lr: 0.000891\n",
      " step: 207, acc: 0.484, loss: 1.228,data_loss: 1.228, reg_loss: 0.000 lr: 0.000891\n",
      " step: 208, acc: 0.555, loss: 1.141,data_loss: 1.141, reg_loss: 0.000 lr: 0.000891\n",
      " step: 209, acc: 0.484, loss: 1.271,data_loss: 1.271, reg_loss: 0.000 lr: 0.000891\n",
      " step: 210, acc: 0.531, loss: 1.190,data_loss: 1.190, reg_loss: 0.000 lr: 0.000890\n",
      " step: 211, acc: 0.531, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000890\n",
      " step: 212, acc: 0.508, loss: 1.303,data_loss: 1.303, reg_loss: 0.000 lr: 0.000890\n",
      " step: 213, acc: 0.547, loss: 1.272,data_loss: 1.272, reg_loss: 0.000 lr: 0.000890\n",
      " step: 214, acc: 0.492, loss: 1.325,data_loss: 1.325, reg_loss: 0.000 lr: 0.000890\n",
      " step: 215, acc: 0.539, loss: 1.245,data_loss: 1.245, reg_loss: 0.000 lr: 0.000890\n",
      " step: 216, acc: 0.508, loss: 1.193,data_loss: 1.193, reg_loss: 0.000 lr: 0.000890\n",
      " step: 217, acc: 0.484, loss: 1.305,data_loss: 1.305, reg_loss: 0.000 lr: 0.000890\n",
      " step: 218, acc: 0.578, loss: 1.187,data_loss: 1.187, reg_loss: 0.000 lr: 0.000890\n",
      " step: 219, acc: 0.500, loss: 1.250,data_loss: 1.250, reg_loss: 0.000 lr: 0.000890\n",
      " step: 220, acc: 0.500, loss: 1.217,data_loss: 1.217, reg_loss: 0.000 lr: 0.000890\n",
      " step: 221, acc: 0.523, loss: 1.221,data_loss: 1.221, reg_loss: 0.000 lr: 0.000890\n",
      " step: 222, acc: 0.547, loss: 1.227,data_loss: 1.227, reg_loss: 0.000 lr: 0.000890\n",
      " step: 223, acc: 0.469, loss: 1.328,data_loss: 1.328, reg_loss: 0.000 lr: 0.000890\n",
      " step: 224, acc: 0.568, loss: 1.114,data_loss: 1.114, reg_loss: 0.000 lr: 0.000890\n",
      "training , acc: 0.502, loss: 1.279,data_loss: 1.279, reg_loss: 0.000 lr: 0.000890\n",
      "validation, acc:0.441 ,loss: 1.494 \n",
      "epoch: 12\n",
      " step: 0, acc: 0.422, loss: 1.401,data_loss: 1.401, reg_loss: 0.000 lr: 0.000890\n",
      " step: 1, acc: 0.516, loss: 1.323,data_loss: 1.323, reg_loss: 0.000 lr: 0.000890\n",
      " step: 2, acc: 0.508, loss: 1.213,data_loss: 1.213, reg_loss: 0.000 lr: 0.000890\n",
      " step: 3, acc: 0.516, loss: 1.241,data_loss: 1.241, reg_loss: 0.000 lr: 0.000890\n",
      " step: 4, acc: 0.570, loss: 1.164,data_loss: 1.164, reg_loss: 0.000 lr: 0.000890\n",
      " step: 5, acc: 0.477, loss: 1.302,data_loss: 1.302, reg_loss: 0.000 lr: 0.000890\n",
      " step: 6, acc: 0.523, loss: 1.180,data_loss: 1.180, reg_loss: 0.000 lr: 0.000890\n",
      " step: 7, acc: 0.555, loss: 1.273,data_loss: 1.273, reg_loss: 0.000 lr: 0.000890\n",
      " step: 8, acc: 0.500, loss: 1.171,data_loss: 1.171, reg_loss: 0.000 lr: 0.000890\n",
      " step: 9, acc: 0.555, loss: 1.211,data_loss: 1.211, reg_loss: 0.000 lr: 0.000890\n",
      " step: 10, acc: 0.578, loss: 1.223,data_loss: 1.223, reg_loss: 0.000 lr: 0.000889\n",
      " step: 11, acc: 0.586, loss: 1.225,data_loss: 1.225, reg_loss: 0.000 lr: 0.000889\n",
      " step: 12, acc: 0.570, loss: 1.171,data_loss: 1.171, reg_loss: 0.000 lr: 0.000889\n",
      " step: 13, acc: 0.461, loss: 1.301,data_loss: 1.301, reg_loss: 0.000 lr: 0.000889\n",
      " step: 14, acc: 0.453, loss: 1.306,data_loss: 1.306, reg_loss: 0.000 lr: 0.000889\n",
      " step: 15, acc: 0.609, loss: 1.150,data_loss: 1.150, reg_loss: 0.000 lr: 0.000889\n",
      " step: 16, acc: 0.516, loss: 1.213,data_loss: 1.213, reg_loss: 0.000 lr: 0.000889\n",
      " step: 17, acc: 0.531, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000889\n",
      " step: 18, acc: 0.469, loss: 1.330,data_loss: 1.330, reg_loss: 0.000 lr: 0.000889\n",
      " step: 19, acc: 0.602, loss: 1.046,data_loss: 1.046, reg_loss: 0.000 lr: 0.000889\n",
      " step: 20, acc: 0.523, loss: 1.261,data_loss: 1.261, reg_loss: 0.000 lr: 0.000889\n",
      " step: 21, acc: 0.555, loss: 1.192,data_loss: 1.192, reg_loss: 0.000 lr: 0.000889\n",
      " step: 22, acc: 0.484, loss: 1.340,data_loss: 1.340, reg_loss: 0.000 lr: 0.000889\n",
      " step: 23, acc: 0.539, loss: 1.345,data_loss: 1.345, reg_loss: 0.000 lr: 0.000889\n",
      " step: 24, acc: 0.523, loss: 1.161,data_loss: 1.161, reg_loss: 0.000 lr: 0.000889\n",
      " step: 25, acc: 0.586, loss: 1.194,data_loss: 1.194, reg_loss: 0.000 lr: 0.000889\n",
      " step: 26, acc: 0.492, loss: 1.214,data_loss: 1.214, reg_loss: 0.000 lr: 0.000889\n",
      " step: 27, acc: 0.531, loss: 1.190,data_loss: 1.190, reg_loss: 0.000 lr: 0.000889\n",
      " step: 28, acc: 0.508, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000889\n",
      " step: 29, acc: 0.469, loss: 1.287,data_loss: 1.287, reg_loss: 0.000 lr: 0.000889\n",
      " step: 30, acc: 0.469, loss: 1.414,data_loss: 1.414, reg_loss: 0.000 lr: 0.000889\n",
      " step: 31, acc: 0.539, loss: 1.150,data_loss: 1.150, reg_loss: 0.000 lr: 0.000889\n",
      " step: 32, acc: 0.445, loss: 1.303,data_loss: 1.303, reg_loss: 0.000 lr: 0.000889\n",
      " step: 33, acc: 0.555, loss: 1.257,data_loss: 1.257, reg_loss: 0.000 lr: 0.000889\n",
      " step: 34, acc: 0.500, loss: 1.261,data_loss: 1.261, reg_loss: 0.000 lr: 0.000889\n",
      " step: 35, acc: 0.461, loss: 1.389,data_loss: 1.389, reg_loss: 0.000 lr: 0.000888\n",
      " step: 36, acc: 0.547, loss: 1.186,data_loss: 1.186, reg_loss: 0.000 lr: 0.000888\n",
      " step: 37, acc: 0.516, loss: 1.259,data_loss: 1.259, reg_loss: 0.000 lr: 0.000888\n",
      " step: 38, acc: 0.523, loss: 1.255,data_loss: 1.255, reg_loss: 0.000 lr: 0.000888\n",
      " step: 39, acc: 0.469, loss: 1.343,data_loss: 1.343, reg_loss: 0.000 lr: 0.000888\n",
      " step: 40, acc: 0.508, loss: 1.280,data_loss: 1.280, reg_loss: 0.000 lr: 0.000888\n",
      " step: 41, acc: 0.484, loss: 1.273,data_loss: 1.273, reg_loss: 0.000 lr: 0.000888\n",
      " step: 42, acc: 0.516, loss: 1.290,data_loss: 1.290, reg_loss: 0.000 lr: 0.000888\n",
      " step: 43, acc: 0.438, loss: 1.301,data_loss: 1.301, reg_loss: 0.000 lr: 0.000888\n",
      " step: 44, acc: 0.500, loss: 1.214,data_loss: 1.214, reg_loss: 0.000 lr: 0.000888\n",
      " step: 45, acc: 0.500, loss: 1.327,data_loss: 1.327, reg_loss: 0.000 lr: 0.000888\n",
      " step: 46, acc: 0.508, loss: 1.334,data_loss: 1.334, reg_loss: 0.000 lr: 0.000888\n",
      " step: 47, acc: 0.508, loss: 1.259,data_loss: 1.259, reg_loss: 0.000 lr: 0.000888\n",
      " step: 48, acc: 0.570, loss: 1.133,data_loss: 1.133, reg_loss: 0.000 lr: 0.000888\n",
      " step: 49, acc: 0.500, loss: 1.228,data_loss: 1.228, reg_loss: 0.000 lr: 0.000888\n",
      " step: 50, acc: 0.570, loss: 1.148,data_loss: 1.148, reg_loss: 0.000 lr: 0.000888\n",
      " step: 51, acc: 0.484, loss: 1.281,data_loss: 1.281, reg_loss: 0.000 lr: 0.000888\n",
      " step: 52, acc: 0.453, loss: 1.310,data_loss: 1.310, reg_loss: 0.000 lr: 0.000888\n",
      " step: 53, acc: 0.562, loss: 1.266,data_loss: 1.266, reg_loss: 0.000 lr: 0.000888\n",
      " step: 54, acc: 0.438, loss: 1.419,data_loss: 1.419, reg_loss: 0.000 lr: 0.000888\n",
      " step: 55, acc: 0.539, loss: 1.251,data_loss: 1.251, reg_loss: 0.000 lr: 0.000888\n",
      " step: 56, acc: 0.516, loss: 1.249,data_loss: 1.249, reg_loss: 0.000 lr: 0.000888\n",
      " step: 57, acc: 0.547, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000888\n",
      " step: 58, acc: 0.562, loss: 1.208,data_loss: 1.208, reg_loss: 0.000 lr: 0.000888\n",
      " step: 59, acc: 0.578, loss: 1.205,data_loss: 1.205, reg_loss: 0.000 lr: 0.000888\n",
      " step: 60, acc: 0.555, loss: 1.124,data_loss: 1.124, reg_loss: 0.000 lr: 0.000888\n",
      " step: 61, acc: 0.531, loss: 1.345,data_loss: 1.345, reg_loss: 0.000 lr: 0.000887\n",
      " step: 62, acc: 0.508, loss: 1.280,data_loss: 1.280, reg_loss: 0.000 lr: 0.000887\n",
      " step: 63, acc: 0.484, loss: 1.231,data_loss: 1.231, reg_loss: 0.000 lr: 0.000887\n",
      " step: 64, acc: 0.445, loss: 1.317,data_loss: 1.317, reg_loss: 0.000 lr: 0.000887\n",
      " step: 65, acc: 0.461, loss: 1.291,data_loss: 1.291, reg_loss: 0.000 lr: 0.000887\n",
      " step: 66, acc: 0.578, loss: 1.155,data_loss: 1.155, reg_loss: 0.000 lr: 0.000887\n",
      " step: 67, acc: 0.516, loss: 1.181,data_loss: 1.181, reg_loss: 0.000 lr: 0.000887\n",
      " step: 68, acc: 0.477, loss: 1.286,data_loss: 1.286, reg_loss: 0.000 lr: 0.000887\n",
      " step: 69, acc: 0.375, loss: 1.549,data_loss: 1.549, reg_loss: 0.000 lr: 0.000887\n",
      " step: 70, acc: 0.539, loss: 1.212,data_loss: 1.212, reg_loss: 0.000 lr: 0.000887\n",
      " step: 71, acc: 0.523, loss: 1.219,data_loss: 1.219, reg_loss: 0.000 lr: 0.000887\n",
      " step: 72, acc: 0.500, loss: 1.222,data_loss: 1.222, reg_loss: 0.000 lr: 0.000887\n",
      " step: 73, acc: 0.469, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000887\n",
      " step: 74, acc: 0.469, loss: 1.340,data_loss: 1.340, reg_loss: 0.000 lr: 0.000887\n",
      " step: 75, acc: 0.523, loss: 1.252,data_loss: 1.252, reg_loss: 0.000 lr: 0.000887\n",
      " step: 76, acc: 0.539, loss: 1.154,data_loss: 1.154, reg_loss: 0.000 lr: 0.000887\n",
      " step: 77, acc: 0.523, loss: 1.218,data_loss: 1.218, reg_loss: 0.000 lr: 0.000887\n",
      " step: 78, acc: 0.516, loss: 1.158,data_loss: 1.158, reg_loss: 0.000 lr: 0.000887\n",
      " step: 79, acc: 0.523, loss: 1.313,data_loss: 1.313, reg_loss: 0.000 lr: 0.000887\n",
      " step: 80, acc: 0.469, loss: 1.336,data_loss: 1.336, reg_loss: 0.000 lr: 0.000887\n",
      " step: 81, acc: 0.461, loss: 1.422,data_loss: 1.422, reg_loss: 0.000 lr: 0.000887\n",
      " step: 82, acc: 0.570, loss: 1.211,data_loss: 1.211, reg_loss: 0.000 lr: 0.000887\n",
      " step: 83, acc: 0.508, loss: 1.233,data_loss: 1.233, reg_loss: 0.000 lr: 0.000887\n",
      " step: 84, acc: 0.508, loss: 1.248,data_loss: 1.248, reg_loss: 0.000 lr: 0.000887\n",
      " step: 85, acc: 0.547, loss: 1.244,data_loss: 1.244, reg_loss: 0.000 lr: 0.000887\n",
      " step: 86, acc: 0.516, loss: 1.182,data_loss: 1.182, reg_loss: 0.000 lr: 0.000886\n",
      " step: 87, acc: 0.555, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000886\n",
      " step: 88, acc: 0.516, loss: 1.352,data_loss: 1.352, reg_loss: 0.000 lr: 0.000886\n",
      " step: 89, acc: 0.492, loss: 1.300,data_loss: 1.300, reg_loss: 0.000 lr: 0.000886\n",
      " step: 90, acc: 0.484, loss: 1.236,data_loss: 1.236, reg_loss: 0.000 lr: 0.000886\n",
      " step: 91, acc: 0.477, loss: 1.316,data_loss: 1.316, reg_loss: 0.000 lr: 0.000886\n",
      " step: 92, acc: 0.484, loss: 1.237,data_loss: 1.237, reg_loss: 0.000 lr: 0.000886\n",
      " step: 93, acc: 0.570, loss: 1.265,data_loss: 1.265, reg_loss: 0.000 lr: 0.000886\n",
      " step: 94, acc: 0.430, loss: 1.471,data_loss: 1.471, reg_loss: 0.000 lr: 0.000886\n",
      " step: 95, acc: 0.570, loss: 1.188,data_loss: 1.188, reg_loss: 0.000 lr: 0.000886\n",
      " step: 96, acc: 0.453, loss: 1.359,data_loss: 1.359, reg_loss: 0.000 lr: 0.000886\n",
      " step: 97, acc: 0.562, loss: 1.261,data_loss: 1.261, reg_loss: 0.000 lr: 0.000886\n",
      " step: 98, acc: 0.531, loss: 1.160,data_loss: 1.160, reg_loss: 0.000 lr: 0.000886\n",
      " step: 99, acc: 0.570, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000886\n",
      " step: 100, acc: 0.492, loss: 1.226,data_loss: 1.226, reg_loss: 0.000 lr: 0.000886\n",
      " step: 101, acc: 0.562, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000886\n",
      " step: 102, acc: 0.516, loss: 1.285,data_loss: 1.285, reg_loss: 0.000 lr: 0.000886\n",
      " step: 103, acc: 0.547, loss: 1.197,data_loss: 1.197, reg_loss: 0.000 lr: 0.000886\n",
      " step: 104, acc: 0.523, loss: 1.186,data_loss: 1.186, reg_loss: 0.000 lr: 0.000886\n",
      " step: 105, acc: 0.547, loss: 1.210,data_loss: 1.210, reg_loss: 0.000 lr: 0.000886\n",
      " step: 106, acc: 0.539, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000886\n",
      " step: 107, acc: 0.578, loss: 1.172,data_loss: 1.172, reg_loss: 0.000 lr: 0.000886\n",
      " step: 108, acc: 0.492, loss: 1.247,data_loss: 1.247, reg_loss: 0.000 lr: 0.000886\n",
      " step: 109, acc: 0.508, loss: 1.294,data_loss: 1.294, reg_loss: 0.000 lr: 0.000886\n",
      " step: 110, acc: 0.531, loss: 1.281,data_loss: 1.281, reg_loss: 0.000 lr: 0.000886\n",
      " step: 111, acc: 0.555, loss: 1.231,data_loss: 1.231, reg_loss: 0.000 lr: 0.000886\n",
      " step: 112, acc: 0.492, loss: 1.298,data_loss: 1.298, reg_loss: 0.000 lr: 0.000885\n",
      " step: 113, acc: 0.562, loss: 1.192,data_loss: 1.192, reg_loss: 0.000 lr: 0.000885\n",
      " step: 114, acc: 0.500, loss: 1.270,data_loss: 1.270, reg_loss: 0.000 lr: 0.000885\n",
      " step: 115, acc: 0.477, loss: 1.390,data_loss: 1.390, reg_loss: 0.000 lr: 0.000885\n",
      " step: 116, acc: 0.539, loss: 1.337,data_loss: 1.337, reg_loss: 0.000 lr: 0.000885\n",
      " step: 117, acc: 0.547, loss: 1.182,data_loss: 1.182, reg_loss: 0.000 lr: 0.000885\n",
      " step: 118, acc: 0.484, loss: 1.269,data_loss: 1.269, reg_loss: 0.000 lr: 0.000885\n",
      " step: 119, acc: 0.578, loss: 1.124,data_loss: 1.124, reg_loss: 0.000 lr: 0.000885\n",
      " step: 120, acc: 0.500, loss: 1.206,data_loss: 1.206, reg_loss: 0.000 lr: 0.000885\n",
      " step: 121, acc: 0.500, loss: 1.279,data_loss: 1.279, reg_loss: 0.000 lr: 0.000885\n",
      " step: 122, acc: 0.484, loss: 1.252,data_loss: 1.252, reg_loss: 0.000 lr: 0.000885\n",
      " step: 123, acc: 0.469, loss: 1.326,data_loss: 1.326, reg_loss: 0.000 lr: 0.000885\n",
      " step: 124, acc: 0.508, loss: 1.227,data_loss: 1.227, reg_loss: 0.000 lr: 0.000885\n",
      " step: 125, acc: 0.484, loss: 1.292,data_loss: 1.292, reg_loss: 0.000 lr: 0.000885\n",
      " step: 126, acc: 0.562, loss: 1.190,data_loss: 1.190, reg_loss: 0.000 lr: 0.000885\n",
      " step: 127, acc: 0.508, loss: 1.303,data_loss: 1.303, reg_loss: 0.000 lr: 0.000885\n",
      " step: 128, acc: 0.477, loss: 1.282,data_loss: 1.282, reg_loss: 0.000 lr: 0.000885\n",
      " step: 129, acc: 0.531, loss: 1.254,data_loss: 1.254, reg_loss: 0.000 lr: 0.000885\n",
      " step: 130, acc: 0.445, loss: 1.389,data_loss: 1.389, reg_loss: 0.000 lr: 0.000885\n",
      " step: 131, acc: 0.539, loss: 1.215,data_loss: 1.215, reg_loss: 0.000 lr: 0.000885\n",
      " step: 132, acc: 0.531, loss: 1.181,data_loss: 1.181, reg_loss: 0.000 lr: 0.000885\n",
      " step: 133, acc: 0.555, loss: 1.247,data_loss: 1.247, reg_loss: 0.000 lr: 0.000885\n",
      " step: 134, acc: 0.570, loss: 1.053,data_loss: 1.053, reg_loss: 0.000 lr: 0.000885\n",
      " step: 135, acc: 0.500, loss: 1.255,data_loss: 1.255, reg_loss: 0.000 lr: 0.000885\n",
      " step: 136, acc: 0.484, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000885\n",
      " step: 137, acc: 0.531, loss: 1.223,data_loss: 1.223, reg_loss: 0.000 lr: 0.000884\n",
      " step: 138, acc: 0.578, loss: 1.137,data_loss: 1.137, reg_loss: 0.000 lr: 0.000884\n",
      " step: 139, acc: 0.570, loss: 1.156,data_loss: 1.156, reg_loss: 0.000 lr: 0.000884\n",
      " step: 140, acc: 0.594, loss: 1.124,data_loss: 1.124, reg_loss: 0.000 lr: 0.000884\n",
      " step: 141, acc: 0.477, loss: 1.252,data_loss: 1.252, reg_loss: 0.000 lr: 0.000884\n",
      " step: 142, acc: 0.523, loss: 1.217,data_loss: 1.217, reg_loss: 0.000 lr: 0.000884\n",
      " step: 143, acc: 0.484, loss: 1.250,data_loss: 1.250, reg_loss: 0.000 lr: 0.000884\n",
      " step: 144, acc: 0.500, loss: 1.258,data_loss: 1.258, reg_loss: 0.000 lr: 0.000884\n",
      " step: 145, acc: 0.523, loss: 1.233,data_loss: 1.233, reg_loss: 0.000 lr: 0.000884\n",
      " step: 146, acc: 0.484, loss: 1.324,data_loss: 1.324, reg_loss: 0.000 lr: 0.000884\n",
      " step: 147, acc: 0.570, loss: 1.146,data_loss: 1.146, reg_loss: 0.000 lr: 0.000884\n",
      " step: 148, acc: 0.547, loss: 1.212,data_loss: 1.212, reg_loss: 0.000 lr: 0.000884\n",
      " step: 149, acc: 0.477, loss: 1.297,data_loss: 1.297, reg_loss: 0.000 lr: 0.000884\n",
      " step: 150, acc: 0.555, loss: 1.216,data_loss: 1.216, reg_loss: 0.000 lr: 0.000884\n",
      " step: 151, acc: 0.461, loss: 1.347,data_loss: 1.347, reg_loss: 0.000 lr: 0.000884\n",
      " step: 152, acc: 0.625, loss: 1.056,data_loss: 1.056, reg_loss: 0.000 lr: 0.000884\n",
      " step: 153, acc: 0.539, loss: 1.188,data_loss: 1.188, reg_loss: 0.000 lr: 0.000884\n",
      " step: 154, acc: 0.523, loss: 1.301,data_loss: 1.301, reg_loss: 0.000 lr: 0.000884\n",
      " step: 155, acc: 0.523, loss: 1.279,data_loss: 1.279, reg_loss: 0.000 lr: 0.000884\n",
      " step: 156, acc: 0.484, loss: 1.283,data_loss: 1.283, reg_loss: 0.000 lr: 0.000884\n",
      " step: 157, acc: 0.578, loss: 1.073,data_loss: 1.073, reg_loss: 0.000 lr: 0.000884\n",
      " step: 158, acc: 0.453, loss: 1.343,data_loss: 1.343, reg_loss: 0.000 lr: 0.000884\n",
      " step: 159, acc: 0.508, loss: 1.313,data_loss: 1.313, reg_loss: 0.000 lr: 0.000884\n",
      " step: 160, acc: 0.461, loss: 1.505,data_loss: 1.505, reg_loss: 0.000 lr: 0.000884\n",
      " step: 161, acc: 0.578, loss: 1.279,data_loss: 1.279, reg_loss: 0.000 lr: 0.000884\n",
      " step: 162, acc: 0.594, loss: 1.202,data_loss: 1.202, reg_loss: 0.000 lr: 0.000884\n",
      " step: 163, acc: 0.531, loss: 1.155,data_loss: 1.155, reg_loss: 0.000 lr: 0.000883\n",
      " step: 164, acc: 0.547, loss: 1.195,data_loss: 1.195, reg_loss: 0.000 lr: 0.000883\n",
      " step: 165, acc: 0.461, loss: 1.303,data_loss: 1.303, reg_loss: 0.000 lr: 0.000883\n",
      " step: 166, acc: 0.477, loss: 1.366,data_loss: 1.366, reg_loss: 0.000 lr: 0.000883\n",
      " step: 167, acc: 0.438, loss: 1.358,data_loss: 1.358, reg_loss: 0.000 lr: 0.000883\n",
      " step: 168, acc: 0.539, loss: 1.217,data_loss: 1.217, reg_loss: 0.000 lr: 0.000883\n",
      " step: 169, acc: 0.562, loss: 1.255,data_loss: 1.255, reg_loss: 0.000 lr: 0.000883\n",
      " step: 170, acc: 0.562, loss: 1.257,data_loss: 1.257, reg_loss: 0.000 lr: 0.000883\n",
      " step: 171, acc: 0.562, loss: 1.096,data_loss: 1.096, reg_loss: 0.000 lr: 0.000883\n",
      " step: 172, acc: 0.461, loss: 1.270,data_loss: 1.270, reg_loss: 0.000 lr: 0.000883\n",
      " step: 173, acc: 0.539, loss: 1.269,data_loss: 1.269, reg_loss: 0.000 lr: 0.000883\n",
      " step: 174, acc: 0.508, loss: 1.295,data_loss: 1.295, reg_loss: 0.000 lr: 0.000883\n",
      " step: 175, acc: 0.516, loss: 1.316,data_loss: 1.316, reg_loss: 0.000 lr: 0.000883\n",
      " step: 176, acc: 0.430, loss: 1.355,data_loss: 1.355, reg_loss: 0.000 lr: 0.000883\n",
      " step: 177, acc: 0.500, loss: 1.131,data_loss: 1.131, reg_loss: 0.000 lr: 0.000883\n",
      " step: 178, acc: 0.523, loss: 1.349,data_loss: 1.349, reg_loss: 0.000 lr: 0.000883\n",
      " step: 179, acc: 0.570, loss: 1.063,data_loss: 1.063, reg_loss: 0.000 lr: 0.000883\n",
      " step: 180, acc: 0.578, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000883\n",
      " step: 181, acc: 0.516, loss: 1.225,data_loss: 1.225, reg_loss: 0.000 lr: 0.000883\n",
      " step: 182, acc: 0.562, loss: 1.140,data_loss: 1.140, reg_loss: 0.000 lr: 0.000883\n",
      " step: 183, acc: 0.547, loss: 1.244,data_loss: 1.244, reg_loss: 0.000 lr: 0.000883\n",
      " step: 184, acc: 0.477, loss: 1.241,data_loss: 1.241, reg_loss: 0.000 lr: 0.000883\n",
      " step: 185, acc: 0.508, loss: 1.299,data_loss: 1.299, reg_loss: 0.000 lr: 0.000883\n",
      " step: 186, acc: 0.531, loss: 1.289,data_loss: 1.289, reg_loss: 0.000 lr: 0.000883\n",
      " step: 187, acc: 0.539, loss: 1.267,data_loss: 1.267, reg_loss: 0.000 lr: 0.000883\n",
      " step: 188, acc: 0.445, loss: 1.346,data_loss: 1.346, reg_loss: 0.000 lr: 0.000882\n",
      " step: 189, acc: 0.492, loss: 1.292,data_loss: 1.292, reg_loss: 0.000 lr: 0.000882\n",
      " step: 190, acc: 0.477, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000882\n",
      " step: 191, acc: 0.547, loss: 1.338,data_loss: 1.338, reg_loss: 0.000 lr: 0.000882\n",
      " step: 192, acc: 0.555, loss: 1.136,data_loss: 1.136, reg_loss: 0.000 lr: 0.000882\n",
      " step: 193, acc: 0.539, loss: 1.227,data_loss: 1.227, reg_loss: 0.000 lr: 0.000882\n",
      " step: 194, acc: 0.539, loss: 1.229,data_loss: 1.229, reg_loss: 0.000 lr: 0.000882\n",
      " step: 195, acc: 0.516, loss: 1.199,data_loss: 1.199, reg_loss: 0.000 lr: 0.000882\n",
      " step: 196, acc: 0.539, loss: 1.199,data_loss: 1.199, reg_loss: 0.000 lr: 0.000882\n",
      " step: 197, acc: 0.516, loss: 1.230,data_loss: 1.230, reg_loss: 0.000 lr: 0.000882\n",
      " step: 198, acc: 0.578, loss: 1.118,data_loss: 1.118, reg_loss: 0.000 lr: 0.000882\n",
      " step: 199, acc: 0.492, loss: 1.295,data_loss: 1.295, reg_loss: 0.000 lr: 0.000882\n",
      " step: 200, acc: 0.523, loss: 1.237,data_loss: 1.237, reg_loss: 0.000 lr: 0.000882\n",
      " step: 201, acc: 0.578, loss: 1.143,data_loss: 1.143, reg_loss: 0.000 lr: 0.000882\n",
      " step: 202, acc: 0.547, loss: 1.157,data_loss: 1.157, reg_loss: 0.000 lr: 0.000882\n",
      " step: 203, acc: 0.438, loss: 1.288,data_loss: 1.288, reg_loss: 0.000 lr: 0.000882\n",
      " step: 204, acc: 0.539, loss: 1.171,data_loss: 1.171, reg_loss: 0.000 lr: 0.000882\n",
      " step: 205, acc: 0.508, loss: 1.240,data_loss: 1.240, reg_loss: 0.000 lr: 0.000882\n",
      " step: 206, acc: 0.602, loss: 1.088,data_loss: 1.088, reg_loss: 0.000 lr: 0.000882\n",
      " step: 207, acc: 0.523, loss: 1.235,data_loss: 1.235, reg_loss: 0.000 lr: 0.000882\n",
      " step: 208, acc: 0.641, loss: 1.101,data_loss: 1.101, reg_loss: 0.000 lr: 0.000882\n",
      " step: 209, acc: 0.484, loss: 1.260,data_loss: 1.260, reg_loss: 0.000 lr: 0.000882\n",
      " step: 210, acc: 0.508, loss: 1.238,data_loss: 1.238, reg_loss: 0.000 lr: 0.000882\n",
      " step: 211, acc: 0.539, loss: 1.121,data_loss: 1.121, reg_loss: 0.000 lr: 0.000882\n",
      " step: 212, acc: 0.539, loss: 1.182,data_loss: 1.182, reg_loss: 0.000 lr: 0.000882\n",
      " step: 213, acc: 0.508, loss: 1.280,data_loss: 1.280, reg_loss: 0.000 lr: 0.000882\n",
      " step: 214, acc: 0.461, loss: 1.262,data_loss: 1.262, reg_loss: 0.000 lr: 0.000881\n",
      " step: 215, acc: 0.555, loss: 1.139,data_loss: 1.139, reg_loss: 0.000 lr: 0.000881\n",
      " step: 216, acc: 0.531, loss: 1.208,data_loss: 1.208, reg_loss: 0.000 lr: 0.000881\n",
      " step: 217, acc: 0.531, loss: 1.206,data_loss: 1.206, reg_loss: 0.000 lr: 0.000881\n",
      " step: 218, acc: 0.586, loss: 1.198,data_loss: 1.198, reg_loss: 0.000 lr: 0.000881\n",
      " step: 219, acc: 0.523, loss: 1.235,data_loss: 1.235, reg_loss: 0.000 lr: 0.000881\n",
      " step: 220, acc: 0.484, loss: 1.209,data_loss: 1.209, reg_loss: 0.000 lr: 0.000881\n",
      " step: 221, acc: 0.523, loss: 1.259,data_loss: 1.259, reg_loss: 0.000 lr: 0.000881\n",
      " step: 222, acc: 0.477, loss: 1.203,data_loss: 1.203, reg_loss: 0.000 lr: 0.000881\n",
      " step: 223, acc: 0.492, loss: 1.242,data_loss: 1.242, reg_loss: 0.000 lr: 0.000881\n",
      " step: 224, acc: 0.568, loss: 1.110,data_loss: 1.110, reg_loss: 0.000 lr: 0.000881\n",
      "training , acc: 0.519, loss: 1.243,data_loss: 1.243, reg_loss: 0.000 lr: 0.000881\n",
      "validation, acc:0.440 ,loss: 1.498 \n",
      "epoch: 13\n",
      " step: 0, acc: 0.430, loss: 1.338,data_loss: 1.338, reg_loss: 0.000 lr: 0.000881\n",
      " step: 1, acc: 0.508, loss: 1.230,data_loss: 1.230, reg_loss: 0.000 lr: 0.000881\n",
      " step: 2, acc: 0.500, loss: 1.220,data_loss: 1.220, reg_loss: 0.000 lr: 0.000881\n",
      " step: 3, acc: 0.492, loss: 1.236,data_loss: 1.236, reg_loss: 0.000 lr: 0.000881\n",
      " step: 4, acc: 0.578, loss: 1.114,data_loss: 1.114, reg_loss: 0.000 lr: 0.000881\n",
      " step: 5, acc: 0.523, loss: 1.303,data_loss: 1.303, reg_loss: 0.000 lr: 0.000881\n",
      " step: 6, acc: 0.523, loss: 1.229,data_loss: 1.229, reg_loss: 0.000 lr: 0.000881\n",
      " step: 7, acc: 0.555, loss: 1.235,data_loss: 1.235, reg_loss: 0.000 lr: 0.000881\n",
      " step: 8, acc: 0.547, loss: 1.106,data_loss: 1.106, reg_loss: 0.000 lr: 0.000881\n",
      " step: 9, acc: 0.617, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000881\n",
      " step: 10, acc: 0.578, loss: 1.071,data_loss: 1.071, reg_loss: 0.000 lr: 0.000881\n",
      " step: 11, acc: 0.523, loss: 1.177,data_loss: 1.177, reg_loss: 0.000 lr: 0.000881\n",
      " step: 12, acc: 0.531, loss: 1.224,data_loss: 1.224, reg_loss: 0.000 lr: 0.000881\n",
      " step: 13, acc: 0.492, loss: 1.202,data_loss: 1.202, reg_loss: 0.000 lr: 0.000881\n",
      " step: 14, acc: 0.500, loss: 1.282,data_loss: 1.282, reg_loss: 0.000 lr: 0.000881\n",
      " step: 15, acc: 0.547, loss: 1.170,data_loss: 1.170, reg_loss: 0.000 lr: 0.000880\n",
      " step: 16, acc: 0.570, loss: 1.199,data_loss: 1.199, reg_loss: 0.000 lr: 0.000880\n",
      " step: 17, acc: 0.523, loss: 1.197,data_loss: 1.197, reg_loss: 0.000 lr: 0.000880\n",
      " step: 18, acc: 0.461, loss: 1.322,data_loss: 1.322, reg_loss: 0.000 lr: 0.000880\n",
      " step: 19, acc: 0.648, loss: 1.071,data_loss: 1.071, reg_loss: 0.000 lr: 0.000880\n",
      " step: 20, acc: 0.586, loss: 1.205,data_loss: 1.205, reg_loss: 0.000 lr: 0.000880\n",
      " step: 21, acc: 0.547, loss: 1.215,data_loss: 1.215, reg_loss: 0.000 lr: 0.000880\n",
      " step: 22, acc: 0.547, loss: 1.198,data_loss: 1.198, reg_loss: 0.000 lr: 0.000880\n",
      " step: 23, acc: 0.570, loss: 1.189,data_loss: 1.189, reg_loss: 0.000 lr: 0.000880\n",
      " step: 24, acc: 0.500, loss: 1.254,data_loss: 1.254, reg_loss: 0.000 lr: 0.000880\n",
      " step: 25, acc: 0.562, loss: 1.149,data_loss: 1.149, reg_loss: 0.000 lr: 0.000880\n",
      " step: 26, acc: 0.516, loss: 1.253,data_loss: 1.253, reg_loss: 0.000 lr: 0.000880\n",
      " step: 27, acc: 0.539, loss: 1.143,data_loss: 1.143, reg_loss: 0.000 lr: 0.000880\n",
      " step: 28, acc: 0.578, loss: 1.089,data_loss: 1.089, reg_loss: 0.000 lr: 0.000880\n",
      " step: 29, acc: 0.531, loss: 1.233,data_loss: 1.233, reg_loss: 0.000 lr: 0.000880\n",
      " step: 30, acc: 0.477, loss: 1.384,data_loss: 1.384, reg_loss: 0.000 lr: 0.000880\n",
      " step: 31, acc: 0.570, loss: 1.088,data_loss: 1.088, reg_loss: 0.000 lr: 0.000880\n",
      " step: 32, acc: 0.547, loss: 1.240,data_loss: 1.240, reg_loss: 0.000 lr: 0.000880\n",
      " step: 33, acc: 0.531, loss: 1.284,data_loss: 1.284, reg_loss: 0.000 lr: 0.000880\n",
      " step: 34, acc: 0.523, loss: 1.174,data_loss: 1.174, reg_loss: 0.000 lr: 0.000880\n",
      " step: 35, acc: 0.508, loss: 1.209,data_loss: 1.209, reg_loss: 0.000 lr: 0.000880\n",
      " step: 36, acc: 0.562, loss: 1.170,data_loss: 1.170, reg_loss: 0.000 lr: 0.000880\n",
      " step: 37, acc: 0.555, loss: 1.201,data_loss: 1.201, reg_loss: 0.000 lr: 0.000880\n",
      " step: 38, acc: 0.609, loss: 1.160,data_loss: 1.160, reg_loss: 0.000 lr: 0.000880\n",
      " step: 39, acc: 0.477, loss: 1.278,data_loss: 1.278, reg_loss: 0.000 lr: 0.000880\n",
      " step: 40, acc: 0.570, loss: 1.187,data_loss: 1.187, reg_loss: 0.000 lr: 0.000880\n",
      " step: 41, acc: 0.484, loss: 1.274,data_loss: 1.274, reg_loss: 0.000 lr: 0.000879\n",
      " step: 42, acc: 0.586, loss: 1.171,data_loss: 1.171, reg_loss: 0.000 lr: 0.000879\n",
      " step: 43, acc: 0.492, loss: 1.279,data_loss: 1.279, reg_loss: 0.000 lr: 0.000879\n",
      " step: 44, acc: 0.531, loss: 1.271,data_loss: 1.271, reg_loss: 0.000 lr: 0.000879\n",
      " step: 45, acc: 0.477, loss: 1.216,data_loss: 1.216, reg_loss: 0.000 lr: 0.000879\n",
      " step: 46, acc: 0.555, loss: 1.294,data_loss: 1.294, reg_loss: 0.000 lr: 0.000879\n",
      " step: 47, acc: 0.531, loss: 1.138,data_loss: 1.138, reg_loss: 0.000 lr: 0.000879\n",
      " step: 48, acc: 0.547, loss: 1.081,data_loss: 1.081, reg_loss: 0.000 lr: 0.000879\n",
      " step: 49, acc: 0.562, loss: 1.161,data_loss: 1.161, reg_loss: 0.000 lr: 0.000879\n",
      " step: 50, acc: 0.555, loss: 1.099,data_loss: 1.099, reg_loss: 0.000 lr: 0.000879\n",
      " step: 51, acc: 0.492, loss: 1.258,data_loss: 1.258, reg_loss: 0.000 lr: 0.000879\n",
      " step: 52, acc: 0.523, loss: 1.251,data_loss: 1.251, reg_loss: 0.000 lr: 0.000879\n",
      " step: 53, acc: 0.555, loss: 1.221,data_loss: 1.221, reg_loss: 0.000 lr: 0.000879\n",
      " step: 54, acc: 0.438, loss: 1.479,data_loss: 1.479, reg_loss: 0.000 lr: 0.000879\n",
      " step: 55, acc: 0.547, loss: 1.218,data_loss: 1.218, reg_loss: 0.000 lr: 0.000879\n",
      " step: 56, acc: 0.562, loss: 1.108,data_loss: 1.108, reg_loss: 0.000 lr: 0.000879\n",
      " step: 57, acc: 0.578, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000879\n",
      " step: 58, acc: 0.500, loss: 1.311,data_loss: 1.311, reg_loss: 0.000 lr: 0.000879\n",
      " step: 59, acc: 0.570, loss: 1.200,data_loss: 1.200, reg_loss: 0.000 lr: 0.000879\n",
      " step: 60, acc: 0.539, loss: 1.120,data_loss: 1.120, reg_loss: 0.000 lr: 0.000879\n",
      " step: 61, acc: 0.531, loss: 1.253,data_loss: 1.253, reg_loss: 0.000 lr: 0.000879\n",
      " step: 62, acc: 0.555, loss: 1.201,data_loss: 1.201, reg_loss: 0.000 lr: 0.000879\n",
      " step: 63, acc: 0.500, loss: 1.213,data_loss: 1.213, reg_loss: 0.000 lr: 0.000879\n",
      " step: 64, acc: 0.477, loss: 1.297,data_loss: 1.297, reg_loss: 0.000 lr: 0.000879\n",
      " step: 65, acc: 0.508, loss: 1.258,data_loss: 1.258, reg_loss: 0.000 lr: 0.000879\n",
      " step: 66, acc: 0.570, loss: 1.090,data_loss: 1.090, reg_loss: 0.000 lr: 0.000879\n",
      " step: 67, acc: 0.508, loss: 1.189,data_loss: 1.189, reg_loss: 0.000 lr: 0.000878\n",
      " step: 68, acc: 0.461, loss: 1.298,data_loss: 1.298, reg_loss: 0.000 lr: 0.000878\n",
      " step: 69, acc: 0.414, loss: 1.345,data_loss: 1.345, reg_loss: 0.000 lr: 0.000878\n",
      " step: 70, acc: 0.555, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000878\n",
      " step: 71, acc: 0.586, loss: 1.119,data_loss: 1.119, reg_loss: 0.000 lr: 0.000878\n",
      " step: 72, acc: 0.500, loss: 1.214,data_loss: 1.214, reg_loss: 0.000 lr: 0.000878\n",
      " step: 73, acc: 0.523, loss: 1.214,data_loss: 1.214, reg_loss: 0.000 lr: 0.000878\n",
      " step: 74, acc: 0.531, loss: 1.220,data_loss: 1.220, reg_loss: 0.000 lr: 0.000878\n",
      " step: 75, acc: 0.570, loss: 1.291,data_loss: 1.291, reg_loss: 0.000 lr: 0.000878\n",
      " step: 76, acc: 0.562, loss: 1.179,data_loss: 1.179, reg_loss: 0.000 lr: 0.000878\n",
      " step: 77, acc: 0.508, loss: 1.243,data_loss: 1.243, reg_loss: 0.000 lr: 0.000878\n",
      " step: 78, acc: 0.547, loss: 1.139,data_loss: 1.139, reg_loss: 0.000 lr: 0.000878\n",
      " step: 79, acc: 0.547, loss: 1.255,data_loss: 1.255, reg_loss: 0.000 lr: 0.000878\n",
      " step: 80, acc: 0.508, loss: 1.371,data_loss: 1.371, reg_loss: 0.000 lr: 0.000878\n",
      " step: 81, acc: 0.500, loss: 1.330,data_loss: 1.330, reg_loss: 0.000 lr: 0.000878\n",
      " step: 82, acc: 0.516, loss: 1.224,data_loss: 1.224, reg_loss: 0.000 lr: 0.000878\n",
      " step: 83, acc: 0.500, loss: 1.181,data_loss: 1.181, reg_loss: 0.000 lr: 0.000878\n",
      " step: 84, acc: 0.516, loss: 1.220,data_loss: 1.220, reg_loss: 0.000 lr: 0.000878\n",
      " step: 85, acc: 0.516, loss: 1.233,data_loss: 1.233, reg_loss: 0.000 lr: 0.000878\n",
      " step: 86, acc: 0.547, loss: 1.214,data_loss: 1.214, reg_loss: 0.000 lr: 0.000878\n",
      " step: 87, acc: 0.500, loss: 1.228,data_loss: 1.228, reg_loss: 0.000 lr: 0.000878\n",
      " step: 88, acc: 0.570, loss: 1.279,data_loss: 1.279, reg_loss: 0.000 lr: 0.000878\n",
      " step: 89, acc: 0.461, loss: 1.296,data_loss: 1.296, reg_loss: 0.000 lr: 0.000878\n",
      " step: 90, acc: 0.602, loss: 1.189,data_loss: 1.189, reg_loss: 0.000 lr: 0.000878\n",
      " step: 91, acc: 0.477, loss: 1.349,data_loss: 1.349, reg_loss: 0.000 lr: 0.000878\n",
      " step: 92, acc: 0.500, loss: 1.208,data_loss: 1.208, reg_loss: 0.000 lr: 0.000878\n",
      " step: 93, acc: 0.594, loss: 1.108,data_loss: 1.108, reg_loss: 0.000 lr: 0.000877\n",
      " step: 94, acc: 0.414, loss: 1.450,data_loss: 1.450, reg_loss: 0.000 lr: 0.000877\n",
      " step: 95, acc: 0.516, loss: 1.151,data_loss: 1.151, reg_loss: 0.000 lr: 0.000877\n",
      " step: 96, acc: 0.516, loss: 1.255,data_loss: 1.255, reg_loss: 0.000 lr: 0.000877\n",
      " step: 97, acc: 0.492, loss: 1.262,data_loss: 1.262, reg_loss: 0.000 lr: 0.000877\n",
      " step: 98, acc: 0.531, loss: 1.135,data_loss: 1.135, reg_loss: 0.000 lr: 0.000877\n",
      " step: 99, acc: 0.578, loss: 1.030,data_loss: 1.030, reg_loss: 0.000 lr: 0.000877\n",
      " step: 100, acc: 0.539, loss: 1.139,data_loss: 1.139, reg_loss: 0.000 lr: 0.000877\n",
      " step: 101, acc: 0.492, loss: 1.199,data_loss: 1.199, reg_loss: 0.000 lr: 0.000877\n",
      " step: 102, acc: 0.562, loss: 1.182,data_loss: 1.182, reg_loss: 0.000 lr: 0.000877\n",
      " step: 103, acc: 0.539, loss: 1.151,data_loss: 1.151, reg_loss: 0.000 lr: 0.000877\n",
      " step: 104, acc: 0.602, loss: 1.061,data_loss: 1.061, reg_loss: 0.000 lr: 0.000877\n",
      " step: 105, acc: 0.516, loss: 1.211,data_loss: 1.211, reg_loss: 0.000 lr: 0.000877\n",
      " step: 106, acc: 0.547, loss: 1.169,data_loss: 1.169, reg_loss: 0.000 lr: 0.000877\n",
      " step: 107, acc: 0.484, loss: 1.218,data_loss: 1.218, reg_loss: 0.000 lr: 0.000877\n",
      " step: 108, acc: 0.500, loss: 1.230,data_loss: 1.230, reg_loss: 0.000 lr: 0.000877\n",
      " step: 109, acc: 0.492, loss: 1.221,data_loss: 1.221, reg_loss: 0.000 lr: 0.000877\n",
      " step: 110, acc: 0.523, loss: 1.277,data_loss: 1.277, reg_loss: 0.000 lr: 0.000877\n",
      " step: 111, acc: 0.539, loss: 1.244,data_loss: 1.244, reg_loss: 0.000 lr: 0.000877\n",
      " step: 112, acc: 0.477, loss: 1.329,data_loss: 1.329, reg_loss: 0.000 lr: 0.000877\n",
      " step: 113, acc: 0.523, loss: 1.193,data_loss: 1.193, reg_loss: 0.000 lr: 0.000877\n",
      " step: 114, acc: 0.523, loss: 1.277,data_loss: 1.277, reg_loss: 0.000 lr: 0.000877\n",
      " step: 115, acc: 0.492, loss: 1.236,data_loss: 1.236, reg_loss: 0.000 lr: 0.000877\n",
      " step: 116, acc: 0.492, loss: 1.377,data_loss: 1.377, reg_loss: 0.000 lr: 0.000877\n",
      " step: 117, acc: 0.547, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000877\n",
      " step: 118, acc: 0.555, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000877\n",
      " step: 119, acc: 0.562, loss: 1.149,data_loss: 1.149, reg_loss: 0.000 lr: 0.000876\n",
      " step: 120, acc: 0.555, loss: 1.193,data_loss: 1.193, reg_loss: 0.000 lr: 0.000876\n",
      " step: 121, acc: 0.539, loss: 1.232,data_loss: 1.232, reg_loss: 0.000 lr: 0.000876\n",
      " step: 122, acc: 0.500, loss: 1.301,data_loss: 1.301, reg_loss: 0.000 lr: 0.000876\n",
      " step: 123, acc: 0.523, loss: 1.188,data_loss: 1.188, reg_loss: 0.000 lr: 0.000876\n",
      " step: 124, acc: 0.477, loss: 1.280,data_loss: 1.280, reg_loss: 0.000 lr: 0.000876\n",
      " step: 125, acc: 0.555, loss: 1.187,data_loss: 1.187, reg_loss: 0.000 lr: 0.000876\n",
      " step: 126, acc: 0.539, loss: 1.174,data_loss: 1.174, reg_loss: 0.000 lr: 0.000876\n",
      " step: 127, acc: 0.453, loss: 1.260,data_loss: 1.260, reg_loss: 0.000 lr: 0.000876\n",
      " step: 128, acc: 0.539, loss: 1.188,data_loss: 1.188, reg_loss: 0.000 lr: 0.000876\n",
      " step: 129, acc: 0.547, loss: 1.226,data_loss: 1.226, reg_loss: 0.000 lr: 0.000876\n",
      " step: 130, acc: 0.500, loss: 1.316,data_loss: 1.316, reg_loss: 0.000 lr: 0.000876\n",
      " step: 131, acc: 0.578, loss: 1.137,data_loss: 1.137, reg_loss: 0.000 lr: 0.000876\n",
      " step: 132, acc: 0.484, loss: 1.188,data_loss: 1.188, reg_loss: 0.000 lr: 0.000876\n",
      " step: 133, acc: 0.516, loss: 1.263,data_loss: 1.263, reg_loss: 0.000 lr: 0.000876\n",
      " step: 134, acc: 0.594, loss: 1.018,data_loss: 1.018, reg_loss: 0.000 lr: 0.000876\n",
      " step: 135, acc: 0.492, loss: 1.179,data_loss: 1.179, reg_loss: 0.000 lr: 0.000876\n",
      " step: 136, acc: 0.508, loss: 1.370,data_loss: 1.370, reg_loss: 0.000 lr: 0.000876\n",
      " step: 137, acc: 0.586, loss: 1.192,data_loss: 1.192, reg_loss: 0.000 lr: 0.000876\n",
      " step: 138, acc: 0.508, loss: 1.269,data_loss: 1.269, reg_loss: 0.000 lr: 0.000876\n",
      " step: 139, acc: 0.586, loss: 1.136,data_loss: 1.136, reg_loss: 0.000 lr: 0.000876\n",
      " step: 140, acc: 0.602, loss: 1.097,data_loss: 1.097, reg_loss: 0.000 lr: 0.000876\n",
      " step: 141, acc: 0.484, loss: 1.243,data_loss: 1.243, reg_loss: 0.000 lr: 0.000876\n",
      " step: 142, acc: 0.586, loss: 1.159,data_loss: 1.159, reg_loss: 0.000 lr: 0.000876\n",
      " step: 143, acc: 0.477, loss: 1.274,data_loss: 1.274, reg_loss: 0.000 lr: 0.000876\n",
      " step: 144, acc: 0.508, loss: 1.291,data_loss: 1.291, reg_loss: 0.000 lr: 0.000876\n",
      " step: 145, acc: 0.539, loss: 1.164,data_loss: 1.164, reg_loss: 0.000 lr: 0.000875\n",
      " step: 146, acc: 0.445, loss: 1.363,data_loss: 1.363, reg_loss: 0.000 lr: 0.000875\n",
      " step: 147, acc: 0.570, loss: 1.210,data_loss: 1.210, reg_loss: 0.000 lr: 0.000875\n",
      " step: 148, acc: 0.547, loss: 1.169,data_loss: 1.169, reg_loss: 0.000 lr: 0.000875\n",
      " step: 149, acc: 0.547, loss: 1.118,data_loss: 1.118, reg_loss: 0.000 lr: 0.000875\n",
      " step: 150, acc: 0.625, loss: 1.086,data_loss: 1.086, reg_loss: 0.000 lr: 0.000875\n",
      " step: 151, acc: 0.484, loss: 1.284,data_loss: 1.284, reg_loss: 0.000 lr: 0.000875\n",
      " step: 152, acc: 0.578, loss: 1.052,data_loss: 1.052, reg_loss: 0.000 lr: 0.000875\n",
      " step: 153, acc: 0.570, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000875\n",
      " step: 154, acc: 0.516, loss: 1.278,data_loss: 1.278, reg_loss: 0.000 lr: 0.000875\n",
      " step: 155, acc: 0.539, loss: 1.171,data_loss: 1.171, reg_loss: 0.000 lr: 0.000875\n",
      " step: 156, acc: 0.500, loss: 1.239,data_loss: 1.239, reg_loss: 0.000 lr: 0.000875\n",
      " step: 157, acc: 0.555, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000875\n",
      " step: 158, acc: 0.383, loss: 1.337,data_loss: 1.337, reg_loss: 0.000 lr: 0.000875\n",
      " step: 159, acc: 0.508, loss: 1.263,data_loss: 1.263, reg_loss: 0.000 lr: 0.000875\n",
      " step: 160, acc: 0.492, loss: 1.446,data_loss: 1.446, reg_loss: 0.000 lr: 0.000875\n",
      " step: 161, acc: 0.516, loss: 1.326,data_loss: 1.326, reg_loss: 0.000 lr: 0.000875\n",
      " step: 162, acc: 0.555, loss: 1.276,data_loss: 1.276, reg_loss: 0.000 lr: 0.000875\n",
      " step: 163, acc: 0.492, loss: 1.249,data_loss: 1.249, reg_loss: 0.000 lr: 0.000875\n",
      " step: 164, acc: 0.539, loss: 1.209,data_loss: 1.209, reg_loss: 0.000 lr: 0.000875\n",
      " step: 165, acc: 0.508, loss: 1.405,data_loss: 1.405, reg_loss: 0.000 lr: 0.000875\n",
      " step: 166, acc: 0.500, loss: 1.273,data_loss: 1.273, reg_loss: 0.000 lr: 0.000875\n",
      " step: 167, acc: 0.461, loss: 1.312,data_loss: 1.312, reg_loss: 0.000 lr: 0.000875\n",
      " step: 168, acc: 0.547, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000875\n",
      " step: 169, acc: 0.516, loss: 1.228,data_loss: 1.228, reg_loss: 0.000 lr: 0.000875\n",
      " step: 170, acc: 0.477, loss: 1.250,data_loss: 1.250, reg_loss: 0.000 lr: 0.000875\n",
      " step: 171, acc: 0.555, loss: 1.135,data_loss: 1.135, reg_loss: 0.000 lr: 0.000874\n",
      " step: 172, acc: 0.477, loss: 1.326,data_loss: 1.326, reg_loss: 0.000 lr: 0.000874\n",
      " step: 173, acc: 0.508, loss: 1.273,data_loss: 1.273, reg_loss: 0.000 lr: 0.000874\n",
      " step: 174, acc: 0.531, loss: 1.230,data_loss: 1.230, reg_loss: 0.000 lr: 0.000874\n",
      " step: 175, acc: 0.547, loss: 1.258,data_loss: 1.258, reg_loss: 0.000 lr: 0.000874\n",
      " step: 176, acc: 0.430, loss: 1.370,data_loss: 1.370, reg_loss: 0.000 lr: 0.000874\n",
      " step: 177, acc: 0.555, loss: 1.194,data_loss: 1.194, reg_loss: 0.000 lr: 0.000874\n",
      " step: 178, acc: 0.508, loss: 1.314,data_loss: 1.314, reg_loss: 0.000 lr: 0.000874\n",
      " step: 179, acc: 0.617, loss: 1.023,data_loss: 1.023, reg_loss: 0.000 lr: 0.000874\n",
      " step: 180, acc: 0.602, loss: 1.067,data_loss: 1.067, reg_loss: 0.000 lr: 0.000874\n",
      " step: 181, acc: 0.602, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000874\n",
      " step: 182, acc: 0.547, loss: 1.204,data_loss: 1.204, reg_loss: 0.000 lr: 0.000874\n",
      " step: 183, acc: 0.586, loss: 1.105,data_loss: 1.105, reg_loss: 0.000 lr: 0.000874\n",
      " step: 184, acc: 0.508, loss: 1.279,data_loss: 1.279, reg_loss: 0.000 lr: 0.000874\n",
      " step: 185, acc: 0.500, loss: 1.282,data_loss: 1.282, reg_loss: 0.000 lr: 0.000874\n",
      " step: 186, acc: 0.500, loss: 1.218,data_loss: 1.218, reg_loss: 0.000 lr: 0.000874\n",
      " step: 187, acc: 0.531, loss: 1.119,data_loss: 1.119, reg_loss: 0.000 lr: 0.000874\n",
      " step: 188, acc: 0.508, loss: 1.245,data_loss: 1.245, reg_loss: 0.000 lr: 0.000874\n",
      " step: 189, acc: 0.477, loss: 1.267,data_loss: 1.267, reg_loss: 0.000 lr: 0.000874\n",
      " step: 190, acc: 0.484, loss: 1.283,data_loss: 1.283, reg_loss: 0.000 lr: 0.000874\n",
      " step: 191, acc: 0.531, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000874\n",
      " step: 192, acc: 0.609, loss: 1.150,data_loss: 1.150, reg_loss: 0.000 lr: 0.000874\n",
      " step: 193, acc: 0.547, loss: 1.107,data_loss: 1.107, reg_loss: 0.000 lr: 0.000874\n",
      " step: 194, acc: 0.586, loss: 1.127,data_loss: 1.127, reg_loss: 0.000 lr: 0.000874\n",
      " step: 195, acc: 0.547, loss: 1.140,data_loss: 1.140, reg_loss: 0.000 lr: 0.000874\n",
      " step: 196, acc: 0.539, loss: 1.196,data_loss: 1.196, reg_loss: 0.000 lr: 0.000874\n",
      " step: 197, acc: 0.539, loss: 1.158,data_loss: 1.158, reg_loss: 0.000 lr: 0.000873\n",
      " step: 198, acc: 0.531, loss: 1.169,data_loss: 1.169, reg_loss: 0.000 lr: 0.000873\n",
      " step: 199, acc: 0.516, loss: 1.343,data_loss: 1.343, reg_loss: 0.000 lr: 0.000873\n",
      " step: 200, acc: 0.523, loss: 1.220,data_loss: 1.220, reg_loss: 0.000 lr: 0.000873\n",
      " step: 201, acc: 0.523, loss: 1.207,data_loss: 1.207, reg_loss: 0.000 lr: 0.000873\n",
      " step: 202, acc: 0.492, loss: 1.219,data_loss: 1.219, reg_loss: 0.000 lr: 0.000873\n",
      " step: 203, acc: 0.523, loss: 1.231,data_loss: 1.231, reg_loss: 0.000 lr: 0.000873\n",
      " step: 204, acc: 0.539, loss: 1.138,data_loss: 1.138, reg_loss: 0.000 lr: 0.000873\n",
      " step: 205, acc: 0.500, loss: 1.317,data_loss: 1.317, reg_loss: 0.000 lr: 0.000873\n",
      " step: 206, acc: 0.508, loss: 1.206,data_loss: 1.206, reg_loss: 0.000 lr: 0.000873\n",
      " step: 207, acc: 0.547, loss: 1.175,data_loss: 1.175, reg_loss: 0.000 lr: 0.000873\n",
      " step: 208, acc: 0.602, loss: 1.168,data_loss: 1.168, reg_loss: 0.000 lr: 0.000873\n",
      " step: 209, acc: 0.516, loss: 1.151,data_loss: 1.151, reg_loss: 0.000 lr: 0.000873\n",
      " step: 210, acc: 0.500, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000873\n",
      " step: 211, acc: 0.578, loss: 1.107,data_loss: 1.107, reg_loss: 0.000 lr: 0.000873\n",
      " step: 212, acc: 0.586, loss: 1.107,data_loss: 1.107, reg_loss: 0.000 lr: 0.000873\n",
      " step: 213, acc: 0.516, loss: 1.203,data_loss: 1.203, reg_loss: 0.000 lr: 0.000873\n",
      " step: 214, acc: 0.453, loss: 1.316,data_loss: 1.316, reg_loss: 0.000 lr: 0.000873\n",
      " step: 215, acc: 0.562, loss: 1.138,data_loss: 1.138, reg_loss: 0.000 lr: 0.000873\n",
      " step: 216, acc: 0.547, loss: 1.082,data_loss: 1.082, reg_loss: 0.000 lr: 0.000873\n",
      " step: 217, acc: 0.500, loss: 1.247,data_loss: 1.247, reg_loss: 0.000 lr: 0.000873\n",
      " step: 218, acc: 0.586, loss: 1.166,data_loss: 1.166, reg_loss: 0.000 lr: 0.000873\n",
      " step: 219, acc: 0.516, loss: 1.241,data_loss: 1.241, reg_loss: 0.000 lr: 0.000873\n",
      " step: 220, acc: 0.617, loss: 1.135,data_loss: 1.135, reg_loss: 0.000 lr: 0.000873\n",
      " step: 221, acc: 0.484, loss: 1.266,data_loss: 1.266, reg_loss: 0.000 lr: 0.000873\n",
      " step: 222, acc: 0.547, loss: 1.207,data_loss: 1.207, reg_loss: 0.000 lr: 0.000873\n",
      " step: 223, acc: 0.484, loss: 1.220,data_loss: 1.220, reg_loss: 0.000 lr: 0.000872\n",
      " step: 224, acc: 0.649, loss: 0.922,data_loss: 0.922, reg_loss: 0.000 lr: 0.000872\n",
      "training , acc: 0.529, loss: 1.213,data_loss: 1.213, reg_loss: 0.000 lr: 0.000872\n",
      "validation, acc:0.446 ,loss: 1.497 \n",
      "epoch: 14\n",
      " step: 0, acc: 0.445, loss: 1.303,data_loss: 1.303, reg_loss: 0.000 lr: 0.000872\n",
      " step: 1, acc: 0.508, loss: 1.226,data_loss: 1.226, reg_loss: 0.000 lr: 0.000872\n",
      " step: 2, acc: 0.492, loss: 1.157,data_loss: 1.157, reg_loss: 0.000 lr: 0.000872\n",
      " step: 3, acc: 0.570, loss: 1.174,data_loss: 1.174, reg_loss: 0.000 lr: 0.000872\n",
      " step: 4, acc: 0.633, loss: 1.041,data_loss: 1.041, reg_loss: 0.000 lr: 0.000872\n",
      " step: 5, acc: 0.531, loss: 1.185,data_loss: 1.185, reg_loss: 0.000 lr: 0.000872\n",
      " step: 6, acc: 0.539, loss: 1.187,data_loss: 1.187, reg_loss: 0.000 lr: 0.000872\n",
      " step: 7, acc: 0.594, loss: 1.165,data_loss: 1.165, reg_loss: 0.000 lr: 0.000872\n",
      " step: 8, acc: 0.570, loss: 1.125,data_loss: 1.125, reg_loss: 0.000 lr: 0.000872\n",
      " step: 9, acc: 0.625, loss: 1.112,data_loss: 1.112, reg_loss: 0.000 lr: 0.000872\n",
      " step: 10, acc: 0.617, loss: 1.087,data_loss: 1.087, reg_loss: 0.000 lr: 0.000872\n",
      " step: 11, acc: 0.570, loss: 1.157,data_loss: 1.157, reg_loss: 0.000 lr: 0.000872\n",
      " step: 12, acc: 0.555, loss: 1.171,data_loss: 1.171, reg_loss: 0.000 lr: 0.000872\n",
      " step: 13, acc: 0.500, loss: 1.166,data_loss: 1.166, reg_loss: 0.000 lr: 0.000872\n",
      " step: 14, acc: 0.469, loss: 1.236,data_loss: 1.236, reg_loss: 0.000 lr: 0.000872\n",
      " step: 15, acc: 0.547, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000872\n",
      " step: 16, acc: 0.484, loss: 1.198,data_loss: 1.198, reg_loss: 0.000 lr: 0.000872\n",
      " step: 17, acc: 0.547, loss: 1.199,data_loss: 1.199, reg_loss: 0.000 lr: 0.000872\n",
      " step: 18, acc: 0.523, loss: 1.295,data_loss: 1.295, reg_loss: 0.000 lr: 0.000872\n",
      " step: 19, acc: 0.531, loss: 1.155,data_loss: 1.155, reg_loss: 0.000 lr: 0.000872\n",
      " step: 20, acc: 0.523, loss: 1.205,data_loss: 1.205, reg_loss: 0.000 lr: 0.000872\n",
      " step: 21, acc: 0.625, loss: 1.133,data_loss: 1.133, reg_loss: 0.000 lr: 0.000872\n",
      " step: 22, acc: 0.492, loss: 1.179,data_loss: 1.179, reg_loss: 0.000 lr: 0.000872\n",
      " step: 23, acc: 0.500, loss: 1.333,data_loss: 1.333, reg_loss: 0.000 lr: 0.000872\n",
      " step: 24, acc: 0.570, loss: 1.116,data_loss: 1.116, reg_loss: 0.000 lr: 0.000871\n",
      " step: 25, acc: 0.594, loss: 1.125,data_loss: 1.125, reg_loss: 0.000 lr: 0.000871\n",
      " step: 26, acc: 0.523, loss: 1.190,data_loss: 1.190, reg_loss: 0.000 lr: 0.000871\n",
      " step: 27, acc: 0.594, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000871\n",
      " step: 28, acc: 0.570, loss: 1.126,data_loss: 1.126, reg_loss: 0.000 lr: 0.000871\n",
      " step: 29, acc: 0.500, loss: 1.209,data_loss: 1.209, reg_loss: 0.000 lr: 0.000871\n",
      " step: 30, acc: 0.500, loss: 1.322,data_loss: 1.322, reg_loss: 0.000 lr: 0.000871\n",
      " step: 31, acc: 0.633, loss: 1.026,data_loss: 1.026, reg_loss: 0.000 lr: 0.000871\n",
      " step: 32, acc: 0.602, loss: 1.153,data_loss: 1.153, reg_loss: 0.000 lr: 0.000871\n",
      " step: 33, acc: 0.570, loss: 1.141,data_loss: 1.141, reg_loss: 0.000 lr: 0.000871\n",
      " step: 34, acc: 0.516, loss: 1.275,data_loss: 1.275, reg_loss: 0.000 lr: 0.000871\n",
      " step: 35, acc: 0.547, loss: 1.207,data_loss: 1.207, reg_loss: 0.000 lr: 0.000871\n",
      " step: 36, acc: 0.555, loss: 1.037,data_loss: 1.037, reg_loss: 0.000 lr: 0.000871\n",
      " step: 37, acc: 0.516, loss: 1.222,data_loss: 1.222, reg_loss: 0.000 lr: 0.000871\n",
      " step: 38, acc: 0.484, loss: 1.175,data_loss: 1.175, reg_loss: 0.000 lr: 0.000871\n",
      " step: 39, acc: 0.508, loss: 1.249,data_loss: 1.249, reg_loss: 0.000 lr: 0.000871\n",
      " step: 40, acc: 0.570, loss: 1.235,data_loss: 1.235, reg_loss: 0.000 lr: 0.000871\n",
      " step: 41, acc: 0.469, loss: 1.326,data_loss: 1.326, reg_loss: 0.000 lr: 0.000871\n",
      " step: 42, acc: 0.602, loss: 1.174,data_loss: 1.174, reg_loss: 0.000 lr: 0.000871\n",
      " step: 43, acc: 0.492, loss: 1.253,data_loss: 1.253, reg_loss: 0.000 lr: 0.000871\n",
      " step: 44, acc: 0.523, loss: 1.157,data_loss: 1.157, reg_loss: 0.000 lr: 0.000871\n",
      " step: 45, acc: 0.562, loss: 1.155,data_loss: 1.155, reg_loss: 0.000 lr: 0.000871\n",
      " step: 46, acc: 0.547, loss: 1.178,data_loss: 1.178, reg_loss: 0.000 lr: 0.000871\n",
      " step: 47, acc: 0.516, loss: 1.166,data_loss: 1.166, reg_loss: 0.000 lr: 0.000871\n",
      " step: 48, acc: 0.586, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000871\n",
      " step: 49, acc: 0.555, loss: 1.202,data_loss: 1.202, reg_loss: 0.000 lr: 0.000871\n",
      " step: 50, acc: 0.602, loss: 1.018,data_loss: 1.018, reg_loss: 0.000 lr: 0.000871\n",
      " step: 51, acc: 0.516, loss: 1.277,data_loss: 1.277, reg_loss: 0.000 lr: 0.000870\n",
      " step: 52, acc: 0.523, loss: 1.240,data_loss: 1.240, reg_loss: 0.000 lr: 0.000870\n",
      " step: 53, acc: 0.594, loss: 1.179,data_loss: 1.179, reg_loss: 0.000 lr: 0.000870\n",
      " step: 54, acc: 0.484, loss: 1.255,data_loss: 1.255, reg_loss: 0.000 lr: 0.000870\n",
      " step: 55, acc: 0.492, loss: 1.208,data_loss: 1.208, reg_loss: 0.000 lr: 0.000870\n",
      " step: 56, acc: 0.570, loss: 1.116,data_loss: 1.116, reg_loss: 0.000 lr: 0.000870\n",
      " step: 57, acc: 0.586, loss: 1.065,data_loss: 1.065, reg_loss: 0.000 lr: 0.000870\n",
      " step: 58, acc: 0.555, loss: 1.260,data_loss: 1.260, reg_loss: 0.000 lr: 0.000870\n",
      " step: 59, acc: 0.594, loss: 1.084,data_loss: 1.084, reg_loss: 0.000 lr: 0.000870\n",
      " step: 60, acc: 0.586, loss: 1.077,data_loss: 1.077, reg_loss: 0.000 lr: 0.000870\n",
      " step: 61, acc: 0.484, loss: 1.248,data_loss: 1.248, reg_loss: 0.000 lr: 0.000870\n",
      " step: 62, acc: 0.516, loss: 1.237,data_loss: 1.237, reg_loss: 0.000 lr: 0.000870\n",
      " step: 63, acc: 0.562, loss: 1.072,data_loss: 1.072, reg_loss: 0.000 lr: 0.000870\n",
      " step: 64, acc: 0.508, loss: 1.286,data_loss: 1.286, reg_loss: 0.000 lr: 0.000870\n",
      " step: 65, acc: 0.500, loss: 1.341,data_loss: 1.341, reg_loss: 0.000 lr: 0.000870\n",
      " step: 66, acc: 0.609, loss: 1.048,data_loss: 1.048, reg_loss: 0.000 lr: 0.000870\n",
      " step: 67, acc: 0.492, loss: 1.257,data_loss: 1.257, reg_loss: 0.000 lr: 0.000870\n",
      " step: 68, acc: 0.484, loss: 1.269,data_loss: 1.269, reg_loss: 0.000 lr: 0.000870\n",
      " step: 69, acc: 0.516, loss: 1.322,data_loss: 1.322, reg_loss: 0.000 lr: 0.000870\n",
      " step: 70, acc: 0.570, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000870\n",
      " step: 71, acc: 0.562, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000870\n",
      " step: 72, acc: 0.578, loss: 1.113,data_loss: 1.113, reg_loss: 0.000 lr: 0.000870\n",
      " step: 73, acc: 0.508, loss: 1.164,data_loss: 1.164, reg_loss: 0.000 lr: 0.000870\n",
      " step: 74, acc: 0.500, loss: 1.303,data_loss: 1.303, reg_loss: 0.000 lr: 0.000870\n",
      " step: 75, acc: 0.562, loss: 1.202,data_loss: 1.202, reg_loss: 0.000 lr: 0.000870\n",
      " step: 76, acc: 0.531, loss: 1.135,data_loss: 1.135, reg_loss: 0.000 lr: 0.000870\n",
      " step: 77, acc: 0.555, loss: 1.143,data_loss: 1.143, reg_loss: 0.000 lr: 0.000869\n",
      " step: 78, acc: 0.578, loss: 1.076,data_loss: 1.076, reg_loss: 0.000 lr: 0.000869\n",
      " step: 79, acc: 0.508, loss: 1.233,data_loss: 1.233, reg_loss: 0.000 lr: 0.000869\n",
      " step: 80, acc: 0.539, loss: 1.286,data_loss: 1.286, reg_loss: 0.000 lr: 0.000869\n",
      " step: 81, acc: 0.484, loss: 1.382,data_loss: 1.382, reg_loss: 0.000 lr: 0.000869\n",
      " step: 82, acc: 0.625, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000869\n",
      " step: 83, acc: 0.539, loss: 1.160,data_loss: 1.160, reg_loss: 0.000 lr: 0.000869\n",
      " step: 84, acc: 0.539, loss: 1.185,data_loss: 1.185, reg_loss: 0.000 lr: 0.000869\n",
      " step: 85, acc: 0.617, loss: 1.131,data_loss: 1.131, reg_loss: 0.000 lr: 0.000869\n",
      " step: 86, acc: 0.555, loss: 1.127,data_loss: 1.127, reg_loss: 0.000 lr: 0.000869\n",
      " step: 87, acc: 0.570, loss: 1.212,data_loss: 1.212, reg_loss: 0.000 lr: 0.000869\n",
      " step: 88, acc: 0.555, loss: 1.252,data_loss: 1.252, reg_loss: 0.000 lr: 0.000869\n",
      " step: 89, acc: 0.531, loss: 1.259,data_loss: 1.259, reg_loss: 0.000 lr: 0.000869\n",
      " step: 90, acc: 0.539, loss: 1.221,data_loss: 1.221, reg_loss: 0.000 lr: 0.000869\n",
      " step: 91, acc: 0.508, loss: 1.279,data_loss: 1.279, reg_loss: 0.000 lr: 0.000869\n",
      " step: 92, acc: 0.492, loss: 1.193,data_loss: 1.193, reg_loss: 0.000 lr: 0.000869\n",
      " step: 93, acc: 0.664, loss: 1.026,data_loss: 1.026, reg_loss: 0.000 lr: 0.000869\n",
      " step: 94, acc: 0.406, loss: 1.346,data_loss: 1.346, reg_loss: 0.000 lr: 0.000869\n",
      " step: 95, acc: 0.570, loss: 1.118,data_loss: 1.118, reg_loss: 0.000 lr: 0.000869\n",
      " step: 96, acc: 0.516, loss: 1.262,data_loss: 1.262, reg_loss: 0.000 lr: 0.000869\n",
      " step: 97, acc: 0.602, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000869\n",
      " step: 98, acc: 0.500, loss: 1.175,data_loss: 1.175, reg_loss: 0.000 lr: 0.000869\n",
      " step: 99, acc: 0.648, loss: 0.980,data_loss: 0.980, reg_loss: 0.000 lr: 0.000869\n",
      " step: 100, acc: 0.531, loss: 1.143,data_loss: 1.143, reg_loss: 0.000 lr: 0.000869\n",
      " step: 101, acc: 0.555, loss: 1.124,data_loss: 1.124, reg_loss: 0.000 lr: 0.000869\n",
      " step: 102, acc: 0.562, loss: 1.167,data_loss: 1.167, reg_loss: 0.000 lr: 0.000869\n",
      " step: 103, acc: 0.547, loss: 1.138,data_loss: 1.138, reg_loss: 0.000 lr: 0.000869\n",
      " step: 104, acc: 0.578, loss: 1.121,data_loss: 1.121, reg_loss: 0.000 lr: 0.000868\n",
      " step: 105, acc: 0.602, loss: 1.096,data_loss: 1.096, reg_loss: 0.000 lr: 0.000868\n",
      " step: 106, acc: 0.555, loss: 1.088,data_loss: 1.088, reg_loss: 0.000 lr: 0.000868\n",
      " step: 107, acc: 0.578, loss: 1.167,data_loss: 1.167, reg_loss: 0.000 lr: 0.000868\n",
      " step: 108, acc: 0.555, loss: 1.174,data_loss: 1.174, reg_loss: 0.000 lr: 0.000868\n",
      " step: 109, acc: 0.578, loss: 1.145,data_loss: 1.145, reg_loss: 0.000 lr: 0.000868\n",
      " step: 110, acc: 0.562, loss: 1.140,data_loss: 1.140, reg_loss: 0.000 lr: 0.000868\n",
      " step: 111, acc: 0.562, loss: 1.253,data_loss: 1.253, reg_loss: 0.000 lr: 0.000868\n",
      " step: 112, acc: 0.539, loss: 1.167,data_loss: 1.167, reg_loss: 0.000 lr: 0.000868\n",
      " step: 113, acc: 0.570, loss: 1.111,data_loss: 1.111, reg_loss: 0.000 lr: 0.000868\n",
      " step: 114, acc: 0.469, loss: 1.327,data_loss: 1.327, reg_loss: 0.000 lr: 0.000868\n",
      " step: 115, acc: 0.516, loss: 1.146,data_loss: 1.146, reg_loss: 0.000 lr: 0.000868\n",
      " step: 116, acc: 0.539, loss: 1.275,data_loss: 1.275, reg_loss: 0.000 lr: 0.000868\n",
      " step: 117, acc: 0.594, loss: 1.080,data_loss: 1.080, reg_loss: 0.000 lr: 0.000868\n",
      " step: 118, acc: 0.570, loss: 1.126,data_loss: 1.126, reg_loss: 0.000 lr: 0.000868\n",
      " step: 119, acc: 0.625, loss: 1.020,data_loss: 1.020, reg_loss: 0.000 lr: 0.000868\n",
      " step: 120, acc: 0.539, loss: 1.211,data_loss: 1.211, reg_loss: 0.000 lr: 0.000868\n",
      " step: 121, acc: 0.484, loss: 1.301,data_loss: 1.301, reg_loss: 0.000 lr: 0.000868\n",
      " step: 122, acc: 0.547, loss: 1.203,data_loss: 1.203, reg_loss: 0.000 lr: 0.000868\n",
      " step: 123, acc: 0.547, loss: 1.185,data_loss: 1.185, reg_loss: 0.000 lr: 0.000868\n",
      " step: 124, acc: 0.531, loss: 1.201,data_loss: 1.201, reg_loss: 0.000 lr: 0.000868\n",
      " step: 125, acc: 0.562, loss: 1.168,data_loss: 1.168, reg_loss: 0.000 lr: 0.000868\n",
      " step: 126, acc: 0.562, loss: 1.265,data_loss: 1.265, reg_loss: 0.000 lr: 0.000868\n",
      " step: 127, acc: 0.445, loss: 1.271,data_loss: 1.271, reg_loss: 0.000 lr: 0.000868\n",
      " step: 128, acc: 0.586, loss: 1.206,data_loss: 1.206, reg_loss: 0.000 lr: 0.000868\n",
      " step: 129, acc: 0.570, loss: 1.210,data_loss: 1.210, reg_loss: 0.000 lr: 0.000868\n",
      " step: 130, acc: 0.461, loss: 1.335,data_loss: 1.335, reg_loss: 0.000 lr: 0.000867\n",
      " step: 131, acc: 0.508, loss: 1.287,data_loss: 1.287, reg_loss: 0.000 lr: 0.000867\n",
      " step: 132, acc: 0.516, loss: 1.114,data_loss: 1.114, reg_loss: 0.000 lr: 0.000867\n",
      " step: 133, acc: 0.547, loss: 1.216,data_loss: 1.216, reg_loss: 0.000 lr: 0.000867\n",
      " step: 134, acc: 0.617, loss: 1.036,data_loss: 1.036, reg_loss: 0.000 lr: 0.000867\n",
      " step: 135, acc: 0.484, loss: 1.186,data_loss: 1.186, reg_loss: 0.000 lr: 0.000867\n",
      " step: 136, acc: 0.500, loss: 1.337,data_loss: 1.337, reg_loss: 0.000 lr: 0.000867\n",
      " step: 137, acc: 0.633, loss: 1.071,data_loss: 1.071, reg_loss: 0.000 lr: 0.000867\n",
      " step: 138, acc: 0.586, loss: 1.121,data_loss: 1.121, reg_loss: 0.000 lr: 0.000867\n",
      " step: 139, acc: 0.555, loss: 1.072,data_loss: 1.072, reg_loss: 0.000 lr: 0.000867\n",
      " step: 140, acc: 0.578, loss: 1.104,data_loss: 1.104, reg_loss: 0.000 lr: 0.000867\n",
      " step: 141, acc: 0.500, loss: 1.175,data_loss: 1.175, reg_loss: 0.000 lr: 0.000867\n",
      " step: 142, acc: 0.578, loss: 1.108,data_loss: 1.108, reg_loss: 0.000 lr: 0.000867\n",
      " step: 143, acc: 0.492, loss: 1.221,data_loss: 1.221, reg_loss: 0.000 lr: 0.000867\n",
      " step: 144, acc: 0.531, loss: 1.279,data_loss: 1.279, reg_loss: 0.000 lr: 0.000867\n",
      " step: 145, acc: 0.539, loss: 1.176,data_loss: 1.176, reg_loss: 0.000 lr: 0.000867\n",
      " step: 146, acc: 0.508, loss: 1.238,data_loss: 1.238, reg_loss: 0.000 lr: 0.000867\n",
      " step: 147, acc: 0.570, loss: 1.166,data_loss: 1.166, reg_loss: 0.000 lr: 0.000867\n",
      " step: 148, acc: 0.523, loss: 1.138,data_loss: 1.138, reg_loss: 0.000 lr: 0.000867\n",
      " step: 149, acc: 0.531, loss: 1.185,data_loss: 1.185, reg_loss: 0.000 lr: 0.000867\n",
      " step: 150, acc: 0.594, loss: 1.045,data_loss: 1.045, reg_loss: 0.000 lr: 0.000867\n",
      " step: 151, acc: 0.492, loss: 1.233,data_loss: 1.233, reg_loss: 0.000 lr: 0.000867\n",
      " step: 152, acc: 0.641, loss: 1.056,data_loss: 1.056, reg_loss: 0.000 lr: 0.000867\n",
      " step: 153, acc: 0.570, loss: 1.052,data_loss: 1.052, reg_loss: 0.000 lr: 0.000867\n",
      " step: 154, acc: 0.547, loss: 1.187,data_loss: 1.187, reg_loss: 0.000 lr: 0.000867\n",
      " step: 155, acc: 0.562, loss: 1.096,data_loss: 1.096, reg_loss: 0.000 lr: 0.000867\n",
      " step: 156, acc: 0.523, loss: 1.290,data_loss: 1.290, reg_loss: 0.000 lr: 0.000867\n",
      " step: 157, acc: 0.578, loss: 1.060,data_loss: 1.060, reg_loss: 0.000 lr: 0.000866\n",
      " step: 158, acc: 0.500, loss: 1.294,data_loss: 1.294, reg_loss: 0.000 lr: 0.000866\n",
      " step: 159, acc: 0.539, loss: 1.238,data_loss: 1.238, reg_loss: 0.000 lr: 0.000866\n",
      " step: 160, acc: 0.484, loss: 1.395,data_loss: 1.395, reg_loss: 0.000 lr: 0.000866\n",
      " step: 161, acc: 0.516, loss: 1.265,data_loss: 1.265, reg_loss: 0.000 lr: 0.000866\n",
      " step: 162, acc: 0.555, loss: 1.218,data_loss: 1.218, reg_loss: 0.000 lr: 0.000866\n",
      " step: 163, acc: 0.484, loss: 1.155,data_loss: 1.155, reg_loss: 0.000 lr: 0.000866\n",
      " step: 164, acc: 0.531, loss: 1.133,data_loss: 1.133, reg_loss: 0.000 lr: 0.000866\n",
      " step: 165, acc: 0.555, loss: 1.335,data_loss: 1.335, reg_loss: 0.000 lr: 0.000866\n",
      " step: 166, acc: 0.461, loss: 1.264,data_loss: 1.264, reg_loss: 0.000 lr: 0.000866\n",
      " step: 167, acc: 0.508, loss: 1.268,data_loss: 1.268, reg_loss: 0.000 lr: 0.000866\n",
      " step: 168, acc: 0.516, loss: 1.160,data_loss: 1.160, reg_loss: 0.000 lr: 0.000866\n",
      " step: 169, acc: 0.570, loss: 1.197,data_loss: 1.197, reg_loss: 0.000 lr: 0.000866\n",
      " step: 170, acc: 0.547, loss: 1.221,data_loss: 1.221, reg_loss: 0.000 lr: 0.000866\n",
      " step: 171, acc: 0.555, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000866\n",
      " step: 172, acc: 0.484, loss: 1.246,data_loss: 1.246, reg_loss: 0.000 lr: 0.000866\n",
      " step: 173, acc: 0.570, loss: 1.149,data_loss: 1.149, reg_loss: 0.000 lr: 0.000866\n",
      " step: 174, acc: 0.508, loss: 1.212,data_loss: 1.212, reg_loss: 0.000 lr: 0.000866\n",
      " step: 175, acc: 0.469, loss: 1.223,data_loss: 1.223, reg_loss: 0.000 lr: 0.000866\n",
      " step: 176, acc: 0.531, loss: 1.258,data_loss: 1.258, reg_loss: 0.000 lr: 0.000866\n",
      " step: 177, acc: 0.539, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000866\n",
      " step: 178, acc: 0.500, loss: 1.343,data_loss: 1.343, reg_loss: 0.000 lr: 0.000866\n",
      " step: 179, acc: 0.578, loss: 1.074,data_loss: 1.074, reg_loss: 0.000 lr: 0.000866\n",
      " step: 180, acc: 0.617, loss: 1.053,data_loss: 1.053, reg_loss: 0.000 lr: 0.000866\n",
      " step: 181, acc: 0.516, loss: 1.205,data_loss: 1.205, reg_loss: 0.000 lr: 0.000866\n",
      " step: 182, acc: 0.531, loss: 1.194,data_loss: 1.194, reg_loss: 0.000 lr: 0.000866\n",
      " step: 183, acc: 0.578, loss: 1.098,data_loss: 1.098, reg_loss: 0.000 lr: 0.000866\n",
      " step: 184, acc: 0.555, loss: 1.197,data_loss: 1.197, reg_loss: 0.000 lr: 0.000865\n",
      " step: 185, acc: 0.578, loss: 1.123,data_loss: 1.123, reg_loss: 0.000 lr: 0.000865\n",
      " step: 186, acc: 0.477, loss: 1.188,data_loss: 1.188, reg_loss: 0.000 lr: 0.000865\n",
      " step: 187, acc: 0.578, loss: 1.061,data_loss: 1.061, reg_loss: 0.000 lr: 0.000865\n",
      " step: 188, acc: 0.484, loss: 1.254,data_loss: 1.254, reg_loss: 0.000 lr: 0.000865\n",
      " step: 189, acc: 0.516, loss: 1.304,data_loss: 1.304, reg_loss: 0.000 lr: 0.000865\n",
      " step: 190, acc: 0.492, loss: 1.361,data_loss: 1.361, reg_loss: 0.000 lr: 0.000865\n",
      " step: 191, acc: 0.555, loss: 1.291,data_loss: 1.291, reg_loss: 0.000 lr: 0.000865\n",
      " step: 192, acc: 0.531, loss: 1.164,data_loss: 1.164, reg_loss: 0.000 lr: 0.000865\n",
      " step: 193, acc: 0.562, loss: 1.199,data_loss: 1.199, reg_loss: 0.000 lr: 0.000865\n",
      " step: 194, acc: 0.570, loss: 1.142,data_loss: 1.142, reg_loss: 0.000 lr: 0.000865\n",
      " step: 195, acc: 0.555, loss: 1.130,data_loss: 1.130, reg_loss: 0.000 lr: 0.000865\n",
      " step: 196, acc: 0.523, loss: 1.161,data_loss: 1.161, reg_loss: 0.000 lr: 0.000865\n",
      " step: 197, acc: 0.547, loss: 1.161,data_loss: 1.161, reg_loss: 0.000 lr: 0.000865\n",
      " step: 198, acc: 0.586, loss: 1.084,data_loss: 1.084, reg_loss: 0.000 lr: 0.000865\n",
      " step: 199, acc: 0.445, loss: 1.342,data_loss: 1.342, reg_loss: 0.000 lr: 0.000865\n",
      " step: 200, acc: 0.547, loss: 1.176,data_loss: 1.176, reg_loss: 0.000 lr: 0.000865\n",
      " step: 201, acc: 0.609, loss: 1.176,data_loss: 1.176, reg_loss: 0.000 lr: 0.000865\n",
      " step: 202, acc: 0.516, loss: 1.154,data_loss: 1.154, reg_loss: 0.000 lr: 0.000865\n",
      " step: 203, acc: 0.477, loss: 1.181,data_loss: 1.181, reg_loss: 0.000 lr: 0.000865\n",
      " step: 204, acc: 0.562, loss: 1.083,data_loss: 1.083, reg_loss: 0.000 lr: 0.000865\n",
      " step: 205, acc: 0.508, loss: 1.241,data_loss: 1.241, reg_loss: 0.000 lr: 0.000865\n",
      " step: 206, acc: 0.617, loss: 1.047,data_loss: 1.047, reg_loss: 0.000 lr: 0.000865\n",
      " step: 207, acc: 0.539, loss: 1.119,data_loss: 1.119, reg_loss: 0.000 lr: 0.000865\n",
      " step: 208, acc: 0.578, loss: 1.096,data_loss: 1.096, reg_loss: 0.000 lr: 0.000865\n",
      " step: 209, acc: 0.570, loss: 1.146,data_loss: 1.146, reg_loss: 0.000 lr: 0.000865\n",
      " step: 210, acc: 0.602, loss: 1.124,data_loss: 1.124, reg_loss: 0.000 lr: 0.000864\n",
      " step: 211, acc: 0.562, loss: 1.083,data_loss: 1.083, reg_loss: 0.000 lr: 0.000864\n",
      " step: 212, acc: 0.562, loss: 1.171,data_loss: 1.171, reg_loss: 0.000 lr: 0.000864\n",
      " step: 213, acc: 0.562, loss: 1.209,data_loss: 1.209, reg_loss: 0.000 lr: 0.000864\n",
      " step: 214, acc: 0.531, loss: 1.294,data_loss: 1.294, reg_loss: 0.000 lr: 0.000864\n",
      " step: 215, acc: 0.617, loss: 1.155,data_loss: 1.155, reg_loss: 0.000 lr: 0.000864\n",
      " step: 216, acc: 0.562, loss: 1.020,data_loss: 1.020, reg_loss: 0.000 lr: 0.000864\n",
      " step: 217, acc: 0.547, loss: 1.184,data_loss: 1.184, reg_loss: 0.000 lr: 0.000864\n",
      " step: 218, acc: 0.609, loss: 1.131,data_loss: 1.131, reg_loss: 0.000 lr: 0.000864\n",
      " step: 219, acc: 0.523, loss: 1.177,data_loss: 1.177, reg_loss: 0.000 lr: 0.000864\n",
      " step: 220, acc: 0.594, loss: 1.095,data_loss: 1.095, reg_loss: 0.000 lr: 0.000864\n",
      " step: 221, acc: 0.523, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000864\n",
      " step: 222, acc: 0.531, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000864\n",
      " step: 223, acc: 0.508, loss: 1.215,data_loss: 1.215, reg_loss: 0.000 lr: 0.000864\n",
      " step: 224, acc: 0.703, loss: 0.893,data_loss: 0.893, reg_loss: 0.000 lr: 0.000864\n",
      "training , acc: 0.544, loss: 1.180,data_loss: 1.180, reg_loss: 0.000 lr: 0.000864\n",
      "validation, acc:0.446 ,loss: 1.505 \n",
      "epoch: 15\n",
      " step: 0, acc: 0.477, loss: 1.299,data_loss: 1.299, reg_loss: 0.000 lr: 0.000864\n",
      " step: 1, acc: 0.516, loss: 1.205,data_loss: 1.205, reg_loss: 0.000 lr: 0.000864\n",
      " step: 2, acc: 0.492, loss: 1.217,data_loss: 1.217, reg_loss: 0.000 lr: 0.000864\n",
      " step: 3, acc: 0.562, loss: 1.166,data_loss: 1.166, reg_loss: 0.000 lr: 0.000864\n",
      " step: 4, acc: 0.617, loss: 1.087,data_loss: 1.087, reg_loss: 0.000 lr: 0.000864\n",
      " step: 5, acc: 0.508, loss: 1.268,data_loss: 1.268, reg_loss: 0.000 lr: 0.000864\n",
      " step: 6, acc: 0.570, loss: 1.099,data_loss: 1.099, reg_loss: 0.000 lr: 0.000864\n",
      " step: 7, acc: 0.492, loss: 1.245,data_loss: 1.245, reg_loss: 0.000 lr: 0.000864\n",
      " step: 8, acc: 0.562, loss: 1.144,data_loss: 1.144, reg_loss: 0.000 lr: 0.000864\n",
      " step: 9, acc: 0.625, loss: 1.064,data_loss: 1.064, reg_loss: 0.000 lr: 0.000864\n",
      " step: 10, acc: 0.688, loss: 0.993,data_loss: 0.993, reg_loss: 0.000 lr: 0.000864\n",
      " step: 11, acc: 0.617, loss: 1.128,data_loss: 1.128, reg_loss: 0.000 lr: 0.000864\n",
      " step: 12, acc: 0.547, loss: 1.168,data_loss: 1.168, reg_loss: 0.000 lr: 0.000863\n",
      " step: 13, acc: 0.516, loss: 1.212,data_loss: 1.212, reg_loss: 0.000 lr: 0.000863\n",
      " step: 14, acc: 0.539, loss: 1.170,data_loss: 1.170, reg_loss: 0.000 lr: 0.000863\n",
      " step: 15, acc: 0.602, loss: 1.145,data_loss: 1.145, reg_loss: 0.000 lr: 0.000863\n",
      " step: 16, acc: 0.531, loss: 1.122,data_loss: 1.122, reg_loss: 0.000 lr: 0.000863\n",
      " step: 17, acc: 0.586, loss: 1.130,data_loss: 1.130, reg_loss: 0.000 lr: 0.000863\n",
      " step: 18, acc: 0.555, loss: 1.181,data_loss: 1.181, reg_loss: 0.000 lr: 0.000863\n",
      " step: 19, acc: 0.617, loss: 0.968,data_loss: 0.968, reg_loss: 0.000 lr: 0.000863\n",
      " step: 20, acc: 0.547, loss: 1.110,data_loss: 1.110, reg_loss: 0.000 lr: 0.000863\n",
      " step: 21, acc: 0.625, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000863\n",
      " step: 22, acc: 0.570, loss: 1.098,data_loss: 1.098, reg_loss: 0.000 lr: 0.000863\n",
      " step: 23, acc: 0.578, loss: 1.115,data_loss: 1.115, reg_loss: 0.000 lr: 0.000863\n",
      " step: 24, acc: 0.602, loss: 1.118,data_loss: 1.118, reg_loss: 0.000 lr: 0.000863\n",
      " step: 25, acc: 0.625, loss: 1.084,data_loss: 1.084, reg_loss: 0.000 lr: 0.000863\n",
      " step: 26, acc: 0.492, loss: 1.219,data_loss: 1.219, reg_loss: 0.000 lr: 0.000863\n",
      " step: 27, acc: 0.531, loss: 1.161,data_loss: 1.161, reg_loss: 0.000 lr: 0.000863\n",
      " step: 28, acc: 0.617, loss: 1.107,data_loss: 1.107, reg_loss: 0.000 lr: 0.000863\n",
      " step: 29, acc: 0.539, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000863\n",
      " step: 30, acc: 0.547, loss: 1.256,data_loss: 1.256, reg_loss: 0.000 lr: 0.000863\n",
      " step: 31, acc: 0.570, loss: 1.005,data_loss: 1.005, reg_loss: 0.000 lr: 0.000863\n",
      " step: 32, acc: 0.570, loss: 1.100,data_loss: 1.100, reg_loss: 0.000 lr: 0.000863\n",
      " step: 33, acc: 0.570, loss: 1.190,data_loss: 1.190, reg_loss: 0.000 lr: 0.000863\n",
      " step: 34, acc: 0.570, loss: 1.177,data_loss: 1.177, reg_loss: 0.000 lr: 0.000863\n",
      " step: 35, acc: 0.570, loss: 1.085,data_loss: 1.085, reg_loss: 0.000 lr: 0.000863\n",
      " step: 36, acc: 0.578, loss: 1.070,data_loss: 1.070, reg_loss: 0.000 lr: 0.000863\n",
      " step: 37, acc: 0.586, loss: 1.067,data_loss: 1.067, reg_loss: 0.000 lr: 0.000863\n",
      " step: 38, acc: 0.578, loss: 1.087,data_loss: 1.087, reg_loss: 0.000 lr: 0.000863\n",
      " step: 39, acc: 0.484, loss: 1.300,data_loss: 1.300, reg_loss: 0.000 lr: 0.000862\n",
      " step: 40, acc: 0.586, loss: 1.055,data_loss: 1.055, reg_loss: 0.000 lr: 0.000862\n",
      " step: 41, acc: 0.539, loss: 1.237,data_loss: 1.237, reg_loss: 0.000 lr: 0.000862\n",
      " step: 42, acc: 0.570, loss: 1.096,data_loss: 1.096, reg_loss: 0.000 lr: 0.000862\n",
      " step: 43, acc: 0.555, loss: 1.164,data_loss: 1.164, reg_loss: 0.000 lr: 0.000862\n",
      " step: 44, acc: 0.570, loss: 1.106,data_loss: 1.106, reg_loss: 0.000 lr: 0.000862\n",
      " step: 45, acc: 0.516, loss: 1.279,data_loss: 1.279, reg_loss: 0.000 lr: 0.000862\n",
      " step: 46, acc: 0.547, loss: 1.200,data_loss: 1.200, reg_loss: 0.000 lr: 0.000862\n",
      " step: 47, acc: 0.547, loss: 1.098,data_loss: 1.098, reg_loss: 0.000 lr: 0.000862\n",
      " step: 48, acc: 0.609, loss: 1.082,data_loss: 1.082, reg_loss: 0.000 lr: 0.000862\n",
      " step: 49, acc: 0.586, loss: 1.113,data_loss: 1.113, reg_loss: 0.000 lr: 0.000862\n",
      " step: 50, acc: 0.625, loss: 1.053,data_loss: 1.053, reg_loss: 0.000 lr: 0.000862\n",
      " step: 51, acc: 0.547, loss: 1.112,data_loss: 1.112, reg_loss: 0.000 lr: 0.000862\n",
      " step: 52, acc: 0.578, loss: 1.103,data_loss: 1.103, reg_loss: 0.000 lr: 0.000862\n",
      " step: 53, acc: 0.586, loss: 1.164,data_loss: 1.164, reg_loss: 0.000 lr: 0.000862\n",
      " step: 54, acc: 0.477, loss: 1.342,data_loss: 1.342, reg_loss: 0.000 lr: 0.000862\n",
      " step: 55, acc: 0.547, loss: 1.088,data_loss: 1.088, reg_loss: 0.000 lr: 0.000862\n",
      " step: 56, acc: 0.555, loss: 1.118,data_loss: 1.118, reg_loss: 0.000 lr: 0.000862\n",
      " step: 57, acc: 0.594, loss: 1.104,data_loss: 1.104, reg_loss: 0.000 lr: 0.000862\n",
      " step: 58, acc: 0.562, loss: 1.160,data_loss: 1.160, reg_loss: 0.000 lr: 0.000862\n",
      " step: 59, acc: 0.570, loss: 1.138,data_loss: 1.138, reg_loss: 0.000 lr: 0.000862\n",
      " step: 60, acc: 0.609, loss: 0.969,data_loss: 0.969, reg_loss: 0.000 lr: 0.000862\n",
      " step: 61, acc: 0.562, loss: 1.201,data_loss: 1.201, reg_loss: 0.000 lr: 0.000862\n",
      " step: 62, acc: 0.562, loss: 1.136,data_loss: 1.136, reg_loss: 0.000 lr: 0.000862\n",
      " step: 63, acc: 0.539, loss: 1.126,data_loss: 1.126, reg_loss: 0.000 lr: 0.000862\n",
      " step: 64, acc: 0.500, loss: 1.286,data_loss: 1.286, reg_loss: 0.000 lr: 0.000862\n",
      " step: 65, acc: 0.492, loss: 1.272,data_loss: 1.272, reg_loss: 0.000 lr: 0.000862\n",
      " step: 66, acc: 0.609, loss: 1.009,data_loss: 1.009, reg_loss: 0.000 lr: 0.000861\n",
      " step: 67, acc: 0.531, loss: 1.240,data_loss: 1.240, reg_loss: 0.000 lr: 0.000861\n",
      " step: 68, acc: 0.531, loss: 1.183,data_loss: 1.183, reg_loss: 0.000 lr: 0.000861\n",
      " step: 69, acc: 0.500, loss: 1.208,data_loss: 1.208, reg_loss: 0.000 lr: 0.000861\n",
      " step: 70, acc: 0.578, loss: 1.108,data_loss: 1.108, reg_loss: 0.000 lr: 0.000861\n",
      " step: 71, acc: 0.594, loss: 1.150,data_loss: 1.150, reg_loss: 0.000 lr: 0.000861\n",
      " step: 72, acc: 0.547, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000861\n",
      " step: 73, acc: 0.492, loss: 1.203,data_loss: 1.203, reg_loss: 0.000 lr: 0.000861\n",
      " step: 74, acc: 0.523, loss: 1.195,data_loss: 1.195, reg_loss: 0.000 lr: 0.000861\n",
      " step: 75, acc: 0.531, loss: 1.249,data_loss: 1.249, reg_loss: 0.000 lr: 0.000861\n",
      " step: 76, acc: 0.617, loss: 1.044,data_loss: 1.044, reg_loss: 0.000 lr: 0.000861\n",
      " step: 77, acc: 0.539, loss: 1.170,data_loss: 1.170, reg_loss: 0.000 lr: 0.000861\n",
      " step: 78, acc: 0.555, loss: 1.088,data_loss: 1.088, reg_loss: 0.000 lr: 0.000861\n",
      " step: 79, acc: 0.516, loss: 1.232,data_loss: 1.232, reg_loss: 0.000 lr: 0.000861\n",
      " step: 80, acc: 0.508, loss: 1.247,data_loss: 1.247, reg_loss: 0.000 lr: 0.000861\n",
      " step: 81, acc: 0.586, loss: 1.149,data_loss: 1.149, reg_loss: 0.000 lr: 0.000861\n",
      " step: 82, acc: 0.547, loss: 1.158,data_loss: 1.158, reg_loss: 0.000 lr: 0.000861\n",
      " step: 83, acc: 0.531, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000861\n",
      " step: 84, acc: 0.547, loss: 1.197,data_loss: 1.197, reg_loss: 0.000 lr: 0.000861\n",
      " step: 85, acc: 0.602, loss: 1.114,data_loss: 1.114, reg_loss: 0.000 lr: 0.000861\n",
      " step: 86, acc: 0.586, loss: 1.051,data_loss: 1.051, reg_loss: 0.000 lr: 0.000861\n",
      " step: 87, acc: 0.539, loss: 1.150,data_loss: 1.150, reg_loss: 0.000 lr: 0.000861\n",
      " step: 88, acc: 0.484, loss: 1.236,data_loss: 1.236, reg_loss: 0.000 lr: 0.000861\n",
      " step: 89, acc: 0.469, loss: 1.280,data_loss: 1.280, reg_loss: 0.000 lr: 0.000861\n",
      " step: 90, acc: 0.570, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000861\n",
      " step: 91, acc: 0.484, loss: 1.313,data_loss: 1.313, reg_loss: 0.000 lr: 0.000861\n",
      " step: 92, acc: 0.523, loss: 1.130,data_loss: 1.130, reg_loss: 0.000 lr: 0.000861\n",
      " step: 93, acc: 0.578, loss: 1.082,data_loss: 1.082, reg_loss: 0.000 lr: 0.000860\n",
      " step: 94, acc: 0.516, loss: 1.295,data_loss: 1.295, reg_loss: 0.000 lr: 0.000860\n",
      " step: 95, acc: 0.609, loss: 1.037,data_loss: 1.037, reg_loss: 0.000 lr: 0.000860\n",
      " step: 96, acc: 0.492, loss: 1.216,data_loss: 1.216, reg_loss: 0.000 lr: 0.000860\n",
      " step: 97, acc: 0.578, loss: 1.093,data_loss: 1.093, reg_loss: 0.000 lr: 0.000860\n",
      " step: 98, acc: 0.516, loss: 1.127,data_loss: 1.127, reg_loss: 0.000 lr: 0.000860\n",
      " step: 99, acc: 0.586, loss: 1.069,data_loss: 1.069, reg_loss: 0.000 lr: 0.000860\n",
      " step: 100, acc: 0.445, loss: 1.157,data_loss: 1.157, reg_loss: 0.000 lr: 0.000860\n",
      " step: 101, acc: 0.508, loss: 1.220,data_loss: 1.220, reg_loss: 0.000 lr: 0.000860\n",
      " step: 102, acc: 0.500, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000860\n",
      " step: 103, acc: 0.586, loss: 1.156,data_loss: 1.156, reg_loss: 0.000 lr: 0.000860\n",
      " step: 104, acc: 0.594, loss: 1.088,data_loss: 1.088, reg_loss: 0.000 lr: 0.000860\n",
      " step: 105, acc: 0.594, loss: 1.091,data_loss: 1.091, reg_loss: 0.000 lr: 0.000860\n",
      " step: 106, acc: 0.562, loss: 1.028,data_loss: 1.028, reg_loss: 0.000 lr: 0.000860\n",
      " step: 107, acc: 0.539, loss: 1.061,data_loss: 1.061, reg_loss: 0.000 lr: 0.000860\n",
      " step: 108, acc: 0.586, loss: 1.123,data_loss: 1.123, reg_loss: 0.000 lr: 0.000860\n",
      " step: 109, acc: 0.531, loss: 1.235,data_loss: 1.235, reg_loss: 0.000 lr: 0.000860\n",
      " step: 110, acc: 0.523, loss: 1.162,data_loss: 1.162, reg_loss: 0.000 lr: 0.000860\n",
      " step: 111, acc: 0.586, loss: 1.196,data_loss: 1.196, reg_loss: 0.000 lr: 0.000860\n",
      " step: 112, acc: 0.555, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000860\n",
      " step: 113, acc: 0.586, loss: 1.101,data_loss: 1.101, reg_loss: 0.000 lr: 0.000860\n",
      " step: 114, acc: 0.508, loss: 1.234,data_loss: 1.234, reg_loss: 0.000 lr: 0.000860\n",
      " step: 115, acc: 0.539, loss: 1.139,data_loss: 1.139, reg_loss: 0.000 lr: 0.000860\n",
      " step: 116, acc: 0.547, loss: 1.275,data_loss: 1.275, reg_loss: 0.000 lr: 0.000860\n",
      " step: 117, acc: 0.547, loss: 1.111,data_loss: 1.111, reg_loss: 0.000 lr: 0.000860\n",
      " step: 118, acc: 0.609, loss: 1.031,data_loss: 1.031, reg_loss: 0.000 lr: 0.000860\n",
      " step: 119, acc: 0.641, loss: 1.026,data_loss: 1.026, reg_loss: 0.000 lr: 0.000860\n",
      " step: 120, acc: 0.516, loss: 1.218,data_loss: 1.218, reg_loss: 0.000 lr: 0.000859\n",
      " step: 121, acc: 0.547, loss: 1.341,data_loss: 1.341, reg_loss: 0.000 lr: 0.000859\n",
      " step: 122, acc: 0.477, loss: 1.271,data_loss: 1.271, reg_loss: 0.000 lr: 0.000859\n",
      " step: 123, acc: 0.547, loss: 1.145,data_loss: 1.145, reg_loss: 0.000 lr: 0.000859\n",
      " step: 124, acc: 0.539, loss: 1.167,data_loss: 1.167, reg_loss: 0.000 lr: 0.000859\n",
      " step: 125, acc: 0.602, loss: 1.073,data_loss: 1.073, reg_loss: 0.000 lr: 0.000859\n",
      " step: 126, acc: 0.516, loss: 1.201,data_loss: 1.201, reg_loss: 0.000 lr: 0.000859\n",
      " step: 127, acc: 0.438, loss: 1.406,data_loss: 1.406, reg_loss: 0.000 lr: 0.000859\n",
      " step: 128, acc: 0.523, loss: 1.105,data_loss: 1.105, reg_loss: 0.000 lr: 0.000859\n",
      " step: 129, acc: 0.594, loss: 1.106,data_loss: 1.106, reg_loss: 0.000 lr: 0.000859\n",
      " step: 130, acc: 0.438, loss: 1.280,data_loss: 1.280, reg_loss: 0.000 lr: 0.000859\n",
      " step: 131, acc: 0.492, loss: 1.257,data_loss: 1.257, reg_loss: 0.000 lr: 0.000859\n",
      " step: 132, acc: 0.547, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000859\n",
      " step: 133, acc: 0.578, loss: 1.164,data_loss: 1.164, reg_loss: 0.000 lr: 0.000859\n",
      " step: 134, acc: 0.633, loss: 0.929,data_loss: 0.929, reg_loss: 0.000 lr: 0.000859\n",
      " step: 135, acc: 0.609, loss: 1.053,data_loss: 1.053, reg_loss: 0.000 lr: 0.000859\n",
      " step: 136, acc: 0.500, loss: 1.365,data_loss: 1.365, reg_loss: 0.000 lr: 0.000859\n",
      " step: 137, acc: 0.648, loss: 0.981,data_loss: 0.981, reg_loss: 0.000 lr: 0.000859\n",
      " step: 138, acc: 0.570, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000859\n",
      " step: 139, acc: 0.539, loss: 1.153,data_loss: 1.153, reg_loss: 0.000 lr: 0.000859\n",
      " step: 140, acc: 0.609, loss: 1.023,data_loss: 1.023, reg_loss: 0.000 lr: 0.000859\n",
      " step: 141, acc: 0.531, loss: 1.155,data_loss: 1.155, reg_loss: 0.000 lr: 0.000859\n",
      " step: 142, acc: 0.609, loss: 1.139,data_loss: 1.139, reg_loss: 0.000 lr: 0.000859\n",
      " step: 143, acc: 0.578, loss: 1.182,data_loss: 1.182, reg_loss: 0.000 lr: 0.000859\n",
      " step: 144, acc: 0.508, loss: 1.232,data_loss: 1.232, reg_loss: 0.000 lr: 0.000859\n",
      " step: 145, acc: 0.586, loss: 1.088,data_loss: 1.088, reg_loss: 0.000 lr: 0.000859\n",
      " step: 146, acc: 0.547, loss: 1.198,data_loss: 1.198, reg_loss: 0.000 lr: 0.000859\n",
      " step: 147, acc: 0.523, loss: 1.183,data_loss: 1.183, reg_loss: 0.000 lr: 0.000858\n",
      " step: 148, acc: 0.586, loss: 1.011,data_loss: 1.011, reg_loss: 0.000 lr: 0.000858\n",
      " step: 149, acc: 0.500, loss: 1.129,data_loss: 1.129, reg_loss: 0.000 lr: 0.000858\n",
      " step: 150, acc: 0.547, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000858\n",
      " step: 151, acc: 0.578, loss: 1.112,data_loss: 1.112, reg_loss: 0.000 lr: 0.000858\n",
      " step: 152, acc: 0.609, loss: 1.051,data_loss: 1.051, reg_loss: 0.000 lr: 0.000858\n",
      " step: 153, acc: 0.617, loss: 1.067,data_loss: 1.067, reg_loss: 0.000 lr: 0.000858\n",
      " step: 154, acc: 0.516, loss: 1.259,data_loss: 1.259, reg_loss: 0.000 lr: 0.000858\n",
      " step: 155, acc: 0.539, loss: 1.123,data_loss: 1.123, reg_loss: 0.000 lr: 0.000858\n",
      " step: 156, acc: 0.516, loss: 1.222,data_loss: 1.222, reg_loss: 0.000 lr: 0.000858\n",
      " step: 157, acc: 0.594, loss: 1.042,data_loss: 1.042, reg_loss: 0.000 lr: 0.000858\n",
      " step: 158, acc: 0.555, loss: 1.231,data_loss: 1.231, reg_loss: 0.000 lr: 0.000858\n",
      " step: 159, acc: 0.523, loss: 1.246,data_loss: 1.246, reg_loss: 0.000 lr: 0.000858\n",
      " step: 160, acc: 0.484, loss: 1.311,data_loss: 1.311, reg_loss: 0.000 lr: 0.000858\n",
      " step: 161, acc: 0.500, loss: 1.258,data_loss: 1.258, reg_loss: 0.000 lr: 0.000858\n",
      " step: 162, acc: 0.516, loss: 1.217,data_loss: 1.217, reg_loss: 0.000 lr: 0.000858\n",
      " step: 163, acc: 0.602, loss: 1.033,data_loss: 1.033, reg_loss: 0.000 lr: 0.000858\n",
      " step: 164, acc: 0.594, loss: 1.060,data_loss: 1.060, reg_loss: 0.000 lr: 0.000858\n",
      " step: 165, acc: 0.508, loss: 1.240,data_loss: 1.240, reg_loss: 0.000 lr: 0.000858\n",
      " step: 166, acc: 0.492, loss: 1.265,data_loss: 1.265, reg_loss: 0.000 lr: 0.000858\n",
      " step: 167, acc: 0.531, loss: 1.185,data_loss: 1.185, reg_loss: 0.000 lr: 0.000858\n",
      " step: 168, acc: 0.531, loss: 1.075,data_loss: 1.075, reg_loss: 0.000 lr: 0.000858\n",
      " step: 169, acc: 0.531, loss: 1.170,data_loss: 1.170, reg_loss: 0.000 lr: 0.000858\n",
      " step: 170, acc: 0.531, loss: 1.165,data_loss: 1.165, reg_loss: 0.000 lr: 0.000858\n",
      " step: 171, acc: 0.609, loss: 1.146,data_loss: 1.146, reg_loss: 0.000 lr: 0.000858\n",
      " step: 172, acc: 0.484, loss: 1.226,data_loss: 1.226, reg_loss: 0.000 lr: 0.000858\n",
      " step: 173, acc: 0.539, loss: 1.232,data_loss: 1.232, reg_loss: 0.000 lr: 0.000858\n",
      " step: 174, acc: 0.562, loss: 1.157,data_loss: 1.157, reg_loss: 0.000 lr: 0.000857\n",
      " step: 175, acc: 0.508, loss: 1.268,data_loss: 1.268, reg_loss: 0.000 lr: 0.000857\n",
      " step: 176, acc: 0.500, loss: 1.257,data_loss: 1.257, reg_loss: 0.000 lr: 0.000857\n",
      " step: 177, acc: 0.570, loss: 1.140,data_loss: 1.140, reg_loss: 0.000 lr: 0.000857\n",
      " step: 178, acc: 0.547, loss: 1.335,data_loss: 1.335, reg_loss: 0.000 lr: 0.000857\n",
      " step: 179, acc: 0.625, loss: 1.011,data_loss: 1.011, reg_loss: 0.000 lr: 0.000857\n",
      " step: 180, acc: 0.609, loss: 1.016,data_loss: 1.016, reg_loss: 0.000 lr: 0.000857\n",
      " step: 181, acc: 0.531, loss: 1.194,data_loss: 1.194, reg_loss: 0.000 lr: 0.000857\n",
      " step: 182, acc: 0.562, loss: 1.114,data_loss: 1.114, reg_loss: 0.000 lr: 0.000857\n",
      " step: 183, acc: 0.555, loss: 1.133,data_loss: 1.133, reg_loss: 0.000 lr: 0.000857\n",
      " step: 184, acc: 0.508, loss: 1.171,data_loss: 1.171, reg_loss: 0.000 lr: 0.000857\n",
      " step: 185, acc: 0.547, loss: 1.199,data_loss: 1.199, reg_loss: 0.000 lr: 0.000857\n",
      " step: 186, acc: 0.547, loss: 1.125,data_loss: 1.125, reg_loss: 0.000 lr: 0.000857\n",
      " step: 187, acc: 0.594, loss: 1.089,data_loss: 1.089, reg_loss: 0.000 lr: 0.000857\n",
      " step: 188, acc: 0.500, loss: 1.248,data_loss: 1.248, reg_loss: 0.000 lr: 0.000857\n",
      " step: 189, acc: 0.539, loss: 1.199,data_loss: 1.199, reg_loss: 0.000 lr: 0.000857\n",
      " step: 190, acc: 0.516, loss: 1.248,data_loss: 1.248, reg_loss: 0.000 lr: 0.000857\n",
      " step: 191, acc: 0.578, loss: 1.179,data_loss: 1.179, reg_loss: 0.000 lr: 0.000857\n",
      " step: 192, acc: 0.594, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000857\n",
      " step: 193, acc: 0.555, loss: 1.145,data_loss: 1.145, reg_loss: 0.000 lr: 0.000857\n",
      " step: 194, acc: 0.586, loss: 1.061,data_loss: 1.061, reg_loss: 0.000 lr: 0.000857\n",
      " step: 195, acc: 0.570, loss: 1.142,data_loss: 1.142, reg_loss: 0.000 lr: 0.000857\n",
      " step: 196, acc: 0.570, loss: 1.091,data_loss: 1.091, reg_loss: 0.000 lr: 0.000857\n",
      " step: 197, acc: 0.578, loss: 1.101,data_loss: 1.101, reg_loss: 0.000 lr: 0.000857\n",
      " step: 198, acc: 0.555, loss: 1.109,data_loss: 1.109, reg_loss: 0.000 lr: 0.000857\n",
      " step: 199, acc: 0.547, loss: 1.252,data_loss: 1.252, reg_loss: 0.000 lr: 0.000857\n",
      " step: 200, acc: 0.586, loss: 1.103,data_loss: 1.103, reg_loss: 0.000 lr: 0.000857\n",
      " step: 201, acc: 0.586, loss: 1.151,data_loss: 1.151, reg_loss: 0.000 lr: 0.000856\n",
      " step: 202, acc: 0.508, loss: 1.139,data_loss: 1.139, reg_loss: 0.000 lr: 0.000856\n",
      " step: 203, acc: 0.570, loss: 1.104,data_loss: 1.104, reg_loss: 0.000 lr: 0.000856\n",
      " step: 204, acc: 0.547, loss: 1.041,data_loss: 1.041, reg_loss: 0.000 lr: 0.000856\n",
      " step: 205, acc: 0.578, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000856\n",
      " step: 206, acc: 0.500, loss: 1.154,data_loss: 1.154, reg_loss: 0.000 lr: 0.000856\n",
      " step: 207, acc: 0.562, loss: 1.222,data_loss: 1.222, reg_loss: 0.000 lr: 0.000856\n",
      " step: 208, acc: 0.562, loss: 1.062,data_loss: 1.062, reg_loss: 0.000 lr: 0.000856\n",
      " step: 209, acc: 0.547, loss: 1.128,data_loss: 1.128, reg_loss: 0.000 lr: 0.000856\n",
      " step: 210, acc: 0.570, loss: 1.153,data_loss: 1.153, reg_loss: 0.000 lr: 0.000856\n",
      " step: 211, acc: 0.633, loss: 1.046,data_loss: 1.046, reg_loss: 0.000 lr: 0.000856\n",
      " step: 212, acc: 0.578, loss: 1.094,data_loss: 1.094, reg_loss: 0.000 lr: 0.000856\n",
      " step: 213, acc: 0.500, loss: 1.225,data_loss: 1.225, reg_loss: 0.000 lr: 0.000856\n",
      " step: 214, acc: 0.516, loss: 1.290,data_loss: 1.290, reg_loss: 0.000 lr: 0.000856\n",
      " step: 215, acc: 0.594, loss: 1.039,data_loss: 1.039, reg_loss: 0.000 lr: 0.000856\n",
      " step: 216, acc: 0.617, loss: 1.141,data_loss: 1.141, reg_loss: 0.000 lr: 0.000856\n",
      " step: 217, acc: 0.570, loss: 1.166,data_loss: 1.166, reg_loss: 0.000 lr: 0.000856\n",
      " step: 218, acc: 0.609, loss: 1.051,data_loss: 1.051, reg_loss: 0.000 lr: 0.000856\n",
      " step: 219, acc: 0.555, loss: 1.100,data_loss: 1.100, reg_loss: 0.000 lr: 0.000856\n",
      " step: 220, acc: 0.516, loss: 1.202,data_loss: 1.202, reg_loss: 0.000 lr: 0.000856\n",
      " step: 221, acc: 0.516, loss: 1.136,data_loss: 1.136, reg_loss: 0.000 lr: 0.000856\n",
      " step: 222, acc: 0.539, loss: 1.187,data_loss: 1.187, reg_loss: 0.000 lr: 0.000856\n",
      " step: 223, acc: 0.586, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000856\n",
      " step: 224, acc: 0.757, loss: 0.814,data_loss: 0.814, reg_loss: 0.000 lr: 0.000856\n",
      "training , acc: 0.554, loss: 1.150,data_loss: 1.150, reg_loss: 0.000 lr: 0.000856\n",
      "validation, acc:0.443 ,loss: 1.518 \n",
      "epoch: 16\n",
      " step: 0, acc: 0.469, loss: 1.250,data_loss: 1.250, reg_loss: 0.000 lr: 0.000856\n",
      " step: 1, acc: 0.508, loss: 1.172,data_loss: 1.172, reg_loss: 0.000 lr: 0.000856\n",
      " step: 2, acc: 0.539, loss: 1.142,data_loss: 1.142, reg_loss: 0.000 lr: 0.000856\n",
      " step: 3, acc: 0.602, loss: 1.046,data_loss: 1.046, reg_loss: 0.000 lr: 0.000856\n",
      " step: 4, acc: 0.609, loss: 1.019,data_loss: 1.019, reg_loss: 0.000 lr: 0.000855\n",
      " step: 5, acc: 0.555, loss: 1.139,data_loss: 1.139, reg_loss: 0.000 lr: 0.000855\n",
      " step: 6, acc: 0.586, loss: 1.090,data_loss: 1.090, reg_loss: 0.000 lr: 0.000855\n",
      " step: 7, acc: 0.477, loss: 1.249,data_loss: 1.249, reg_loss: 0.000 lr: 0.000855\n",
      " step: 8, acc: 0.633, loss: 1.101,data_loss: 1.101, reg_loss: 0.000 lr: 0.000855\n",
      " step: 9, acc: 0.586, loss: 1.079,data_loss: 1.079, reg_loss: 0.000 lr: 0.000855\n",
      " step: 10, acc: 0.672, loss: 0.968,data_loss: 0.968, reg_loss: 0.000 lr: 0.000855\n",
      " step: 11, acc: 0.586, loss: 1.097,data_loss: 1.097, reg_loss: 0.000 lr: 0.000855\n",
      " step: 12, acc: 0.570, loss: 1.061,data_loss: 1.061, reg_loss: 0.000 lr: 0.000855\n",
      " step: 13, acc: 0.555, loss: 1.121,data_loss: 1.121, reg_loss: 0.000 lr: 0.000855\n",
      " step: 14, acc: 0.570, loss: 1.032,data_loss: 1.032, reg_loss: 0.000 lr: 0.000855\n",
      " step: 15, acc: 0.586, loss: 1.115,data_loss: 1.115, reg_loss: 0.000 lr: 0.000855\n",
      " step: 16, acc: 0.578, loss: 1.098,data_loss: 1.098, reg_loss: 0.000 lr: 0.000855\n",
      " step: 17, acc: 0.539, loss: 1.159,data_loss: 1.159, reg_loss: 0.000 lr: 0.000855\n",
      " step: 18, acc: 0.500, loss: 1.191,data_loss: 1.191, reg_loss: 0.000 lr: 0.000855\n",
      " step: 19, acc: 0.625, loss: 0.911,data_loss: 0.911, reg_loss: 0.000 lr: 0.000855\n",
      " step: 20, acc: 0.570, loss: 1.117,data_loss: 1.117, reg_loss: 0.000 lr: 0.000855\n",
      " step: 21, acc: 0.641, loss: 0.969,data_loss: 0.969, reg_loss: 0.000 lr: 0.000855\n",
      " step: 22, acc: 0.570, loss: 1.091,data_loss: 1.091, reg_loss: 0.000 lr: 0.000855\n",
      " step: 23, acc: 0.578, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000855\n",
      " step: 24, acc: 0.523, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000855\n",
      " step: 25, acc: 0.609, loss: 1.029,data_loss: 1.029, reg_loss: 0.000 lr: 0.000855\n",
      " step: 26, acc: 0.594, loss: 1.067,data_loss: 1.067, reg_loss: 0.000 lr: 0.000855\n",
      " step: 27, acc: 0.500, loss: 1.183,data_loss: 1.183, reg_loss: 0.000 lr: 0.000855\n",
      " step: 28, acc: 0.617, loss: 1.014,data_loss: 1.014, reg_loss: 0.000 lr: 0.000855\n",
      " step: 29, acc: 0.523, loss: 1.205,data_loss: 1.205, reg_loss: 0.000 lr: 0.000855\n",
      " step: 30, acc: 0.547, loss: 1.267,data_loss: 1.267, reg_loss: 0.000 lr: 0.000855\n",
      " step: 31, acc: 0.656, loss: 0.946,data_loss: 0.946, reg_loss: 0.000 lr: 0.000854\n",
      " step: 32, acc: 0.609, loss: 1.045,data_loss: 1.045, reg_loss: 0.000 lr: 0.000854\n",
      " step: 33, acc: 0.602, loss: 1.098,data_loss: 1.098, reg_loss: 0.000 lr: 0.000854\n",
      " step: 34, acc: 0.617, loss: 1.084,data_loss: 1.084, reg_loss: 0.000 lr: 0.000854\n",
      " step: 35, acc: 0.531, loss: 1.137,data_loss: 1.137, reg_loss: 0.000 lr: 0.000854\n",
      " step: 36, acc: 0.531, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000854\n",
      " step: 37, acc: 0.570, loss: 1.137,data_loss: 1.137, reg_loss: 0.000 lr: 0.000854\n",
      " step: 38, acc: 0.633, loss: 1.062,data_loss: 1.062, reg_loss: 0.000 lr: 0.000854\n",
      " step: 39, acc: 0.516, loss: 1.231,data_loss: 1.231, reg_loss: 0.000 lr: 0.000854\n",
      " step: 40, acc: 0.617, loss: 1.141,data_loss: 1.141, reg_loss: 0.000 lr: 0.000854\n",
      " step: 41, acc: 0.523, loss: 1.159,data_loss: 1.159, reg_loss: 0.000 lr: 0.000854\n",
      " step: 42, acc: 0.617, loss: 1.122,data_loss: 1.122, reg_loss: 0.000 lr: 0.000854\n",
      " step: 43, acc: 0.500, loss: 1.159,data_loss: 1.159, reg_loss: 0.000 lr: 0.000854\n",
      " step: 44, acc: 0.602, loss: 1.123,data_loss: 1.123, reg_loss: 0.000 lr: 0.000854\n",
      " step: 45, acc: 0.508, loss: 1.195,data_loss: 1.195, reg_loss: 0.000 lr: 0.000854\n",
      " step: 46, acc: 0.562, loss: 1.133,data_loss: 1.133, reg_loss: 0.000 lr: 0.000854\n",
      " step: 47, acc: 0.617, loss: 1.081,data_loss: 1.081, reg_loss: 0.000 lr: 0.000854\n",
      " step: 48, acc: 0.617, loss: 1.069,data_loss: 1.069, reg_loss: 0.000 lr: 0.000854\n",
      " step: 49, acc: 0.617, loss: 1.117,data_loss: 1.117, reg_loss: 0.000 lr: 0.000854\n",
      " step: 50, acc: 0.656, loss: 1.046,data_loss: 1.046, reg_loss: 0.000 lr: 0.000854\n",
      " step: 51, acc: 0.602, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000854\n",
      " step: 52, acc: 0.508, loss: 1.159,data_loss: 1.159, reg_loss: 0.000 lr: 0.000854\n",
      " step: 53, acc: 0.594, loss: 1.134,data_loss: 1.134, reg_loss: 0.000 lr: 0.000854\n",
      " step: 54, acc: 0.477, loss: 1.309,data_loss: 1.309, reg_loss: 0.000 lr: 0.000854\n",
      " step: 55, acc: 0.570, loss: 1.076,data_loss: 1.076, reg_loss: 0.000 lr: 0.000854\n",
      " step: 56, acc: 0.555, loss: 1.109,data_loss: 1.109, reg_loss: 0.000 lr: 0.000854\n",
      " step: 57, acc: 0.516, loss: 1.183,data_loss: 1.183, reg_loss: 0.000 lr: 0.000854\n",
      " step: 58, acc: 0.594, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000853\n",
      " step: 59, acc: 0.578, loss: 1.110,data_loss: 1.110, reg_loss: 0.000 lr: 0.000853\n",
      " step: 60, acc: 0.617, loss: 1.088,data_loss: 1.088, reg_loss: 0.000 lr: 0.000853\n",
      " step: 61, acc: 0.586, loss: 1.065,data_loss: 1.065, reg_loss: 0.000 lr: 0.000853\n",
      " step: 62, acc: 0.562, loss: 1.196,data_loss: 1.196, reg_loss: 0.000 lr: 0.000853\n",
      " step: 63, acc: 0.570, loss: 1.062,data_loss: 1.062, reg_loss: 0.000 lr: 0.000853\n",
      " step: 64, acc: 0.555, loss: 1.181,data_loss: 1.181, reg_loss: 0.000 lr: 0.000853\n",
      " step: 65, acc: 0.500, loss: 1.261,data_loss: 1.261, reg_loss: 0.000 lr: 0.000853\n",
      " step: 66, acc: 0.641, loss: 1.012,data_loss: 1.012, reg_loss: 0.000 lr: 0.000853\n",
      " step: 67, acc: 0.562, loss: 1.067,data_loss: 1.067, reg_loss: 0.000 lr: 0.000853\n",
      " step: 68, acc: 0.500, loss: 1.103,data_loss: 1.103, reg_loss: 0.000 lr: 0.000853\n",
      " step: 69, acc: 0.500, loss: 1.209,data_loss: 1.209, reg_loss: 0.000 lr: 0.000853\n",
      " step: 70, acc: 0.617, loss: 1.056,data_loss: 1.056, reg_loss: 0.000 lr: 0.000853\n",
      " step: 71, acc: 0.672, loss: 0.974,data_loss: 0.974, reg_loss: 0.000 lr: 0.000853\n",
      " step: 72, acc: 0.555, loss: 1.051,data_loss: 1.051, reg_loss: 0.000 lr: 0.000853\n",
      " step: 73, acc: 0.531, loss: 1.159,data_loss: 1.159, reg_loss: 0.000 lr: 0.000853\n",
      " step: 74, acc: 0.539, loss: 1.181,data_loss: 1.181, reg_loss: 0.000 lr: 0.000853\n",
      " step: 75, acc: 0.523, loss: 1.180,data_loss: 1.180, reg_loss: 0.000 lr: 0.000853\n",
      " step: 76, acc: 0.609, loss: 1.021,data_loss: 1.021, reg_loss: 0.000 lr: 0.000853\n",
      " step: 77, acc: 0.602, loss: 1.123,data_loss: 1.123, reg_loss: 0.000 lr: 0.000853\n",
      " step: 78, acc: 0.648, loss: 0.961,data_loss: 0.961, reg_loss: 0.000 lr: 0.000853\n",
      " step: 79, acc: 0.523, loss: 1.154,data_loss: 1.154, reg_loss: 0.000 lr: 0.000853\n",
      " step: 80, acc: 0.594, loss: 1.142,data_loss: 1.142, reg_loss: 0.000 lr: 0.000853\n",
      " step: 81, acc: 0.594, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000853\n",
      " step: 82, acc: 0.602, loss: 1.110,data_loss: 1.110, reg_loss: 0.000 lr: 0.000853\n",
      " step: 83, acc: 0.578, loss: 1.062,data_loss: 1.062, reg_loss: 0.000 lr: 0.000853\n",
      " step: 84, acc: 0.523, loss: 1.189,data_loss: 1.189, reg_loss: 0.000 lr: 0.000853\n",
      " step: 85, acc: 0.562, loss: 1.122,data_loss: 1.122, reg_loss: 0.000 lr: 0.000853\n",
      " step: 86, acc: 0.547, loss: 1.053,data_loss: 1.053, reg_loss: 0.000 lr: 0.000852\n",
      " step: 87, acc: 0.547, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000852\n",
      " step: 88, acc: 0.523, loss: 1.261,data_loss: 1.261, reg_loss: 0.000 lr: 0.000852\n",
      " step: 89, acc: 0.539, loss: 1.113,data_loss: 1.113, reg_loss: 0.000 lr: 0.000852\n",
      " step: 90, acc: 0.555, loss: 1.176,data_loss: 1.176, reg_loss: 0.000 lr: 0.000852\n",
      " step: 91, acc: 0.508, loss: 1.311,data_loss: 1.311, reg_loss: 0.000 lr: 0.000852\n",
      " step: 92, acc: 0.578, loss: 1.055,data_loss: 1.055, reg_loss: 0.000 lr: 0.000852\n",
      " step: 93, acc: 0.625, loss: 1.065,data_loss: 1.065, reg_loss: 0.000 lr: 0.000852\n",
      " step: 94, acc: 0.508, loss: 1.298,data_loss: 1.298, reg_loss: 0.000 lr: 0.000852\n",
      " step: 95, acc: 0.594, loss: 1.047,data_loss: 1.047, reg_loss: 0.000 lr: 0.000852\n",
      " step: 96, acc: 0.492, loss: 1.327,data_loss: 1.327, reg_loss: 0.000 lr: 0.000852\n",
      " step: 97, acc: 0.570, loss: 1.109,data_loss: 1.109, reg_loss: 0.000 lr: 0.000852\n",
      " step: 98, acc: 0.586, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000852\n",
      " step: 99, acc: 0.602, loss: 1.022,data_loss: 1.022, reg_loss: 0.000 lr: 0.000852\n",
      " step: 100, acc: 0.562, loss: 1.140,data_loss: 1.140, reg_loss: 0.000 lr: 0.000852\n",
      " step: 101, acc: 0.562, loss: 1.091,data_loss: 1.091, reg_loss: 0.000 lr: 0.000852\n",
      " step: 102, acc: 0.578, loss: 1.205,data_loss: 1.205, reg_loss: 0.000 lr: 0.000852\n",
      " step: 103, acc: 0.602, loss: 1.161,data_loss: 1.161, reg_loss: 0.000 lr: 0.000852\n",
      " step: 104, acc: 0.570, loss: 1.076,data_loss: 1.076, reg_loss: 0.000 lr: 0.000852\n",
      " step: 105, acc: 0.602, loss: 1.055,data_loss: 1.055, reg_loss: 0.000 lr: 0.000852\n",
      " step: 106, acc: 0.539, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000852\n",
      " step: 107, acc: 0.555, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000852\n",
      " step: 108, acc: 0.562, loss: 1.134,data_loss: 1.134, reg_loss: 0.000 lr: 0.000852\n",
      " step: 109, acc: 0.508, loss: 1.138,data_loss: 1.138, reg_loss: 0.000 lr: 0.000852\n",
      " step: 110, acc: 0.562, loss: 1.176,data_loss: 1.176, reg_loss: 0.000 lr: 0.000852\n",
      " step: 111, acc: 0.562, loss: 1.207,data_loss: 1.207, reg_loss: 0.000 lr: 0.000852\n",
      " step: 112, acc: 0.562, loss: 1.138,data_loss: 1.138, reg_loss: 0.000 lr: 0.000852\n",
      " step: 113, acc: 0.586, loss: 1.119,data_loss: 1.119, reg_loss: 0.000 lr: 0.000851\n",
      " step: 114, acc: 0.516, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000851\n",
      " step: 115, acc: 0.562, loss: 1.148,data_loss: 1.148, reg_loss: 0.000 lr: 0.000851\n",
      " step: 116, acc: 0.570, loss: 1.130,data_loss: 1.130, reg_loss: 0.000 lr: 0.000851\n",
      " step: 117, acc: 0.508, loss: 1.145,data_loss: 1.145, reg_loss: 0.000 lr: 0.000851\n",
      " step: 118, acc: 0.562, loss: 1.142,data_loss: 1.142, reg_loss: 0.000 lr: 0.000851\n",
      " step: 119, acc: 0.648, loss: 0.985,data_loss: 0.985, reg_loss: 0.000 lr: 0.000851\n",
      " step: 120, acc: 0.633, loss: 1.082,data_loss: 1.082, reg_loss: 0.000 lr: 0.000851\n",
      " step: 121, acc: 0.492, loss: 1.225,data_loss: 1.225, reg_loss: 0.000 lr: 0.000851\n",
      " step: 122, acc: 0.508, loss: 1.171,data_loss: 1.171, reg_loss: 0.000 lr: 0.000851\n",
      " step: 123, acc: 0.523, loss: 1.158,data_loss: 1.158, reg_loss: 0.000 lr: 0.000851\n",
      " step: 124, acc: 0.586, loss: 1.123,data_loss: 1.123, reg_loss: 0.000 lr: 0.000851\n",
      " step: 125, acc: 0.602, loss: 1.079,data_loss: 1.079, reg_loss: 0.000 lr: 0.000851\n",
      " step: 126, acc: 0.469, loss: 1.257,data_loss: 1.257, reg_loss: 0.000 lr: 0.000851\n",
      " step: 127, acc: 0.477, loss: 1.252,data_loss: 1.252, reg_loss: 0.000 lr: 0.000851\n",
      " step: 128, acc: 0.531, loss: 1.177,data_loss: 1.177, reg_loss: 0.000 lr: 0.000851\n",
      " step: 129, acc: 0.594, loss: 1.155,data_loss: 1.155, reg_loss: 0.000 lr: 0.000851\n",
      " step: 130, acc: 0.500, loss: 1.283,data_loss: 1.283, reg_loss: 0.000 lr: 0.000851\n",
      " step: 131, acc: 0.547, loss: 1.142,data_loss: 1.142, reg_loss: 0.000 lr: 0.000851\n",
      " step: 132, acc: 0.492, loss: 1.107,data_loss: 1.107, reg_loss: 0.000 lr: 0.000851\n",
      " step: 133, acc: 0.625, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000851\n",
      " step: 134, acc: 0.656, loss: 0.984,data_loss: 0.984, reg_loss: 0.000 lr: 0.000851\n",
      " step: 135, acc: 0.555, loss: 1.140,data_loss: 1.140, reg_loss: 0.000 lr: 0.000851\n",
      " step: 136, acc: 0.477, loss: 1.230,data_loss: 1.230, reg_loss: 0.000 lr: 0.000851\n",
      " step: 137, acc: 0.625, loss: 1.043,data_loss: 1.043, reg_loss: 0.000 lr: 0.000851\n",
      " step: 138, acc: 0.562, loss: 1.167,data_loss: 1.167, reg_loss: 0.000 lr: 0.000851\n",
      " step: 139, acc: 0.570, loss: 1.059,data_loss: 1.059, reg_loss: 0.000 lr: 0.000851\n",
      " step: 140, acc: 0.555, loss: 1.108,data_loss: 1.108, reg_loss: 0.000 lr: 0.000851\n",
      " step: 141, acc: 0.539, loss: 1.229,data_loss: 1.229, reg_loss: 0.000 lr: 0.000850\n",
      " step: 142, acc: 0.617, loss: 1.114,data_loss: 1.114, reg_loss: 0.000 lr: 0.000850\n",
      " step: 143, acc: 0.531, loss: 1.166,data_loss: 1.166, reg_loss: 0.000 lr: 0.000850\n",
      " step: 144, acc: 0.531, loss: 1.245,data_loss: 1.245, reg_loss: 0.000 lr: 0.000850\n",
      " step: 145, acc: 0.578, loss: 1.149,data_loss: 1.149, reg_loss: 0.000 lr: 0.000850\n",
      " step: 146, acc: 0.531, loss: 1.134,data_loss: 1.134, reg_loss: 0.000 lr: 0.000850\n",
      " step: 147, acc: 0.578, loss: 1.096,data_loss: 1.096, reg_loss: 0.000 lr: 0.000850\n",
      " step: 148, acc: 0.602, loss: 1.037,data_loss: 1.037, reg_loss: 0.000 lr: 0.000850\n",
      " step: 149, acc: 0.586, loss: 1.133,data_loss: 1.133, reg_loss: 0.000 lr: 0.000850\n",
      " step: 150, acc: 0.602, loss: 1.117,data_loss: 1.117, reg_loss: 0.000 lr: 0.000850\n",
      " step: 151, acc: 0.461, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000850\n",
      " step: 152, acc: 0.633, loss: 0.968,data_loss: 0.968, reg_loss: 0.000 lr: 0.000850\n",
      " step: 153, acc: 0.609, loss: 1.047,data_loss: 1.047, reg_loss: 0.000 lr: 0.000850\n",
      " step: 154, acc: 0.602, loss: 1.108,data_loss: 1.108, reg_loss: 0.000 lr: 0.000850\n",
      " step: 155, acc: 0.578, loss: 1.126,data_loss: 1.126, reg_loss: 0.000 lr: 0.000850\n",
      " step: 156, acc: 0.531, loss: 1.185,data_loss: 1.185, reg_loss: 0.000 lr: 0.000850\n",
      " step: 157, acc: 0.641, loss: 0.920,data_loss: 0.920, reg_loss: 0.000 lr: 0.000850\n",
      " step: 158, acc: 0.570, loss: 1.197,data_loss: 1.197, reg_loss: 0.000 lr: 0.000850\n",
      " step: 159, acc: 0.523, loss: 1.233,data_loss: 1.233, reg_loss: 0.000 lr: 0.000850\n",
      " step: 160, acc: 0.492, loss: 1.264,data_loss: 1.264, reg_loss: 0.000 lr: 0.000850\n",
      " step: 161, acc: 0.516, loss: 1.188,data_loss: 1.188, reg_loss: 0.000 lr: 0.000850\n",
      " step: 162, acc: 0.570, loss: 1.167,data_loss: 1.167, reg_loss: 0.000 lr: 0.000850\n",
      " step: 163, acc: 0.523, loss: 1.045,data_loss: 1.045, reg_loss: 0.000 lr: 0.000850\n",
      " step: 164, acc: 0.570, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000850\n",
      " step: 165, acc: 0.477, loss: 1.283,data_loss: 1.283, reg_loss: 0.000 lr: 0.000850\n",
      " step: 166, acc: 0.555, loss: 1.186,data_loss: 1.186, reg_loss: 0.000 lr: 0.000850\n",
      " step: 167, acc: 0.477, loss: 1.281,data_loss: 1.281, reg_loss: 0.000 lr: 0.000850\n",
      " step: 168, acc: 0.531, loss: 1.185,data_loss: 1.185, reg_loss: 0.000 lr: 0.000850\n",
      " step: 169, acc: 0.539, loss: 1.099,data_loss: 1.099, reg_loss: 0.000 lr: 0.000849\n",
      " step: 170, acc: 0.516, loss: 1.224,data_loss: 1.224, reg_loss: 0.000 lr: 0.000849\n",
      " step: 171, acc: 0.594, loss: 1.098,data_loss: 1.098, reg_loss: 0.000 lr: 0.000849\n",
      " step: 172, acc: 0.562, loss: 1.176,data_loss: 1.176, reg_loss: 0.000 lr: 0.000849\n",
      " step: 173, acc: 0.539, loss: 1.226,data_loss: 1.226, reg_loss: 0.000 lr: 0.000849\n",
      " step: 174, acc: 0.531, loss: 1.180,data_loss: 1.180, reg_loss: 0.000 lr: 0.000849\n",
      " step: 175, acc: 0.523, loss: 1.157,data_loss: 1.157, reg_loss: 0.000 lr: 0.000849\n",
      " step: 176, acc: 0.500, loss: 1.189,data_loss: 1.189, reg_loss: 0.000 lr: 0.000849\n",
      " step: 177, acc: 0.617, loss: 1.093,data_loss: 1.093, reg_loss: 0.000 lr: 0.000849\n",
      " step: 178, acc: 0.602, loss: 1.251,data_loss: 1.251, reg_loss: 0.000 lr: 0.000849\n",
      " step: 179, acc: 0.633, loss: 1.012,data_loss: 1.012, reg_loss: 0.000 lr: 0.000849\n",
      " step: 180, acc: 0.625, loss: 0.925,data_loss: 0.925, reg_loss: 0.000 lr: 0.000849\n",
      " step: 181, acc: 0.578, loss: 1.278,data_loss: 1.278, reg_loss: 0.000 lr: 0.000849\n",
      " step: 182, acc: 0.531, loss: 1.192,data_loss: 1.192, reg_loss: 0.000 lr: 0.000849\n",
      " step: 183, acc: 0.562, loss: 1.103,data_loss: 1.103, reg_loss: 0.000 lr: 0.000849\n",
      " step: 184, acc: 0.609, loss: 1.172,data_loss: 1.172, reg_loss: 0.000 lr: 0.000849\n",
      " step: 185, acc: 0.531, loss: 1.139,data_loss: 1.139, reg_loss: 0.000 lr: 0.000849\n",
      " step: 186, acc: 0.609, loss: 1.055,data_loss: 1.055, reg_loss: 0.000 lr: 0.000849\n",
      " step: 187, acc: 0.609, loss: 0.986,data_loss: 0.986, reg_loss: 0.000 lr: 0.000849\n",
      " step: 188, acc: 0.516, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000849\n",
      " step: 189, acc: 0.586, loss: 1.106,data_loss: 1.106, reg_loss: 0.000 lr: 0.000849\n",
      " step: 190, acc: 0.508, loss: 1.259,data_loss: 1.259, reg_loss: 0.000 lr: 0.000849\n",
      " step: 191, acc: 0.516, loss: 1.310,data_loss: 1.310, reg_loss: 0.000 lr: 0.000849\n",
      " step: 192, acc: 0.648, loss: 1.065,data_loss: 1.065, reg_loss: 0.000 lr: 0.000849\n",
      " step: 193, acc: 0.633, loss: 1.093,data_loss: 1.093, reg_loss: 0.000 lr: 0.000849\n",
      " step: 194, acc: 0.562, loss: 1.056,data_loss: 1.056, reg_loss: 0.000 lr: 0.000849\n",
      " step: 195, acc: 0.586, loss: 1.052,data_loss: 1.052, reg_loss: 0.000 lr: 0.000849\n",
      " step: 196, acc: 0.602, loss: 1.162,data_loss: 1.162, reg_loss: 0.000 lr: 0.000849\n",
      " step: 197, acc: 0.594, loss: 1.073,data_loss: 1.073, reg_loss: 0.000 lr: 0.000848\n",
      " step: 198, acc: 0.617, loss: 1.059,data_loss: 1.059, reg_loss: 0.000 lr: 0.000848\n",
      " step: 199, acc: 0.492, loss: 1.305,data_loss: 1.305, reg_loss: 0.000 lr: 0.000848\n",
      " step: 200, acc: 0.570, loss: 1.074,data_loss: 1.074, reg_loss: 0.000 lr: 0.000848\n",
      " step: 201, acc: 0.578, loss: 1.141,data_loss: 1.141, reg_loss: 0.000 lr: 0.000848\n",
      " step: 202, acc: 0.547, loss: 1.178,data_loss: 1.178, reg_loss: 0.000 lr: 0.000848\n",
      " step: 203, acc: 0.555, loss: 1.110,data_loss: 1.110, reg_loss: 0.000 lr: 0.000848\n",
      " step: 204, acc: 0.578, loss: 1.081,data_loss: 1.081, reg_loss: 0.000 lr: 0.000848\n",
      " step: 205, acc: 0.562, loss: 1.166,data_loss: 1.166, reg_loss: 0.000 lr: 0.000848\n",
      " step: 206, acc: 0.531, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000848\n",
      " step: 207, acc: 0.531, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000848\n",
      " step: 208, acc: 0.602, loss: 1.043,data_loss: 1.043, reg_loss: 0.000 lr: 0.000848\n",
      " step: 209, acc: 0.523, loss: 1.095,data_loss: 1.095, reg_loss: 0.000 lr: 0.000848\n",
      " step: 210, acc: 0.617, loss: 1.019,data_loss: 1.019, reg_loss: 0.000 lr: 0.000848\n",
      " step: 211, acc: 0.562, loss: 1.128,data_loss: 1.128, reg_loss: 0.000 lr: 0.000848\n",
      " step: 212, acc: 0.617, loss: 1.064,data_loss: 1.064, reg_loss: 0.000 lr: 0.000848\n",
      " step: 213, acc: 0.578, loss: 1.110,data_loss: 1.110, reg_loss: 0.000 lr: 0.000848\n",
      " step: 214, acc: 0.531, loss: 1.198,data_loss: 1.198, reg_loss: 0.000 lr: 0.000848\n",
      " step: 215, acc: 0.672, loss: 0.950,data_loss: 0.950, reg_loss: 0.000 lr: 0.000848\n",
      " step: 216, acc: 0.531, loss: 1.136,data_loss: 1.136, reg_loss: 0.000 lr: 0.000848\n",
      " step: 217, acc: 0.508, loss: 1.246,data_loss: 1.246, reg_loss: 0.000 lr: 0.000848\n",
      " step: 218, acc: 0.625, loss: 1.100,data_loss: 1.100, reg_loss: 0.000 lr: 0.000848\n",
      " step: 219, acc: 0.570, loss: 1.127,data_loss: 1.127, reg_loss: 0.000 lr: 0.000848\n",
      " step: 220, acc: 0.555, loss: 1.094,data_loss: 1.094, reg_loss: 0.000 lr: 0.000848\n",
      " step: 221, acc: 0.562, loss: 1.104,data_loss: 1.104, reg_loss: 0.000 lr: 0.000848\n",
      " step: 222, acc: 0.586, loss: 1.070,data_loss: 1.070, reg_loss: 0.000 lr: 0.000848\n",
      " step: 223, acc: 0.539, loss: 1.164,data_loss: 1.164, reg_loss: 0.000 lr: 0.000848\n",
      " step: 224, acc: 0.730, loss: 0.816,data_loss: 0.816, reg_loss: 0.000 lr: 0.000847\n",
      "training , acc: 0.566, loss: 1.127,data_loss: 1.127, reg_loss: 0.000 lr: 0.000847\n",
      "validation, acc:0.453 ,loss: 1.517 \n",
      "epoch: 17\n",
      " step: 0, acc: 0.469, loss: 1.251,data_loss: 1.251, reg_loss: 0.000 lr: 0.000847\n",
      " step: 1, acc: 0.570, loss: 1.109,data_loss: 1.109, reg_loss: 0.000 lr: 0.000847\n",
      " step: 2, acc: 0.594, loss: 1.115,data_loss: 1.115, reg_loss: 0.000 lr: 0.000847\n",
      " step: 3, acc: 0.539, loss: 1.137,data_loss: 1.137, reg_loss: 0.000 lr: 0.000847\n",
      " step: 4, acc: 0.609, loss: 1.042,data_loss: 1.042, reg_loss: 0.000 lr: 0.000847\n",
      " step: 5, acc: 0.547, loss: 1.122,data_loss: 1.122, reg_loss: 0.000 lr: 0.000847\n",
      " step: 6, acc: 0.594, loss: 1.040,data_loss: 1.040, reg_loss: 0.000 lr: 0.000847\n",
      " step: 7, acc: 0.492, loss: 1.262,data_loss: 1.262, reg_loss: 0.000 lr: 0.000847\n",
      " step: 8, acc: 0.578, loss: 1.095,data_loss: 1.095, reg_loss: 0.000 lr: 0.000847\n",
      " step: 9, acc: 0.688, loss: 0.923,data_loss: 0.923, reg_loss: 0.000 lr: 0.000847\n",
      " step: 10, acc: 0.641, loss: 1.013,data_loss: 1.013, reg_loss: 0.000 lr: 0.000847\n",
      " step: 11, acc: 0.641, loss: 1.020,data_loss: 1.020, reg_loss: 0.000 lr: 0.000847\n",
      " step: 12, acc: 0.625, loss: 1.025,data_loss: 1.025, reg_loss: 0.000 lr: 0.000847\n",
      " step: 13, acc: 0.570, loss: 1.102,data_loss: 1.102, reg_loss: 0.000 lr: 0.000847\n",
      " step: 14, acc: 0.539, loss: 1.133,data_loss: 1.133, reg_loss: 0.000 lr: 0.000847\n",
      " step: 15, acc: 0.656, loss: 0.992,data_loss: 0.992, reg_loss: 0.000 lr: 0.000847\n",
      " step: 16, acc: 0.602, loss: 1.055,data_loss: 1.055, reg_loss: 0.000 lr: 0.000847\n",
      " step: 17, acc: 0.531, loss: 1.218,data_loss: 1.218, reg_loss: 0.000 lr: 0.000847\n",
      " step: 18, acc: 0.539, loss: 1.223,data_loss: 1.223, reg_loss: 0.000 lr: 0.000847\n",
      " step: 19, acc: 0.625, loss: 0.949,data_loss: 0.949, reg_loss: 0.000 lr: 0.000847\n",
      " step: 20, acc: 0.633, loss: 1.060,data_loss: 1.060, reg_loss: 0.000 lr: 0.000847\n",
      " step: 21, acc: 0.570, loss: 1.082,data_loss: 1.082, reg_loss: 0.000 lr: 0.000847\n",
      " step: 22, acc: 0.594, loss: 1.111,data_loss: 1.111, reg_loss: 0.000 lr: 0.000847\n",
      " step: 23, acc: 0.562, loss: 1.109,data_loss: 1.109, reg_loss: 0.000 lr: 0.000847\n",
      " step: 24, acc: 0.594, loss: 1.054,data_loss: 1.054, reg_loss: 0.000 lr: 0.000847\n",
      " step: 25, acc: 0.586, loss: 1.052,data_loss: 1.052, reg_loss: 0.000 lr: 0.000847\n",
      " step: 26, acc: 0.578, loss: 1.104,data_loss: 1.104, reg_loss: 0.000 lr: 0.000847\n",
      " step: 27, acc: 0.609, loss: 1.017,data_loss: 1.017, reg_loss: 0.000 lr: 0.000846\n",
      " step: 28, acc: 0.562, loss: 1.005,data_loss: 1.005, reg_loss: 0.000 lr: 0.000846\n",
      " step: 29, acc: 0.570, loss: 1.130,data_loss: 1.130, reg_loss: 0.000 lr: 0.000846\n",
      " step: 30, acc: 0.562, loss: 1.295,data_loss: 1.295, reg_loss: 0.000 lr: 0.000846\n",
      " step: 31, acc: 0.586, loss: 1.024,data_loss: 1.024, reg_loss: 0.000 lr: 0.000846\n",
      " step: 32, acc: 0.578, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000846\n",
      " step: 33, acc: 0.594, loss: 1.088,data_loss: 1.088, reg_loss: 0.000 lr: 0.000846\n",
      " step: 34, acc: 0.562, loss: 1.140,data_loss: 1.140, reg_loss: 0.000 lr: 0.000846\n",
      " step: 35, acc: 0.508, loss: 1.143,data_loss: 1.143, reg_loss: 0.000 lr: 0.000846\n",
      " step: 36, acc: 0.625, loss: 0.991,data_loss: 0.991, reg_loss: 0.000 lr: 0.000846\n",
      " step: 37, acc: 0.602, loss: 1.076,data_loss: 1.076, reg_loss: 0.000 lr: 0.000846\n",
      " step: 38, acc: 0.578, loss: 1.121,data_loss: 1.121, reg_loss: 0.000 lr: 0.000846\n",
      " step: 39, acc: 0.508, loss: 1.237,data_loss: 1.237, reg_loss: 0.000 lr: 0.000846\n",
      " step: 40, acc: 0.578, loss: 1.131,data_loss: 1.131, reg_loss: 0.000 lr: 0.000846\n",
      " step: 41, acc: 0.562, loss: 1.141,data_loss: 1.141, reg_loss: 0.000 lr: 0.000846\n",
      " step: 42, acc: 0.531, loss: 1.113,data_loss: 1.113, reg_loss: 0.000 lr: 0.000846\n",
      " step: 43, acc: 0.602, loss: 1.123,data_loss: 1.123, reg_loss: 0.000 lr: 0.000846\n",
      " step: 44, acc: 0.617, loss: 1.072,data_loss: 1.072, reg_loss: 0.000 lr: 0.000846\n",
      " step: 45, acc: 0.570, loss: 1.198,data_loss: 1.198, reg_loss: 0.000 lr: 0.000846\n",
      " step: 46, acc: 0.609, loss: 1.069,data_loss: 1.069, reg_loss: 0.000 lr: 0.000846\n",
      " step: 47, acc: 0.578, loss: 1.051,data_loss: 1.051, reg_loss: 0.000 lr: 0.000846\n",
      " step: 48, acc: 0.578, loss: 1.054,data_loss: 1.054, reg_loss: 0.000 lr: 0.000846\n",
      " step: 49, acc: 0.656, loss: 1.038,data_loss: 1.038, reg_loss: 0.000 lr: 0.000846\n",
      " step: 50, acc: 0.641, loss: 0.938,data_loss: 0.938, reg_loss: 0.000 lr: 0.000846\n",
      " step: 51, acc: 0.570, loss: 1.210,data_loss: 1.210, reg_loss: 0.000 lr: 0.000846\n",
      " step: 52, acc: 0.625, loss: 1.018,data_loss: 1.018, reg_loss: 0.000 lr: 0.000846\n",
      " step: 53, acc: 0.625, loss: 1.060,data_loss: 1.060, reg_loss: 0.000 lr: 0.000846\n",
      " step: 54, acc: 0.516, loss: 1.348,data_loss: 1.348, reg_loss: 0.000 lr: 0.000846\n",
      " step: 55, acc: 0.562, loss: 1.069,data_loss: 1.069, reg_loss: 0.000 lr: 0.000845\n",
      " step: 56, acc: 0.602, loss: 0.965,data_loss: 0.965, reg_loss: 0.000 lr: 0.000845\n",
      " step: 57, acc: 0.664, loss: 1.017,data_loss: 1.017, reg_loss: 0.000 lr: 0.000845\n",
      " step: 58, acc: 0.539, loss: 1.217,data_loss: 1.217, reg_loss: 0.000 lr: 0.000845\n",
      " step: 59, acc: 0.625, loss: 1.057,data_loss: 1.057, reg_loss: 0.000 lr: 0.000845\n",
      " step: 60, acc: 0.617, loss: 0.968,data_loss: 0.968, reg_loss: 0.000 lr: 0.000845\n",
      " step: 61, acc: 0.555, loss: 1.133,data_loss: 1.133, reg_loss: 0.000 lr: 0.000845\n",
      " step: 62, acc: 0.523, loss: 1.194,data_loss: 1.194, reg_loss: 0.000 lr: 0.000845\n",
      " step: 63, acc: 0.602, loss: 1.061,data_loss: 1.061, reg_loss: 0.000 lr: 0.000845\n",
      " step: 64, acc: 0.523, loss: 1.128,data_loss: 1.128, reg_loss: 0.000 lr: 0.000845\n",
      " step: 65, acc: 0.562, loss: 1.143,data_loss: 1.143, reg_loss: 0.000 lr: 0.000845\n",
      " step: 66, acc: 0.625, loss: 1.007,data_loss: 1.007, reg_loss: 0.000 lr: 0.000845\n",
      " step: 67, acc: 0.547, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000845\n",
      " step: 68, acc: 0.562, loss: 1.076,data_loss: 1.076, reg_loss: 0.000 lr: 0.000845\n",
      " step: 69, acc: 0.547, loss: 1.125,data_loss: 1.125, reg_loss: 0.000 lr: 0.000845\n",
      " step: 70, acc: 0.609, loss: 1.086,data_loss: 1.086, reg_loss: 0.000 lr: 0.000845\n",
      " step: 71, acc: 0.656, loss: 0.955,data_loss: 0.955, reg_loss: 0.000 lr: 0.000845\n",
      " step: 72, acc: 0.602, loss: 1.013,data_loss: 1.013, reg_loss: 0.000 lr: 0.000845\n",
      " step: 73, acc: 0.531, loss: 1.136,data_loss: 1.136, reg_loss: 0.000 lr: 0.000845\n",
      " step: 74, acc: 0.633, loss: 1.048,data_loss: 1.048, reg_loss: 0.000 lr: 0.000845\n",
      " step: 75, acc: 0.570, loss: 1.143,data_loss: 1.143, reg_loss: 0.000 lr: 0.000845\n",
      " step: 76, acc: 0.617, loss: 0.990,data_loss: 0.990, reg_loss: 0.000 lr: 0.000845\n",
      " step: 77, acc: 0.602, loss: 1.012,data_loss: 1.012, reg_loss: 0.000 lr: 0.000845\n",
      " step: 78, acc: 0.570, loss: 1.014,data_loss: 1.014, reg_loss: 0.000 lr: 0.000845\n",
      " step: 79, acc: 0.539, loss: 1.142,data_loss: 1.142, reg_loss: 0.000 lr: 0.000845\n",
      " step: 80, acc: 0.547, loss: 1.134,data_loss: 1.134, reg_loss: 0.000 lr: 0.000845\n",
      " step: 81, acc: 0.555, loss: 1.282,data_loss: 1.282, reg_loss: 0.000 lr: 0.000845\n",
      " step: 82, acc: 0.594, loss: 1.091,data_loss: 1.091, reg_loss: 0.000 lr: 0.000845\n",
      " step: 83, acc: 0.594, loss: 1.057,data_loss: 1.057, reg_loss: 0.000 lr: 0.000844\n",
      " step: 84, acc: 0.555, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000844\n",
      " step: 85, acc: 0.570, loss: 1.168,data_loss: 1.168, reg_loss: 0.000 lr: 0.000844\n",
      " step: 86, acc: 0.547, loss: 1.061,data_loss: 1.061, reg_loss: 0.000 lr: 0.000844\n",
      " step: 87, acc: 0.539, loss: 1.127,data_loss: 1.127, reg_loss: 0.000 lr: 0.000844\n",
      " step: 88, acc: 0.562, loss: 1.170,data_loss: 1.170, reg_loss: 0.000 lr: 0.000844\n",
      " step: 89, acc: 0.586, loss: 1.113,data_loss: 1.113, reg_loss: 0.000 lr: 0.000844\n",
      " step: 90, acc: 0.555, loss: 1.172,data_loss: 1.172, reg_loss: 0.000 lr: 0.000844\n",
      " step: 91, acc: 0.516, loss: 1.205,data_loss: 1.205, reg_loss: 0.000 lr: 0.000844\n",
      " step: 92, acc: 0.547, loss: 1.056,data_loss: 1.056, reg_loss: 0.000 lr: 0.000844\n",
      " step: 93, acc: 0.641, loss: 0.983,data_loss: 0.983, reg_loss: 0.000 lr: 0.000844\n",
      " step: 94, acc: 0.492, loss: 1.308,data_loss: 1.308, reg_loss: 0.000 lr: 0.000844\n",
      " step: 95, acc: 0.625, loss: 1.014,data_loss: 1.014, reg_loss: 0.000 lr: 0.000844\n",
      " step: 96, acc: 0.539, loss: 1.194,data_loss: 1.194, reg_loss: 0.000 lr: 0.000844\n",
      " step: 97, acc: 0.578, loss: 1.087,data_loss: 1.087, reg_loss: 0.000 lr: 0.000844\n",
      " step: 98, acc: 0.562, loss: 0.976,data_loss: 0.976, reg_loss: 0.000 lr: 0.000844\n",
      " step: 99, acc: 0.633, loss: 0.977,data_loss: 0.977, reg_loss: 0.000 lr: 0.000844\n",
      " step: 100, acc: 0.539, loss: 1.121,data_loss: 1.121, reg_loss: 0.000 lr: 0.000844\n",
      " step: 101, acc: 0.594, loss: 1.057,data_loss: 1.057, reg_loss: 0.000 lr: 0.000844\n",
      " step: 102, acc: 0.602, loss: 1.155,data_loss: 1.155, reg_loss: 0.000 lr: 0.000844\n",
      " step: 103, acc: 0.578, loss: 1.039,data_loss: 1.039, reg_loss: 0.000 lr: 0.000844\n",
      " step: 104, acc: 0.617, loss: 1.032,data_loss: 1.032, reg_loss: 0.000 lr: 0.000844\n",
      " step: 105, acc: 0.594, loss: 1.020,data_loss: 1.020, reg_loss: 0.000 lr: 0.000844\n",
      " step: 106, acc: 0.625, loss: 1.033,data_loss: 1.033, reg_loss: 0.000 lr: 0.000844\n",
      " step: 107, acc: 0.562, loss: 1.081,data_loss: 1.081, reg_loss: 0.000 lr: 0.000844\n",
      " step: 108, acc: 0.578, loss: 1.048,data_loss: 1.048, reg_loss: 0.000 lr: 0.000844\n",
      " step: 109, acc: 0.586, loss: 1.095,data_loss: 1.095, reg_loss: 0.000 lr: 0.000844\n",
      " step: 110, acc: 0.641, loss: 1.080,data_loss: 1.080, reg_loss: 0.000 lr: 0.000844\n",
      " step: 111, acc: 0.562, loss: 1.002,data_loss: 1.002, reg_loss: 0.000 lr: 0.000843\n",
      " step: 112, acc: 0.547, loss: 1.187,data_loss: 1.187, reg_loss: 0.000 lr: 0.000843\n",
      " step: 113, acc: 0.617, loss: 1.014,data_loss: 1.014, reg_loss: 0.000 lr: 0.000843\n",
      " step: 114, acc: 0.531, loss: 1.181,data_loss: 1.181, reg_loss: 0.000 lr: 0.000843\n",
      " step: 115, acc: 0.602, loss: 1.003,data_loss: 1.003, reg_loss: 0.000 lr: 0.000843\n",
      " step: 116, acc: 0.523, loss: 1.162,data_loss: 1.162, reg_loss: 0.000 lr: 0.000843\n",
      " step: 117, acc: 0.594, loss: 1.001,data_loss: 1.001, reg_loss: 0.000 lr: 0.000843\n",
      " step: 118, acc: 0.625, loss: 1.001,data_loss: 1.001, reg_loss: 0.000 lr: 0.000843\n",
      " step: 119, acc: 0.641, loss: 0.986,data_loss: 0.986, reg_loss: 0.000 lr: 0.000843\n",
      " step: 120, acc: 0.547, loss: 1.101,data_loss: 1.101, reg_loss: 0.000 lr: 0.000843\n",
      " step: 121, acc: 0.523, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000843\n",
      " step: 122, acc: 0.562, loss: 1.077,data_loss: 1.077, reg_loss: 0.000 lr: 0.000843\n",
      " step: 123, acc: 0.484, loss: 1.140,data_loss: 1.140, reg_loss: 0.000 lr: 0.000843\n",
      " step: 124, acc: 0.562, loss: 1.063,data_loss: 1.063, reg_loss: 0.000 lr: 0.000843\n",
      " step: 125, acc: 0.555, loss: 1.058,data_loss: 1.058, reg_loss: 0.000 lr: 0.000843\n",
      " step: 126, acc: 0.523, loss: 1.157,data_loss: 1.157, reg_loss: 0.000 lr: 0.000843\n",
      " step: 127, acc: 0.516, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000843\n",
      " step: 128, acc: 0.617, loss: 1.001,data_loss: 1.001, reg_loss: 0.000 lr: 0.000843\n",
      " step: 129, acc: 0.617, loss: 1.003,data_loss: 1.003, reg_loss: 0.000 lr: 0.000843\n",
      " step: 130, acc: 0.523, loss: 1.234,data_loss: 1.234, reg_loss: 0.000 lr: 0.000843\n",
      " step: 131, acc: 0.477, loss: 1.187,data_loss: 1.187, reg_loss: 0.000 lr: 0.000843\n",
      " step: 132, acc: 0.570, loss: 1.033,data_loss: 1.033, reg_loss: 0.000 lr: 0.000843\n",
      " step: 133, acc: 0.594, loss: 1.084,data_loss: 1.084, reg_loss: 0.000 lr: 0.000843\n",
      " step: 134, acc: 0.641, loss: 0.975,data_loss: 0.975, reg_loss: 0.000 lr: 0.000843\n",
      " step: 135, acc: 0.547, loss: 1.054,data_loss: 1.054, reg_loss: 0.000 lr: 0.000843\n",
      " step: 136, acc: 0.570, loss: 1.106,data_loss: 1.106, reg_loss: 0.000 lr: 0.000843\n",
      " step: 137, acc: 0.648, loss: 0.928,data_loss: 0.928, reg_loss: 0.000 lr: 0.000843\n",
      " step: 138, acc: 0.664, loss: 1.017,data_loss: 1.017, reg_loss: 0.000 lr: 0.000843\n",
      " step: 139, acc: 0.586, loss: 1.046,data_loss: 1.046, reg_loss: 0.000 lr: 0.000842\n",
      " step: 140, acc: 0.617, loss: 1.039,data_loss: 1.039, reg_loss: 0.000 lr: 0.000842\n",
      " step: 141, acc: 0.508, loss: 1.125,data_loss: 1.125, reg_loss: 0.000 lr: 0.000842\n",
      " step: 142, acc: 0.664, loss: 0.990,data_loss: 0.990, reg_loss: 0.000 lr: 0.000842\n",
      " step: 143, acc: 0.586, loss: 1.119,data_loss: 1.119, reg_loss: 0.000 lr: 0.000842\n",
      " step: 144, acc: 0.555, loss: 1.108,data_loss: 1.108, reg_loss: 0.000 lr: 0.000842\n",
      " step: 145, acc: 0.602, loss: 1.043,data_loss: 1.043, reg_loss: 0.000 lr: 0.000842\n",
      " step: 146, acc: 0.555, loss: 1.119,data_loss: 1.119, reg_loss: 0.000 lr: 0.000842\n",
      " step: 147, acc: 0.594, loss: 1.025,data_loss: 1.025, reg_loss: 0.000 lr: 0.000842\n",
      " step: 148, acc: 0.594, loss: 1.110,data_loss: 1.110, reg_loss: 0.000 lr: 0.000842\n",
      " step: 149, acc: 0.555, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000842\n",
      " step: 150, acc: 0.594, loss: 1.039,data_loss: 1.039, reg_loss: 0.000 lr: 0.000842\n",
      " step: 151, acc: 0.547, loss: 1.204,data_loss: 1.204, reg_loss: 0.000 lr: 0.000842\n",
      " step: 152, acc: 0.641, loss: 0.912,data_loss: 0.912, reg_loss: 0.000 lr: 0.000842\n",
      " step: 153, acc: 0.641, loss: 0.914,data_loss: 0.914, reg_loss: 0.000 lr: 0.000842\n",
      " step: 154, acc: 0.578, loss: 1.056,data_loss: 1.056, reg_loss: 0.000 lr: 0.000842\n",
      " step: 155, acc: 0.641, loss: 1.102,data_loss: 1.102, reg_loss: 0.000 lr: 0.000842\n",
      " step: 156, acc: 0.523, loss: 1.196,data_loss: 1.196, reg_loss: 0.000 lr: 0.000842\n",
      " step: 157, acc: 0.594, loss: 1.056,data_loss: 1.056, reg_loss: 0.000 lr: 0.000842\n",
      " step: 158, acc: 0.555, loss: 1.180,data_loss: 1.180, reg_loss: 0.000 lr: 0.000842\n",
      " step: 159, acc: 0.547, loss: 1.181,data_loss: 1.181, reg_loss: 0.000 lr: 0.000842\n",
      " step: 160, acc: 0.539, loss: 1.380,data_loss: 1.380, reg_loss: 0.000 lr: 0.000842\n",
      " step: 161, acc: 0.523, loss: 1.225,data_loss: 1.225, reg_loss: 0.000 lr: 0.000842\n",
      " step: 162, acc: 0.602, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000842\n",
      " step: 163, acc: 0.602, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000842\n",
      " step: 164, acc: 0.617, loss: 1.047,data_loss: 1.047, reg_loss: 0.000 lr: 0.000842\n",
      " step: 165, acc: 0.523, loss: 1.268,data_loss: 1.268, reg_loss: 0.000 lr: 0.000842\n",
      " step: 166, acc: 0.531, loss: 1.227,data_loss: 1.227, reg_loss: 0.000 lr: 0.000842\n",
      " step: 167, acc: 0.516, loss: 1.187,data_loss: 1.187, reg_loss: 0.000 lr: 0.000842\n",
      " step: 168, acc: 0.586, loss: 1.007,data_loss: 1.007, reg_loss: 0.000 lr: 0.000841\n",
      " step: 169, acc: 0.586, loss: 1.077,data_loss: 1.077, reg_loss: 0.000 lr: 0.000841\n",
      " step: 170, acc: 0.555, loss: 1.144,data_loss: 1.144, reg_loss: 0.000 lr: 0.000841\n",
      " step: 171, acc: 0.586, loss: 1.104,data_loss: 1.104, reg_loss: 0.000 lr: 0.000841\n",
      " step: 172, acc: 0.500, loss: 1.189,data_loss: 1.189, reg_loss: 0.000 lr: 0.000841\n",
      " step: 173, acc: 0.531, loss: 1.190,data_loss: 1.190, reg_loss: 0.000 lr: 0.000841\n",
      " step: 174, acc: 0.594, loss: 1.092,data_loss: 1.092, reg_loss: 0.000 lr: 0.000841\n",
      " step: 175, acc: 0.578, loss: 1.120,data_loss: 1.120, reg_loss: 0.000 lr: 0.000841\n",
      " step: 176, acc: 0.562, loss: 1.178,data_loss: 1.178, reg_loss: 0.000 lr: 0.000841\n",
      " step: 177, acc: 0.648, loss: 1.025,data_loss: 1.025, reg_loss: 0.000 lr: 0.000841\n",
      " step: 178, acc: 0.594, loss: 1.143,data_loss: 1.143, reg_loss: 0.000 lr: 0.000841\n",
      " step: 179, acc: 0.594, loss: 0.976,data_loss: 0.976, reg_loss: 0.000 lr: 0.000841\n",
      " step: 180, acc: 0.711, loss: 0.852,data_loss: 0.852, reg_loss: 0.000 lr: 0.000841\n",
      " step: 181, acc: 0.531, loss: 1.216,data_loss: 1.216, reg_loss: 0.000 lr: 0.000841\n",
      " step: 182, acc: 0.656, loss: 0.933,data_loss: 0.933, reg_loss: 0.000 lr: 0.000841\n",
      " step: 183, acc: 0.586, loss: 1.041,data_loss: 1.041, reg_loss: 0.000 lr: 0.000841\n",
      " step: 184, acc: 0.539, loss: 1.199,data_loss: 1.199, reg_loss: 0.000 lr: 0.000841\n",
      " step: 185, acc: 0.531, loss: 1.143,data_loss: 1.143, reg_loss: 0.000 lr: 0.000841\n",
      " step: 186, acc: 0.617, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000841\n",
      " step: 187, acc: 0.594, loss: 1.047,data_loss: 1.047, reg_loss: 0.000 lr: 0.000841\n",
      " step: 188, acc: 0.562, loss: 1.253,data_loss: 1.253, reg_loss: 0.000 lr: 0.000841\n",
      " step: 189, acc: 0.547, loss: 1.136,data_loss: 1.136, reg_loss: 0.000 lr: 0.000841\n",
      " step: 190, acc: 0.578, loss: 1.177,data_loss: 1.177, reg_loss: 0.000 lr: 0.000841\n",
      " step: 191, acc: 0.617, loss: 1.040,data_loss: 1.040, reg_loss: 0.000 lr: 0.000841\n",
      " step: 192, acc: 0.594, loss: 1.040,data_loss: 1.040, reg_loss: 0.000 lr: 0.000841\n",
      " step: 193, acc: 0.570, loss: 1.072,data_loss: 1.072, reg_loss: 0.000 lr: 0.000841\n",
      " step: 194, acc: 0.570, loss: 1.036,data_loss: 1.036, reg_loss: 0.000 lr: 0.000841\n",
      " step: 195, acc: 0.594, loss: 1.048,data_loss: 1.048, reg_loss: 0.000 lr: 0.000841\n",
      " step: 196, acc: 0.562, loss: 1.050,data_loss: 1.050, reg_loss: 0.000 lr: 0.000840\n",
      " step: 197, acc: 0.617, loss: 0.973,data_loss: 0.973, reg_loss: 0.000 lr: 0.000840\n",
      " step: 198, acc: 0.523, loss: 1.163,data_loss: 1.163, reg_loss: 0.000 lr: 0.000840\n",
      " step: 199, acc: 0.492, loss: 1.286,data_loss: 1.286, reg_loss: 0.000 lr: 0.000840\n",
      " step: 200, acc: 0.523, loss: 1.120,data_loss: 1.120, reg_loss: 0.000 lr: 0.000840\n",
      " step: 201, acc: 0.648, loss: 0.948,data_loss: 0.948, reg_loss: 0.000 lr: 0.000840\n",
      " step: 202, acc: 0.633, loss: 1.001,data_loss: 1.001, reg_loss: 0.000 lr: 0.000840\n",
      " step: 203, acc: 0.547, loss: 1.061,data_loss: 1.061, reg_loss: 0.000 lr: 0.000840\n",
      " step: 204, acc: 0.547, loss: 1.064,data_loss: 1.064, reg_loss: 0.000 lr: 0.000840\n",
      " step: 205, acc: 0.578, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000840\n",
      " step: 206, acc: 0.594, loss: 1.059,data_loss: 1.059, reg_loss: 0.000 lr: 0.000840\n",
      " step: 207, acc: 0.586, loss: 1.112,data_loss: 1.112, reg_loss: 0.000 lr: 0.000840\n",
      " step: 208, acc: 0.648, loss: 0.990,data_loss: 0.990, reg_loss: 0.000 lr: 0.000840\n",
      " step: 209, acc: 0.602, loss: 1.022,data_loss: 1.022, reg_loss: 0.000 lr: 0.000840\n",
      " step: 210, acc: 0.586, loss: 1.066,data_loss: 1.066, reg_loss: 0.000 lr: 0.000840\n",
      " step: 211, acc: 0.602, loss: 0.981,data_loss: 0.981, reg_loss: 0.000 lr: 0.000840\n",
      " step: 212, acc: 0.586, loss: 1.028,data_loss: 1.028, reg_loss: 0.000 lr: 0.000840\n",
      " step: 213, acc: 0.523, loss: 1.114,data_loss: 1.114, reg_loss: 0.000 lr: 0.000840\n",
      " step: 214, acc: 0.555, loss: 1.183,data_loss: 1.183, reg_loss: 0.000 lr: 0.000840\n",
      " step: 215, acc: 0.625, loss: 0.999,data_loss: 0.999, reg_loss: 0.000 lr: 0.000840\n",
      " step: 216, acc: 0.594, loss: 0.926,data_loss: 0.926, reg_loss: 0.000 lr: 0.000840\n",
      " step: 217, acc: 0.516, loss: 1.165,data_loss: 1.165, reg_loss: 0.000 lr: 0.000840\n",
      " step: 218, acc: 0.656, loss: 1.080,data_loss: 1.080, reg_loss: 0.000 lr: 0.000840\n",
      " step: 219, acc: 0.570, loss: 1.118,data_loss: 1.118, reg_loss: 0.000 lr: 0.000840\n",
      " step: 220, acc: 0.648, loss: 1.023,data_loss: 1.023, reg_loss: 0.000 lr: 0.000840\n",
      " step: 221, acc: 0.562, loss: 1.041,data_loss: 1.041, reg_loss: 0.000 lr: 0.000840\n",
      " step: 222, acc: 0.570, loss: 1.049,data_loss: 1.049, reg_loss: 0.000 lr: 0.000840\n",
      " step: 223, acc: 0.508, loss: 1.182,data_loss: 1.182, reg_loss: 0.000 lr: 0.000840\n",
      " step: 224, acc: 0.703, loss: 0.904,data_loss: 0.904, reg_loss: 0.000 lr: 0.000839\n",
      "training , acc: 0.579, loss: 1.089,data_loss: 1.089, reg_loss: 0.000 lr: 0.000839\n",
      "validation, acc:0.457 ,loss: 1.543 \n",
      "epoch: 18\n",
      " step: 0, acc: 0.539, loss: 1.251,data_loss: 1.251, reg_loss: 0.000 lr: 0.000839\n",
      " step: 1, acc: 0.625, loss: 1.161,data_loss: 1.161, reg_loss: 0.000 lr: 0.000839\n",
      " step: 2, acc: 0.578, loss: 1.117,data_loss: 1.117, reg_loss: 0.000 lr: 0.000839\n",
      " step: 3, acc: 0.562, loss: 1.047,data_loss: 1.047, reg_loss: 0.000 lr: 0.000839\n",
      " step: 4, acc: 0.586, loss: 1.036,data_loss: 1.036, reg_loss: 0.000 lr: 0.000839\n",
      " step: 5, acc: 0.539, loss: 1.103,data_loss: 1.103, reg_loss: 0.000 lr: 0.000839\n",
      " step: 6, acc: 0.641, loss: 1.058,data_loss: 1.058, reg_loss: 0.000 lr: 0.000839\n",
      " step: 7, acc: 0.648, loss: 1.094,data_loss: 1.094, reg_loss: 0.000 lr: 0.000839\n",
      " step: 8, acc: 0.648, loss: 0.923,data_loss: 0.923, reg_loss: 0.000 lr: 0.000839\n",
      " step: 9, acc: 0.602, loss: 1.020,data_loss: 1.020, reg_loss: 0.000 lr: 0.000839\n",
      " step: 10, acc: 0.625, loss: 1.020,data_loss: 1.020, reg_loss: 0.000 lr: 0.000839\n",
      " step: 11, acc: 0.578, loss: 1.140,data_loss: 1.140, reg_loss: 0.000 lr: 0.000839\n",
      " step: 12, acc: 0.617, loss: 1.025,data_loss: 1.025, reg_loss: 0.000 lr: 0.000839\n",
      " step: 13, acc: 0.586, loss: 1.080,data_loss: 1.080, reg_loss: 0.000 lr: 0.000839\n",
      " step: 14, acc: 0.562, loss: 1.092,data_loss: 1.092, reg_loss: 0.000 lr: 0.000839\n",
      " step: 15, acc: 0.617, loss: 1.059,data_loss: 1.059, reg_loss: 0.000 lr: 0.000839\n",
      " step: 16, acc: 0.602, loss: 0.925,data_loss: 0.925, reg_loss: 0.000 lr: 0.000839\n",
      " step: 17, acc: 0.555, loss: 1.097,data_loss: 1.097, reg_loss: 0.000 lr: 0.000839\n",
      " step: 18, acc: 0.562, loss: 1.148,data_loss: 1.148, reg_loss: 0.000 lr: 0.000839\n",
      " step: 19, acc: 0.625, loss: 0.940,data_loss: 0.940, reg_loss: 0.000 lr: 0.000839\n",
      " step: 20, acc: 0.570, loss: 1.055,data_loss: 1.055, reg_loss: 0.000 lr: 0.000839\n",
      " step: 21, acc: 0.602, loss: 1.023,data_loss: 1.023, reg_loss: 0.000 lr: 0.000839\n",
      " step: 22, acc: 0.609, loss: 1.057,data_loss: 1.057, reg_loss: 0.000 lr: 0.000839\n",
      " step: 23, acc: 0.625, loss: 1.129,data_loss: 1.129, reg_loss: 0.000 lr: 0.000839\n",
      " step: 24, acc: 0.602, loss: 1.023,data_loss: 1.023, reg_loss: 0.000 lr: 0.000839\n",
      " step: 25, acc: 0.602, loss: 1.043,data_loss: 1.043, reg_loss: 0.000 lr: 0.000839\n",
      " step: 26, acc: 0.648, loss: 1.033,data_loss: 1.033, reg_loss: 0.000 lr: 0.000839\n",
      " step: 27, acc: 0.633, loss: 0.976,data_loss: 0.976, reg_loss: 0.000 lr: 0.000839\n",
      " step: 28, acc: 0.602, loss: 1.051,data_loss: 1.051, reg_loss: 0.000 lr: 0.000838\n",
      " step: 29, acc: 0.602, loss: 1.059,data_loss: 1.059, reg_loss: 0.000 lr: 0.000838\n",
      " step: 30, acc: 0.539, loss: 1.244,data_loss: 1.244, reg_loss: 0.000 lr: 0.000838\n",
      " step: 31, acc: 0.625, loss: 1.014,data_loss: 1.014, reg_loss: 0.000 lr: 0.000838\n",
      " step: 32, acc: 0.594, loss: 1.099,data_loss: 1.099, reg_loss: 0.000 lr: 0.000838\n",
      " step: 33, acc: 0.602, loss: 1.029,data_loss: 1.029, reg_loss: 0.000 lr: 0.000838\n",
      " step: 34, acc: 0.578, loss: 1.048,data_loss: 1.048, reg_loss: 0.000 lr: 0.000838\n",
      " step: 35, acc: 0.578, loss: 1.064,data_loss: 1.064, reg_loss: 0.000 lr: 0.000838\n",
      " step: 36, acc: 0.570, loss: 1.052,data_loss: 1.052, reg_loss: 0.000 lr: 0.000838\n",
      " step: 37, acc: 0.570, loss: 1.021,data_loss: 1.021, reg_loss: 0.000 lr: 0.000838\n",
      " step: 38, acc: 0.672, loss: 0.989,data_loss: 0.989, reg_loss: 0.000 lr: 0.000838\n",
      " step: 39, acc: 0.547, loss: 1.169,data_loss: 1.169, reg_loss: 0.000 lr: 0.000838\n",
      " step: 40, acc: 0.586, loss: 1.083,data_loss: 1.083, reg_loss: 0.000 lr: 0.000838\n",
      " step: 41, acc: 0.531, loss: 1.080,data_loss: 1.080, reg_loss: 0.000 lr: 0.000838\n",
      " step: 42, acc: 0.617, loss: 0.996,data_loss: 0.996, reg_loss: 0.000 lr: 0.000838\n",
      " step: 43, acc: 0.547, loss: 1.119,data_loss: 1.119, reg_loss: 0.000 lr: 0.000838\n",
      " step: 44, acc: 0.641, loss: 0.992,data_loss: 0.992, reg_loss: 0.000 lr: 0.000838\n",
      " step: 45, acc: 0.555, loss: 1.146,data_loss: 1.146, reg_loss: 0.000 lr: 0.000838\n",
      " step: 46, acc: 0.586, loss: 1.179,data_loss: 1.179, reg_loss: 0.000 lr: 0.000838\n",
      " step: 47, acc: 0.656, loss: 0.985,data_loss: 0.985, reg_loss: 0.000 lr: 0.000838\n",
      " step: 48, acc: 0.672, loss: 0.936,data_loss: 0.936, reg_loss: 0.000 lr: 0.000838\n",
      " step: 49, acc: 0.555, loss: 1.091,data_loss: 1.091, reg_loss: 0.000 lr: 0.000838\n",
      " step: 50, acc: 0.602, loss: 1.013,data_loss: 1.013, reg_loss: 0.000 lr: 0.000838\n",
      " step: 51, acc: 0.539, loss: 1.108,data_loss: 1.108, reg_loss: 0.000 lr: 0.000838\n",
      " step: 52, acc: 0.594, loss: 1.050,data_loss: 1.050, reg_loss: 0.000 lr: 0.000838\n",
      " step: 53, acc: 0.609, loss: 1.053,data_loss: 1.053, reg_loss: 0.000 lr: 0.000838\n",
      " step: 54, acc: 0.508, loss: 1.186,data_loss: 1.186, reg_loss: 0.000 lr: 0.000838\n",
      " step: 55, acc: 0.570, loss: 1.070,data_loss: 1.070, reg_loss: 0.000 lr: 0.000838\n",
      " step: 56, acc: 0.625, loss: 1.013,data_loss: 1.013, reg_loss: 0.000 lr: 0.000837\n",
      " step: 57, acc: 0.695, loss: 0.922,data_loss: 0.922, reg_loss: 0.000 lr: 0.000837\n",
      " step: 58, acc: 0.555, loss: 1.207,data_loss: 1.207, reg_loss: 0.000 lr: 0.000837\n",
      " step: 59, acc: 0.609, loss: 1.038,data_loss: 1.038, reg_loss: 0.000 lr: 0.000837\n",
      " step: 60, acc: 0.602, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000837\n",
      " step: 61, acc: 0.594, loss: 1.096,data_loss: 1.096, reg_loss: 0.000 lr: 0.000837\n",
      " step: 62, acc: 0.602, loss: 1.103,data_loss: 1.103, reg_loss: 0.000 lr: 0.000837\n",
      " step: 63, acc: 0.609, loss: 1.069,data_loss: 1.069, reg_loss: 0.000 lr: 0.000837\n",
      " step: 64, acc: 0.539, loss: 1.127,data_loss: 1.127, reg_loss: 0.000 lr: 0.000837\n",
      " step: 65, acc: 0.602, loss: 1.086,data_loss: 1.086, reg_loss: 0.000 lr: 0.000837\n",
      " step: 66, acc: 0.586, loss: 1.036,data_loss: 1.036, reg_loss: 0.000 lr: 0.000837\n",
      " step: 67, acc: 0.555, loss: 1.148,data_loss: 1.148, reg_loss: 0.000 lr: 0.000837\n",
      " step: 68, acc: 0.586, loss: 1.021,data_loss: 1.021, reg_loss: 0.000 lr: 0.000837\n",
      " step: 69, acc: 0.508, loss: 1.228,data_loss: 1.228, reg_loss: 0.000 lr: 0.000837\n",
      " step: 70, acc: 0.609, loss: 0.976,data_loss: 0.976, reg_loss: 0.000 lr: 0.000837\n",
      " step: 71, acc: 0.625, loss: 1.029,data_loss: 1.029, reg_loss: 0.000 lr: 0.000837\n",
      " step: 72, acc: 0.531, loss: 1.066,data_loss: 1.066, reg_loss: 0.000 lr: 0.000837\n",
      " step: 73, acc: 0.547, loss: 1.039,data_loss: 1.039, reg_loss: 0.000 lr: 0.000837\n",
      " step: 74, acc: 0.602, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000837\n",
      " step: 75, acc: 0.586, loss: 1.030,data_loss: 1.030, reg_loss: 0.000 lr: 0.000837\n",
      " step: 76, acc: 0.641, loss: 0.995,data_loss: 0.995, reg_loss: 0.000 lr: 0.000837\n",
      " step: 77, acc: 0.555, loss: 1.091,data_loss: 1.091, reg_loss: 0.000 lr: 0.000837\n",
      " step: 78, acc: 0.672, loss: 0.901,data_loss: 0.901, reg_loss: 0.000 lr: 0.000837\n",
      " step: 79, acc: 0.586, loss: 1.170,data_loss: 1.170, reg_loss: 0.000 lr: 0.000837\n",
      " step: 80, acc: 0.578, loss: 1.091,data_loss: 1.091, reg_loss: 0.000 lr: 0.000837\n",
      " step: 81, acc: 0.578, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000837\n",
      " step: 82, acc: 0.641, loss: 0.997,data_loss: 0.997, reg_loss: 0.000 lr: 0.000837\n",
      " step: 83, acc: 0.547, loss: 1.084,data_loss: 1.084, reg_loss: 0.000 lr: 0.000837\n",
      " step: 84, acc: 0.648, loss: 0.963,data_loss: 0.963, reg_loss: 0.000 lr: 0.000837\n",
      " step: 85, acc: 0.625, loss: 0.966,data_loss: 0.966, reg_loss: 0.000 lr: 0.000836\n",
      " step: 86, acc: 0.625, loss: 1.003,data_loss: 1.003, reg_loss: 0.000 lr: 0.000836\n",
      " step: 87, acc: 0.547, loss: 1.141,data_loss: 1.141, reg_loss: 0.000 lr: 0.000836\n",
      " step: 88, acc: 0.602, loss: 1.168,data_loss: 1.168, reg_loss: 0.000 lr: 0.000836\n",
      " step: 89, acc: 0.586, loss: 1.143,data_loss: 1.143, reg_loss: 0.000 lr: 0.000836\n",
      " step: 90, acc: 0.664, loss: 0.975,data_loss: 0.975, reg_loss: 0.000 lr: 0.000836\n",
      " step: 91, acc: 0.570, loss: 1.169,data_loss: 1.169, reg_loss: 0.000 lr: 0.000836\n",
      " step: 92, acc: 0.578, loss: 1.032,data_loss: 1.032, reg_loss: 0.000 lr: 0.000836\n",
      " step: 93, acc: 0.641, loss: 0.960,data_loss: 0.960, reg_loss: 0.000 lr: 0.000836\n",
      " step: 94, acc: 0.508, loss: 1.251,data_loss: 1.251, reg_loss: 0.000 lr: 0.000836\n",
      " step: 95, acc: 0.633, loss: 1.019,data_loss: 1.019, reg_loss: 0.000 lr: 0.000836\n",
      " step: 96, acc: 0.508, loss: 1.213,data_loss: 1.213, reg_loss: 0.000 lr: 0.000836\n",
      " step: 97, acc: 0.594, loss: 1.005,data_loss: 1.005, reg_loss: 0.000 lr: 0.000836\n",
      " step: 98, acc: 0.625, loss: 0.989,data_loss: 0.989, reg_loss: 0.000 lr: 0.000836\n",
      " step: 99, acc: 0.578, loss: 1.044,data_loss: 1.044, reg_loss: 0.000 lr: 0.000836\n",
      " step: 100, acc: 0.578, loss: 1.130,data_loss: 1.130, reg_loss: 0.000 lr: 0.000836\n",
      " step: 101, acc: 0.594, loss: 1.028,data_loss: 1.028, reg_loss: 0.000 lr: 0.000836\n",
      " step: 102, acc: 0.555, loss: 1.085,data_loss: 1.085, reg_loss: 0.000 lr: 0.000836\n",
      " step: 103, acc: 0.664, loss: 0.985,data_loss: 0.985, reg_loss: 0.000 lr: 0.000836\n",
      " step: 104, acc: 0.625, loss: 0.940,data_loss: 0.940, reg_loss: 0.000 lr: 0.000836\n",
      " step: 105, acc: 0.617, loss: 1.016,data_loss: 1.016, reg_loss: 0.000 lr: 0.000836\n",
      " step: 106, acc: 0.656, loss: 1.010,data_loss: 1.010, reg_loss: 0.000 lr: 0.000836\n",
      " step: 107, acc: 0.609, loss: 1.005,data_loss: 1.005, reg_loss: 0.000 lr: 0.000836\n",
      " step: 108, acc: 0.555, loss: 1.066,data_loss: 1.066, reg_loss: 0.000 lr: 0.000836\n",
      " step: 109, acc: 0.617, loss: 0.990,data_loss: 0.990, reg_loss: 0.000 lr: 0.000836\n",
      " step: 110, acc: 0.617, loss: 0.977,data_loss: 0.977, reg_loss: 0.000 lr: 0.000836\n",
      " step: 111, acc: 0.609, loss: 1.196,data_loss: 1.196, reg_loss: 0.000 lr: 0.000836\n",
      " step: 112, acc: 0.562, loss: 1.126,data_loss: 1.126, reg_loss: 0.000 lr: 0.000836\n",
      " step: 113, acc: 0.578, loss: 1.102,data_loss: 1.102, reg_loss: 0.000 lr: 0.000835\n",
      " step: 114, acc: 0.523, loss: 1.212,data_loss: 1.212, reg_loss: 0.000 lr: 0.000835\n",
      " step: 115, acc: 0.578, loss: 1.080,data_loss: 1.080, reg_loss: 0.000 lr: 0.000835\n",
      " step: 116, acc: 0.500, loss: 1.172,data_loss: 1.172, reg_loss: 0.000 lr: 0.000835\n",
      " step: 117, acc: 0.578, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000835\n",
      " step: 118, acc: 0.562, loss: 1.120,data_loss: 1.120, reg_loss: 0.000 lr: 0.000835\n",
      " step: 119, acc: 0.602, loss: 1.010,data_loss: 1.010, reg_loss: 0.000 lr: 0.000835\n",
      " step: 120, acc: 0.641, loss: 0.993,data_loss: 0.993, reg_loss: 0.000 lr: 0.000835\n",
      " step: 121, acc: 0.547, loss: 1.190,data_loss: 1.190, reg_loss: 0.000 lr: 0.000835\n",
      " step: 122, acc: 0.562, loss: 1.062,data_loss: 1.062, reg_loss: 0.000 lr: 0.000835\n",
      " step: 123, acc: 0.531, loss: 1.095,data_loss: 1.095, reg_loss: 0.000 lr: 0.000835\n",
      " step: 124, acc: 0.562, loss: 1.086,data_loss: 1.086, reg_loss: 0.000 lr: 0.000835\n",
      " step: 125, acc: 0.602, loss: 1.059,data_loss: 1.059, reg_loss: 0.000 lr: 0.000835\n",
      " step: 126, acc: 0.586, loss: 1.125,data_loss: 1.125, reg_loss: 0.000 lr: 0.000835\n",
      " step: 127, acc: 0.484, loss: 1.189,data_loss: 1.189, reg_loss: 0.000 lr: 0.000835\n",
      " step: 128, acc: 0.562, loss: 1.086,data_loss: 1.086, reg_loss: 0.000 lr: 0.000835\n",
      " step: 129, acc: 0.648, loss: 0.990,data_loss: 0.990, reg_loss: 0.000 lr: 0.000835\n",
      " step: 130, acc: 0.469, loss: 1.262,data_loss: 1.262, reg_loss: 0.000 lr: 0.000835\n",
      " step: 131, acc: 0.531, loss: 1.072,data_loss: 1.072, reg_loss: 0.000 lr: 0.000835\n",
      " step: 132, acc: 0.555, loss: 1.079,data_loss: 1.079, reg_loss: 0.000 lr: 0.000835\n",
      " step: 133, acc: 0.594, loss: 1.125,data_loss: 1.125, reg_loss: 0.000 lr: 0.000835\n",
      " step: 134, acc: 0.695, loss: 0.908,data_loss: 0.908, reg_loss: 0.000 lr: 0.000835\n",
      " step: 135, acc: 0.539, loss: 1.126,data_loss: 1.126, reg_loss: 0.000 lr: 0.000835\n",
      " step: 136, acc: 0.578, loss: 1.179,data_loss: 1.179, reg_loss: 0.000 lr: 0.000835\n",
      " step: 137, acc: 0.648, loss: 0.959,data_loss: 0.959, reg_loss: 0.000 lr: 0.000835\n",
      " step: 138, acc: 0.547, loss: 1.180,data_loss: 1.180, reg_loss: 0.000 lr: 0.000835\n",
      " step: 139, acc: 0.648, loss: 0.990,data_loss: 0.990, reg_loss: 0.000 lr: 0.000835\n",
      " step: 140, acc: 0.578, loss: 1.023,data_loss: 1.023, reg_loss: 0.000 lr: 0.000835\n",
      " step: 141, acc: 0.570, loss: 1.109,data_loss: 1.109, reg_loss: 0.000 lr: 0.000835\n",
      " step: 142, acc: 0.664, loss: 0.904,data_loss: 0.904, reg_loss: 0.000 lr: 0.000834\n",
      " step: 143, acc: 0.602, loss: 1.087,data_loss: 1.087, reg_loss: 0.000 lr: 0.000834\n",
      " step: 144, acc: 0.555, loss: 1.132,data_loss: 1.132, reg_loss: 0.000 lr: 0.000834\n",
      " step: 145, acc: 0.602, loss: 0.998,data_loss: 0.998, reg_loss: 0.000 lr: 0.000834\n",
      " step: 146, acc: 0.562, loss: 1.148,data_loss: 1.148, reg_loss: 0.000 lr: 0.000834\n",
      " step: 147, acc: 0.625, loss: 0.983,data_loss: 0.983, reg_loss: 0.000 lr: 0.000834\n",
      " step: 148, acc: 0.672, loss: 0.973,data_loss: 0.973, reg_loss: 0.000 lr: 0.000834\n",
      " step: 149, acc: 0.586, loss: 1.056,data_loss: 1.056, reg_loss: 0.000 lr: 0.000834\n",
      " step: 150, acc: 0.609, loss: 0.961,data_loss: 0.961, reg_loss: 0.000 lr: 0.000834\n",
      " step: 151, acc: 0.578, loss: 1.101,data_loss: 1.101, reg_loss: 0.000 lr: 0.000834\n",
      " step: 152, acc: 0.625, loss: 0.969,data_loss: 0.969, reg_loss: 0.000 lr: 0.000834\n",
      " step: 153, acc: 0.570, loss: 0.974,data_loss: 0.974, reg_loss: 0.000 lr: 0.000834\n",
      " step: 154, acc: 0.609, loss: 1.028,data_loss: 1.028, reg_loss: 0.000 lr: 0.000834\n",
      " step: 155, acc: 0.633, loss: 0.984,data_loss: 0.984, reg_loss: 0.000 lr: 0.000834\n",
      " step: 156, acc: 0.602, loss: 1.062,data_loss: 1.062, reg_loss: 0.000 lr: 0.000834\n",
      " step: 157, acc: 0.641, loss: 0.896,data_loss: 0.896, reg_loss: 0.000 lr: 0.000834\n",
      " step: 158, acc: 0.523, loss: 1.173,data_loss: 1.173, reg_loss: 0.000 lr: 0.000834\n",
      " step: 159, acc: 0.617, loss: 1.090,data_loss: 1.090, reg_loss: 0.000 lr: 0.000834\n",
      " step: 160, acc: 0.500, loss: 1.247,data_loss: 1.247, reg_loss: 0.000 lr: 0.000834\n",
      " step: 161, acc: 0.602, loss: 1.070,data_loss: 1.070, reg_loss: 0.000 lr: 0.000834\n",
      " step: 162, acc: 0.594, loss: 1.153,data_loss: 1.153, reg_loss: 0.000 lr: 0.000834\n",
      " step: 163, acc: 0.586, loss: 0.996,data_loss: 0.996, reg_loss: 0.000 lr: 0.000834\n",
      " step: 164, acc: 0.633, loss: 1.037,data_loss: 1.037, reg_loss: 0.000 lr: 0.000834\n",
      " step: 165, acc: 0.570, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000834\n",
      " step: 166, acc: 0.508, loss: 1.166,data_loss: 1.166, reg_loss: 0.000 lr: 0.000834\n",
      " step: 167, acc: 0.539, loss: 1.120,data_loss: 1.120, reg_loss: 0.000 lr: 0.000834\n",
      " step: 168, acc: 0.617, loss: 1.126,data_loss: 1.126, reg_loss: 0.000 lr: 0.000834\n",
      " step: 169, acc: 0.617, loss: 1.022,data_loss: 1.022, reg_loss: 0.000 lr: 0.000834\n",
      " step: 170, acc: 0.602, loss: 1.054,data_loss: 1.054, reg_loss: 0.000 lr: 0.000834\n",
      " step: 171, acc: 0.633, loss: 0.967,data_loss: 0.967, reg_loss: 0.000 lr: 0.000833\n",
      " step: 172, acc: 0.523, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000833\n",
      " step: 173, acc: 0.609, loss: 1.089,data_loss: 1.089, reg_loss: 0.000 lr: 0.000833\n",
      " step: 174, acc: 0.570, loss: 1.119,data_loss: 1.119, reg_loss: 0.000 lr: 0.000833\n",
      " step: 175, acc: 0.539, loss: 1.147,data_loss: 1.147, reg_loss: 0.000 lr: 0.000833\n",
      " step: 176, acc: 0.570, loss: 1.238,data_loss: 1.238, reg_loss: 0.000 lr: 0.000833\n",
      " step: 177, acc: 0.570, loss: 1.019,data_loss: 1.019, reg_loss: 0.000 lr: 0.000833\n",
      " step: 178, acc: 0.555, loss: 1.165,data_loss: 1.165, reg_loss: 0.000 lr: 0.000833\n",
      " step: 179, acc: 0.656, loss: 0.932,data_loss: 0.932, reg_loss: 0.000 lr: 0.000833\n",
      " step: 180, acc: 0.633, loss: 0.939,data_loss: 0.939, reg_loss: 0.000 lr: 0.000833\n",
      " step: 181, acc: 0.633, loss: 1.066,data_loss: 1.066, reg_loss: 0.000 lr: 0.000833\n",
      " step: 182, acc: 0.594, loss: 1.025,data_loss: 1.025, reg_loss: 0.000 lr: 0.000833\n",
      " step: 183, acc: 0.594, loss: 1.007,data_loss: 1.007, reg_loss: 0.000 lr: 0.000833\n",
      " step: 184, acc: 0.594, loss: 1.141,data_loss: 1.141, reg_loss: 0.000 lr: 0.000833\n",
      " step: 185, acc: 0.570, loss: 1.135,data_loss: 1.135, reg_loss: 0.000 lr: 0.000833\n",
      " step: 186, acc: 0.586, loss: 1.030,data_loss: 1.030, reg_loss: 0.000 lr: 0.000833\n",
      " step: 187, acc: 0.570, loss: 1.000,data_loss: 1.000, reg_loss: 0.000 lr: 0.000833\n",
      " step: 188, acc: 0.531, loss: 1.141,data_loss: 1.141, reg_loss: 0.000 lr: 0.000833\n",
      " step: 189, acc: 0.625, loss: 1.035,data_loss: 1.035, reg_loss: 0.000 lr: 0.000833\n",
      " step: 190, acc: 0.508, loss: 1.229,data_loss: 1.229, reg_loss: 0.000 lr: 0.000833\n",
      " step: 191, acc: 0.531, loss: 1.105,data_loss: 1.105, reg_loss: 0.000 lr: 0.000833\n",
      " step: 192, acc: 0.609, loss: 1.069,data_loss: 1.069, reg_loss: 0.000 lr: 0.000833\n",
      " step: 193, acc: 0.555, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000833\n",
      " step: 194, acc: 0.617, loss: 0.971,data_loss: 0.971, reg_loss: 0.000 lr: 0.000833\n",
      " step: 195, acc: 0.602, loss: 1.060,data_loss: 1.060, reg_loss: 0.000 lr: 0.000833\n",
      " step: 196, acc: 0.570, loss: 1.063,data_loss: 1.063, reg_loss: 0.000 lr: 0.000833\n",
      " step: 197, acc: 0.633, loss: 0.966,data_loss: 0.966, reg_loss: 0.000 lr: 0.000833\n",
      " step: 198, acc: 0.648, loss: 0.987,data_loss: 0.987, reg_loss: 0.000 lr: 0.000833\n",
      " step: 199, acc: 0.508, loss: 1.217,data_loss: 1.217, reg_loss: 0.000 lr: 0.000833\n",
      " step: 200, acc: 0.602, loss: 1.021,data_loss: 1.021, reg_loss: 0.000 lr: 0.000832\n",
      " step: 201, acc: 0.578, loss: 0.998,data_loss: 0.998, reg_loss: 0.000 lr: 0.000832\n",
      " step: 202, acc: 0.570, loss: 0.980,data_loss: 0.980, reg_loss: 0.000 lr: 0.000832\n",
      " step: 203, acc: 0.586, loss: 1.017,data_loss: 1.017, reg_loss: 0.000 lr: 0.000832\n",
      " step: 204, acc: 0.625, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000832\n",
      " step: 205, acc: 0.594, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000832\n",
      " step: 206, acc: 0.578, loss: 1.014,data_loss: 1.014, reg_loss: 0.000 lr: 0.000832\n",
      " step: 207, acc: 0.625, loss: 1.013,data_loss: 1.013, reg_loss: 0.000 lr: 0.000832\n",
      " step: 208, acc: 0.680, loss: 0.941,data_loss: 0.941, reg_loss: 0.000 lr: 0.000832\n",
      " step: 209, acc: 0.625, loss: 0.976,data_loss: 0.976, reg_loss: 0.000 lr: 0.000832\n",
      " step: 210, acc: 0.586, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000832\n",
      " step: 211, acc: 0.648, loss: 0.959,data_loss: 0.959, reg_loss: 0.000 lr: 0.000832\n",
      " step: 212, acc: 0.617, loss: 1.124,data_loss: 1.124, reg_loss: 0.000 lr: 0.000832\n",
      " step: 213, acc: 0.539, loss: 1.129,data_loss: 1.129, reg_loss: 0.000 lr: 0.000832\n",
      " step: 214, acc: 0.508, loss: 1.112,data_loss: 1.112, reg_loss: 0.000 lr: 0.000832\n",
      " step: 215, acc: 0.617, loss: 0.911,data_loss: 0.911, reg_loss: 0.000 lr: 0.000832\n",
      " step: 216, acc: 0.594, loss: 0.944,data_loss: 0.944, reg_loss: 0.000 lr: 0.000832\n",
      " step: 217, acc: 0.617, loss: 1.034,data_loss: 1.034, reg_loss: 0.000 lr: 0.000832\n",
      " step: 218, acc: 0.656, loss: 0.919,data_loss: 0.919, reg_loss: 0.000 lr: 0.000832\n",
      " step: 219, acc: 0.578, loss: 1.059,data_loss: 1.059, reg_loss: 0.000 lr: 0.000832\n",
      " step: 220, acc: 0.609, loss: 1.083,data_loss: 1.083, reg_loss: 0.000 lr: 0.000832\n",
      " step: 221, acc: 0.609, loss: 1.070,data_loss: 1.070, reg_loss: 0.000 lr: 0.000832\n",
      " step: 222, acc: 0.539, loss: 1.119,data_loss: 1.119, reg_loss: 0.000 lr: 0.000832\n",
      " step: 223, acc: 0.547, loss: 1.187,data_loss: 1.187, reg_loss: 0.000 lr: 0.000832\n",
      " step: 224, acc: 0.703, loss: 0.759,data_loss: 0.759, reg_loss: 0.000 lr: 0.000832\n",
      "training , acc: 0.591, loss: 1.064,data_loss: 1.064, reg_loss: 0.000 lr: 0.000832\n",
      "validation, acc:0.460 ,loss: 1.557 \n",
      "epoch: 19\n",
      " step: 0, acc: 0.523, loss: 1.246,data_loss: 1.246, reg_loss: 0.000 lr: 0.000832\n",
      " step: 1, acc: 0.547, loss: 1.157,data_loss: 1.157, reg_loss: 0.000 lr: 0.000832\n",
      " step: 2, acc: 0.555, loss: 1.094,data_loss: 1.094, reg_loss: 0.000 lr: 0.000832\n",
      " step: 3, acc: 0.586, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000831\n",
      " step: 4, acc: 0.625, loss: 0.958,data_loss: 0.958, reg_loss: 0.000 lr: 0.000831\n",
      " step: 5, acc: 0.625, loss: 0.995,data_loss: 0.995, reg_loss: 0.000 lr: 0.000831\n",
      " step: 6, acc: 0.594, loss: 1.060,data_loss: 1.060, reg_loss: 0.000 lr: 0.000831\n",
      " step: 7, acc: 0.531, loss: 1.175,data_loss: 1.175, reg_loss: 0.000 lr: 0.000831\n",
      " step: 8, acc: 0.664, loss: 0.935,data_loss: 0.935, reg_loss: 0.000 lr: 0.000831\n",
      " step: 9, acc: 0.664, loss: 0.897,data_loss: 0.897, reg_loss: 0.000 lr: 0.000831\n",
      " step: 10, acc: 0.648, loss: 0.875,data_loss: 0.875, reg_loss: 0.000 lr: 0.000831\n",
      " step: 11, acc: 0.633, loss: 1.008,data_loss: 1.008, reg_loss: 0.000 lr: 0.000831\n",
      " step: 12, acc: 0.570, loss: 1.070,data_loss: 1.070, reg_loss: 0.000 lr: 0.000831\n",
      " step: 13, acc: 0.617, loss: 1.024,data_loss: 1.024, reg_loss: 0.000 lr: 0.000831\n",
      " step: 14, acc: 0.617, loss: 1.006,data_loss: 1.006, reg_loss: 0.000 lr: 0.000831\n",
      " step: 15, acc: 0.602, loss: 1.161,data_loss: 1.161, reg_loss: 0.000 lr: 0.000831\n",
      " step: 16, acc: 0.648, loss: 0.965,data_loss: 0.965, reg_loss: 0.000 lr: 0.000831\n",
      " step: 17, acc: 0.617, loss: 0.963,data_loss: 0.963, reg_loss: 0.000 lr: 0.000831\n",
      " step: 18, acc: 0.602, loss: 1.120,data_loss: 1.120, reg_loss: 0.000 lr: 0.000831\n",
      " step: 19, acc: 0.703, loss: 0.821,data_loss: 0.821, reg_loss: 0.000 lr: 0.000831\n",
      " step: 20, acc: 0.555, loss: 1.177,data_loss: 1.177, reg_loss: 0.000 lr: 0.000831\n",
      " step: 21, acc: 0.594, loss: 0.958,data_loss: 0.958, reg_loss: 0.000 lr: 0.000831\n",
      " step: 22, acc: 0.648, loss: 0.889,data_loss: 0.889, reg_loss: 0.000 lr: 0.000831\n",
      " step: 23, acc: 0.609, loss: 1.037,data_loss: 1.037, reg_loss: 0.000 lr: 0.000831\n",
      " step: 24, acc: 0.633, loss: 0.925,data_loss: 0.925, reg_loss: 0.000 lr: 0.000831\n",
      " step: 25, acc: 0.625, loss: 1.001,data_loss: 1.001, reg_loss: 0.000 lr: 0.000831\n",
      " step: 26, acc: 0.633, loss: 1.076,data_loss: 1.076, reg_loss: 0.000 lr: 0.000831\n",
      " step: 27, acc: 0.656, loss: 0.940,data_loss: 0.940, reg_loss: 0.000 lr: 0.000831\n",
      " step: 28, acc: 0.648, loss: 0.927,data_loss: 0.927, reg_loss: 0.000 lr: 0.000831\n",
      " step: 29, acc: 0.633, loss: 0.986,data_loss: 0.986, reg_loss: 0.000 lr: 0.000831\n",
      " step: 30, acc: 0.523, loss: 1.286,data_loss: 1.286, reg_loss: 0.000 lr: 0.000831\n",
      " step: 31, acc: 0.641, loss: 0.949,data_loss: 0.949, reg_loss: 0.000 lr: 0.000831\n",
      " step: 32, acc: 0.602, loss: 0.945,data_loss: 0.945, reg_loss: 0.000 lr: 0.000830\n",
      " step: 33, acc: 0.625, loss: 1.010,data_loss: 1.010, reg_loss: 0.000 lr: 0.000830\n",
      " step: 34, acc: 0.547, loss: 1.074,data_loss: 1.074, reg_loss: 0.000 lr: 0.000830\n",
      " step: 35, acc: 0.578, loss: 1.040,data_loss: 1.040, reg_loss: 0.000 lr: 0.000830\n",
      " step: 36, acc: 0.672, loss: 0.885,data_loss: 0.885, reg_loss: 0.000 lr: 0.000830\n",
      " step: 37, acc: 0.578, loss: 0.970,data_loss: 0.970, reg_loss: 0.000 lr: 0.000830\n",
      " step: 38, acc: 0.625, loss: 1.028,data_loss: 1.028, reg_loss: 0.000 lr: 0.000830\n",
      " step: 39, acc: 0.500, loss: 1.207,data_loss: 1.207, reg_loss: 0.000 lr: 0.000830\n",
      " step: 40, acc: 0.594, loss: 1.045,data_loss: 1.045, reg_loss: 0.000 lr: 0.000830\n",
      " step: 41, acc: 0.625, loss: 1.061,data_loss: 1.061, reg_loss: 0.000 lr: 0.000830\n",
      " step: 42, acc: 0.664, loss: 0.953,data_loss: 0.953, reg_loss: 0.000 lr: 0.000830\n",
      " step: 43, acc: 0.547, loss: 1.216,data_loss: 1.216, reg_loss: 0.000 lr: 0.000830\n",
      " step: 44, acc: 0.586, loss: 1.080,data_loss: 1.080, reg_loss: 0.000 lr: 0.000830\n",
      " step: 45, acc: 0.578, loss: 1.104,data_loss: 1.104, reg_loss: 0.000 lr: 0.000830\n",
      " step: 46, acc: 0.531, loss: 1.195,data_loss: 1.195, reg_loss: 0.000 lr: 0.000830\n",
      " step: 47, acc: 0.602, loss: 0.992,data_loss: 0.992, reg_loss: 0.000 lr: 0.000830\n",
      " step: 48, acc: 0.664, loss: 1.014,data_loss: 1.014, reg_loss: 0.000 lr: 0.000830\n",
      " step: 49, acc: 0.633, loss: 1.059,data_loss: 1.059, reg_loss: 0.000 lr: 0.000830\n",
      " step: 50, acc: 0.625, loss: 0.942,data_loss: 0.942, reg_loss: 0.000 lr: 0.000830\n",
      " step: 51, acc: 0.539, loss: 1.128,data_loss: 1.128, reg_loss: 0.000 lr: 0.000830\n",
      " step: 52, acc: 0.586, loss: 1.050,data_loss: 1.050, reg_loss: 0.000 lr: 0.000830\n",
      " step: 53, acc: 0.625, loss: 1.048,data_loss: 1.048, reg_loss: 0.000 lr: 0.000830\n",
      " step: 54, acc: 0.531, loss: 1.222,data_loss: 1.222, reg_loss: 0.000 lr: 0.000830\n",
      " step: 55, acc: 0.617, loss: 0.916,data_loss: 0.916, reg_loss: 0.000 lr: 0.000830\n",
      " step: 56, acc: 0.625, loss: 1.012,data_loss: 1.012, reg_loss: 0.000 lr: 0.000830\n",
      " step: 57, acc: 0.672, loss: 0.979,data_loss: 0.979, reg_loss: 0.000 lr: 0.000830\n",
      " step: 58, acc: 0.586, loss: 1.203,data_loss: 1.203, reg_loss: 0.000 lr: 0.000830\n",
      " step: 59, acc: 0.633, loss: 1.030,data_loss: 1.030, reg_loss: 0.000 lr: 0.000830\n",
      " step: 60, acc: 0.586, loss: 0.982,data_loss: 0.982, reg_loss: 0.000 lr: 0.000830\n",
      " step: 61, acc: 0.570, loss: 1.080,data_loss: 1.080, reg_loss: 0.000 lr: 0.000829\n",
      " step: 62, acc: 0.602, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000829\n",
      " step: 63, acc: 0.594, loss: 1.047,data_loss: 1.047, reg_loss: 0.000 lr: 0.000829\n",
      " step: 64, acc: 0.562, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000829\n",
      " step: 65, acc: 0.602, loss: 1.139,data_loss: 1.139, reg_loss: 0.000 lr: 0.000829\n",
      " step: 66, acc: 0.672, loss: 0.934,data_loss: 0.934, reg_loss: 0.000 lr: 0.000829\n",
      " step: 67, acc: 0.562, loss: 1.071,data_loss: 1.071, reg_loss: 0.000 lr: 0.000829\n",
      " step: 68, acc: 0.555, loss: 1.002,data_loss: 1.002, reg_loss: 0.000 lr: 0.000829\n",
      " step: 69, acc: 0.531, loss: 1.140,data_loss: 1.140, reg_loss: 0.000 lr: 0.000829\n",
      " step: 70, acc: 0.680, loss: 0.923,data_loss: 0.923, reg_loss: 0.000 lr: 0.000829\n",
      " step: 71, acc: 0.570, loss: 1.001,data_loss: 1.001, reg_loss: 0.000 lr: 0.000829\n",
      " step: 72, acc: 0.578, loss: 0.944,data_loss: 0.944, reg_loss: 0.000 lr: 0.000829\n",
      " step: 73, acc: 0.547, loss: 1.181,data_loss: 1.181, reg_loss: 0.000 lr: 0.000829\n",
      " step: 74, acc: 0.625, loss: 1.043,data_loss: 1.043, reg_loss: 0.000 lr: 0.000829\n",
      " step: 75, acc: 0.602, loss: 1.134,data_loss: 1.134, reg_loss: 0.000 lr: 0.000829\n",
      " step: 76, acc: 0.648, loss: 0.935,data_loss: 0.935, reg_loss: 0.000 lr: 0.000829\n",
      " step: 77, acc: 0.531, loss: 1.067,data_loss: 1.067, reg_loss: 0.000 lr: 0.000829\n",
      " step: 78, acc: 0.648, loss: 0.932,data_loss: 0.932, reg_loss: 0.000 lr: 0.000829\n",
      " step: 79, acc: 0.633, loss: 1.032,data_loss: 1.032, reg_loss: 0.000 lr: 0.000829\n",
      " step: 80, acc: 0.547, loss: 1.105,data_loss: 1.105, reg_loss: 0.000 lr: 0.000829\n",
      " step: 81, acc: 0.547, loss: 1.235,data_loss: 1.235, reg_loss: 0.000 lr: 0.000829\n",
      " step: 82, acc: 0.594, loss: 1.074,data_loss: 1.074, reg_loss: 0.000 lr: 0.000829\n",
      " step: 83, acc: 0.578, loss: 1.024,data_loss: 1.024, reg_loss: 0.000 lr: 0.000829\n",
      " step: 84, acc: 0.656, loss: 0.906,data_loss: 0.906, reg_loss: 0.000 lr: 0.000829\n",
      " step: 85, acc: 0.602, loss: 1.046,data_loss: 1.046, reg_loss: 0.000 lr: 0.000829\n",
      " step: 86, acc: 0.648, loss: 0.913,data_loss: 0.913, reg_loss: 0.000 lr: 0.000829\n",
      " step: 87, acc: 0.586, loss: 1.070,data_loss: 1.070, reg_loss: 0.000 lr: 0.000829\n",
      " step: 88, acc: 0.586, loss: 1.136,data_loss: 1.136, reg_loss: 0.000 lr: 0.000829\n",
      " step: 89, acc: 0.625, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000829\n",
      " step: 90, acc: 0.586, loss: 1.012,data_loss: 1.012, reg_loss: 0.000 lr: 0.000829\n",
      " step: 91, acc: 0.594, loss: 1.103,data_loss: 1.103, reg_loss: 0.000 lr: 0.000828\n",
      " step: 92, acc: 0.617, loss: 0.995,data_loss: 0.995, reg_loss: 0.000 lr: 0.000828\n",
      " step: 93, acc: 0.672, loss: 0.846,data_loss: 0.846, reg_loss: 0.000 lr: 0.000828\n",
      " step: 94, acc: 0.516, loss: 1.331,data_loss: 1.331, reg_loss: 0.000 lr: 0.000828\n",
      " step: 95, acc: 0.664, loss: 0.893,data_loss: 0.893, reg_loss: 0.000 lr: 0.000828\n",
      " step: 96, acc: 0.586, loss: 1.114,data_loss: 1.114, reg_loss: 0.000 lr: 0.000828\n",
      " step: 97, acc: 0.641, loss: 0.977,data_loss: 0.977, reg_loss: 0.000 lr: 0.000828\n",
      " step: 98, acc: 0.625, loss: 1.005,data_loss: 1.005, reg_loss: 0.000 lr: 0.000828\n",
      " step: 99, acc: 0.625, loss: 0.940,data_loss: 0.940, reg_loss: 0.000 lr: 0.000828\n",
      " step: 100, acc: 0.617, loss: 1.020,data_loss: 1.020, reg_loss: 0.000 lr: 0.000828\n",
      " step: 101, acc: 0.609, loss: 1.032,data_loss: 1.032, reg_loss: 0.000 lr: 0.000828\n",
      " step: 102, acc: 0.602, loss: 1.092,data_loss: 1.092, reg_loss: 0.000 lr: 0.000828\n",
      " step: 103, acc: 0.602, loss: 1.076,data_loss: 1.076, reg_loss: 0.000 lr: 0.000828\n",
      " step: 104, acc: 0.656, loss: 0.958,data_loss: 0.958, reg_loss: 0.000 lr: 0.000828\n",
      " step: 105, acc: 0.609, loss: 0.993,data_loss: 0.993, reg_loss: 0.000 lr: 0.000828\n",
      " step: 106, acc: 0.633, loss: 0.996,data_loss: 0.996, reg_loss: 0.000 lr: 0.000828\n",
      " step: 107, acc: 0.664, loss: 0.912,data_loss: 0.912, reg_loss: 0.000 lr: 0.000828\n",
      " step: 108, acc: 0.594, loss: 0.992,data_loss: 0.992, reg_loss: 0.000 lr: 0.000828\n",
      " step: 109, acc: 0.547, loss: 1.175,data_loss: 1.175, reg_loss: 0.000 lr: 0.000828\n",
      " step: 110, acc: 0.586, loss: 1.018,data_loss: 1.018, reg_loss: 0.000 lr: 0.000828\n",
      " step: 111, acc: 0.594, loss: 1.110,data_loss: 1.110, reg_loss: 0.000 lr: 0.000828\n",
      " step: 112, acc: 0.539, loss: 1.085,data_loss: 1.085, reg_loss: 0.000 lr: 0.000828\n",
      " step: 113, acc: 0.656, loss: 0.994,data_loss: 0.994, reg_loss: 0.000 lr: 0.000828\n",
      " step: 114, acc: 0.547, loss: 1.128,data_loss: 1.128, reg_loss: 0.000 lr: 0.000828\n",
      " step: 115, acc: 0.578, loss: 1.091,data_loss: 1.091, reg_loss: 0.000 lr: 0.000828\n",
      " step: 116, acc: 0.609, loss: 1.103,data_loss: 1.103, reg_loss: 0.000 lr: 0.000828\n",
      " step: 117, acc: 0.570, loss: 1.114,data_loss: 1.114, reg_loss: 0.000 lr: 0.000828\n",
      " step: 118, acc: 0.562, loss: 1.016,data_loss: 1.016, reg_loss: 0.000 lr: 0.000828\n",
      " step: 119, acc: 0.625, loss: 0.941,data_loss: 0.941, reg_loss: 0.000 lr: 0.000828\n",
      " step: 120, acc: 0.555, loss: 1.092,data_loss: 1.092, reg_loss: 0.000 lr: 0.000827\n",
      " step: 121, acc: 0.547, loss: 1.141,data_loss: 1.141, reg_loss: 0.000 lr: 0.000827\n",
      " step: 122, acc: 0.570, loss: 1.002,data_loss: 1.002, reg_loss: 0.000 lr: 0.000827\n",
      " step: 123, acc: 0.594, loss: 1.070,data_loss: 1.070, reg_loss: 0.000 lr: 0.000827\n",
      " step: 124, acc: 0.570, loss: 1.117,data_loss: 1.117, reg_loss: 0.000 lr: 0.000827\n",
      " step: 125, acc: 0.609, loss: 1.043,data_loss: 1.043, reg_loss: 0.000 lr: 0.000827\n",
      " step: 126, acc: 0.609, loss: 0.972,data_loss: 0.972, reg_loss: 0.000 lr: 0.000827\n",
      " step: 127, acc: 0.539, loss: 1.124,data_loss: 1.124, reg_loss: 0.000 lr: 0.000827\n",
      " step: 128, acc: 0.578, loss: 1.162,data_loss: 1.162, reg_loss: 0.000 lr: 0.000827\n",
      " step: 129, acc: 0.625, loss: 1.017,data_loss: 1.017, reg_loss: 0.000 lr: 0.000827\n",
      " step: 130, acc: 0.539, loss: 1.131,data_loss: 1.131, reg_loss: 0.000 lr: 0.000827\n",
      " step: 131, acc: 0.602, loss: 0.978,data_loss: 0.978, reg_loss: 0.000 lr: 0.000827\n",
      " step: 132, acc: 0.562, loss: 1.035,data_loss: 1.035, reg_loss: 0.000 lr: 0.000827\n",
      " step: 133, acc: 0.547, loss: 1.123,data_loss: 1.123, reg_loss: 0.000 lr: 0.000827\n",
      " step: 134, acc: 0.648, loss: 0.881,data_loss: 0.881, reg_loss: 0.000 lr: 0.000827\n",
      " step: 135, acc: 0.625, loss: 1.098,data_loss: 1.098, reg_loss: 0.000 lr: 0.000827\n",
      " step: 136, acc: 0.570, loss: 1.287,data_loss: 1.287, reg_loss: 0.000 lr: 0.000827\n",
      " step: 137, acc: 0.633, loss: 0.962,data_loss: 0.962, reg_loss: 0.000 lr: 0.000827\n",
      " step: 138, acc: 0.633, loss: 0.962,data_loss: 0.962, reg_loss: 0.000 lr: 0.000827\n",
      " step: 139, acc: 0.617, loss: 0.973,data_loss: 0.973, reg_loss: 0.000 lr: 0.000827\n",
      " step: 140, acc: 0.594, loss: 1.062,data_loss: 1.062, reg_loss: 0.000 lr: 0.000827\n",
      " step: 141, acc: 0.570, loss: 1.025,data_loss: 1.025, reg_loss: 0.000 lr: 0.000827\n",
      " step: 142, acc: 0.602, loss: 1.008,data_loss: 1.008, reg_loss: 0.000 lr: 0.000827\n",
      " step: 143, acc: 0.555, loss: 1.182,data_loss: 1.182, reg_loss: 0.000 lr: 0.000827\n",
      " step: 144, acc: 0.602, loss: 1.058,data_loss: 1.058, reg_loss: 0.000 lr: 0.000827\n",
      " step: 145, acc: 0.617, loss: 0.980,data_loss: 0.980, reg_loss: 0.000 lr: 0.000827\n",
      " step: 146, acc: 0.500, loss: 1.164,data_loss: 1.164, reg_loss: 0.000 lr: 0.000827\n",
      " step: 147, acc: 0.594, loss: 0.976,data_loss: 0.976, reg_loss: 0.000 lr: 0.000827\n",
      " step: 148, acc: 0.578, loss: 0.994,data_loss: 0.994, reg_loss: 0.000 lr: 0.000827\n",
      " step: 149, acc: 0.633, loss: 1.035,data_loss: 1.035, reg_loss: 0.000 lr: 0.000826\n",
      " step: 150, acc: 0.594, loss: 0.946,data_loss: 0.946, reg_loss: 0.000 lr: 0.000826\n",
      " step: 151, acc: 0.516, loss: 1.124,data_loss: 1.124, reg_loss: 0.000 lr: 0.000826\n",
      " step: 152, acc: 0.578, loss: 0.987,data_loss: 0.987, reg_loss: 0.000 lr: 0.000826\n",
      " step: 153, acc: 0.641, loss: 0.946,data_loss: 0.946, reg_loss: 0.000 lr: 0.000826\n",
      " step: 154, acc: 0.641, loss: 0.969,data_loss: 0.969, reg_loss: 0.000 lr: 0.000826\n",
      " step: 155, acc: 0.609, loss: 1.020,data_loss: 1.020, reg_loss: 0.000 lr: 0.000826\n",
      " step: 156, acc: 0.531, loss: 1.190,data_loss: 1.190, reg_loss: 0.000 lr: 0.000826\n",
      " step: 157, acc: 0.633, loss: 0.959,data_loss: 0.959, reg_loss: 0.000 lr: 0.000826\n",
      " step: 158, acc: 0.539, loss: 1.175,data_loss: 1.175, reg_loss: 0.000 lr: 0.000826\n",
      " step: 159, acc: 0.570, loss: 1.162,data_loss: 1.162, reg_loss: 0.000 lr: 0.000826\n",
      " step: 160, acc: 0.570, loss: 1.194,data_loss: 1.194, reg_loss: 0.000 lr: 0.000826\n",
      " step: 161, acc: 0.586, loss: 1.104,data_loss: 1.104, reg_loss: 0.000 lr: 0.000826\n",
      " step: 162, acc: 0.578, loss: 1.098,data_loss: 1.098, reg_loss: 0.000 lr: 0.000826\n",
      " step: 163, acc: 0.602, loss: 0.915,data_loss: 0.915, reg_loss: 0.000 lr: 0.000826\n",
      " step: 164, acc: 0.617, loss: 0.958,data_loss: 0.958, reg_loss: 0.000 lr: 0.000826\n",
      " step: 165, acc: 0.531, loss: 1.171,data_loss: 1.171, reg_loss: 0.000 lr: 0.000826\n",
      " step: 166, acc: 0.609, loss: 1.095,data_loss: 1.095, reg_loss: 0.000 lr: 0.000826\n",
      " step: 167, acc: 0.562, loss: 1.210,data_loss: 1.210, reg_loss: 0.000 lr: 0.000826\n",
      " step: 168, acc: 0.562, loss: 1.013,data_loss: 1.013, reg_loss: 0.000 lr: 0.000826\n",
      " step: 169, acc: 0.617, loss: 0.971,data_loss: 0.971, reg_loss: 0.000 lr: 0.000826\n",
      " step: 170, acc: 0.609, loss: 1.046,data_loss: 1.046, reg_loss: 0.000 lr: 0.000826\n",
      " step: 171, acc: 0.633, loss: 0.934,data_loss: 0.934, reg_loss: 0.000 lr: 0.000826\n",
      " step: 172, acc: 0.562, loss: 1.127,data_loss: 1.127, reg_loss: 0.000 lr: 0.000826\n",
      " step: 173, acc: 0.516, loss: 1.195,data_loss: 1.195, reg_loss: 0.000 lr: 0.000826\n",
      " step: 174, acc: 0.516, loss: 1.235,data_loss: 1.235, reg_loss: 0.000 lr: 0.000826\n",
      " step: 175, acc: 0.562, loss: 1.122,data_loss: 1.122, reg_loss: 0.000 lr: 0.000826\n",
      " step: 176, acc: 0.602, loss: 1.063,data_loss: 1.063, reg_loss: 0.000 lr: 0.000826\n",
      " step: 177, acc: 0.688, loss: 0.869,data_loss: 0.869, reg_loss: 0.000 lr: 0.000826\n",
      " step: 178, acc: 0.625, loss: 1.205,data_loss: 1.205, reg_loss: 0.000 lr: 0.000825\n",
      " step: 179, acc: 0.641, loss: 0.959,data_loss: 0.959, reg_loss: 0.000 lr: 0.000825\n",
      " step: 180, acc: 0.680, loss: 0.869,data_loss: 0.869, reg_loss: 0.000 lr: 0.000825\n",
      " step: 181, acc: 0.617, loss: 0.970,data_loss: 0.970, reg_loss: 0.000 lr: 0.000825\n",
      " step: 182, acc: 0.672, loss: 1.023,data_loss: 1.023, reg_loss: 0.000 lr: 0.000825\n",
      " step: 183, acc: 0.594, loss: 1.062,data_loss: 1.062, reg_loss: 0.000 lr: 0.000825\n",
      " step: 184, acc: 0.555, loss: 1.085,data_loss: 1.085, reg_loss: 0.000 lr: 0.000825\n",
      " step: 185, acc: 0.617, loss: 1.075,data_loss: 1.075, reg_loss: 0.000 lr: 0.000825\n",
      " step: 186, acc: 0.578, loss: 0.952,data_loss: 0.952, reg_loss: 0.000 lr: 0.000825\n",
      " step: 187, acc: 0.609, loss: 1.027,data_loss: 1.027, reg_loss: 0.000 lr: 0.000825\n",
      " step: 188, acc: 0.547, loss: 1.105,data_loss: 1.105, reg_loss: 0.000 lr: 0.000825\n",
      " step: 189, acc: 0.562, loss: 1.125,data_loss: 1.125, reg_loss: 0.000 lr: 0.000825\n",
      " step: 190, acc: 0.539, loss: 1.160,data_loss: 1.160, reg_loss: 0.000 lr: 0.000825\n",
      " step: 191, acc: 0.609, loss: 1.002,data_loss: 1.002, reg_loss: 0.000 lr: 0.000825\n",
      " step: 192, acc: 0.609, loss: 1.037,data_loss: 1.037, reg_loss: 0.000 lr: 0.000825\n",
      " step: 193, acc: 0.570, loss: 1.023,data_loss: 1.023, reg_loss: 0.000 lr: 0.000825\n",
      " step: 194, acc: 0.594, loss: 0.963,data_loss: 0.963, reg_loss: 0.000 lr: 0.000825\n",
      " step: 195, acc: 0.609, loss: 1.011,data_loss: 1.011, reg_loss: 0.000 lr: 0.000825\n",
      " step: 196, acc: 0.617, loss: 1.028,data_loss: 1.028, reg_loss: 0.000 lr: 0.000825\n",
      " step: 197, acc: 0.664, loss: 0.969,data_loss: 0.969, reg_loss: 0.000 lr: 0.000825\n",
      " step: 198, acc: 0.555, loss: 1.022,data_loss: 1.022, reg_loss: 0.000 lr: 0.000825\n",
      " step: 199, acc: 0.547, loss: 1.158,data_loss: 1.158, reg_loss: 0.000 lr: 0.000825\n",
      " step: 200, acc: 0.617, loss: 0.973,data_loss: 0.973, reg_loss: 0.000 lr: 0.000825\n",
      " step: 201, acc: 0.656, loss: 0.924,data_loss: 0.924, reg_loss: 0.000 lr: 0.000825\n",
      " step: 202, acc: 0.586, loss: 1.047,data_loss: 1.047, reg_loss: 0.000 lr: 0.000825\n",
      " step: 203, acc: 0.594, loss: 0.964,data_loss: 0.964, reg_loss: 0.000 lr: 0.000825\n",
      " step: 204, acc: 0.617, loss: 1.043,data_loss: 1.043, reg_loss: 0.000 lr: 0.000825\n",
      " step: 205, acc: 0.539, loss: 1.072,data_loss: 1.072, reg_loss: 0.000 lr: 0.000825\n",
      " step: 206, acc: 0.570, loss: 1.032,data_loss: 1.032, reg_loss: 0.000 lr: 0.000825\n",
      " step: 207, acc: 0.570, loss: 1.081,data_loss: 1.081, reg_loss: 0.000 lr: 0.000825\n",
      " step: 208, acc: 0.656, loss: 0.949,data_loss: 0.949, reg_loss: 0.000 lr: 0.000824\n",
      " step: 209, acc: 0.625, loss: 1.007,data_loss: 1.007, reg_loss: 0.000 lr: 0.000824\n",
      " step: 210, acc: 0.688, loss: 0.872,data_loss: 0.872, reg_loss: 0.000 lr: 0.000824\n",
      " step: 211, acc: 0.633, loss: 0.936,data_loss: 0.936, reg_loss: 0.000 lr: 0.000824\n",
      " step: 212, acc: 0.586, loss: 1.075,data_loss: 1.075, reg_loss: 0.000 lr: 0.000824\n",
      " step: 213, acc: 0.609, loss: 1.019,data_loss: 1.019, reg_loss: 0.000 lr: 0.000824\n",
      " step: 214, acc: 0.625, loss: 1.060,data_loss: 1.060, reg_loss: 0.000 lr: 0.000824\n",
      " step: 215, acc: 0.703, loss: 0.859,data_loss: 0.859, reg_loss: 0.000 lr: 0.000824\n",
      " step: 216, acc: 0.648, loss: 0.939,data_loss: 0.939, reg_loss: 0.000 lr: 0.000824\n",
      " step: 217, acc: 0.523, loss: 1.091,data_loss: 1.091, reg_loss: 0.000 lr: 0.000824\n",
      " step: 218, acc: 0.664, loss: 0.904,data_loss: 0.904, reg_loss: 0.000 lr: 0.000824\n",
      " step: 219, acc: 0.562, loss: 1.117,data_loss: 1.117, reg_loss: 0.000 lr: 0.000824\n",
      " step: 220, acc: 0.633, loss: 1.026,data_loss: 1.026, reg_loss: 0.000 lr: 0.000824\n",
      " step: 221, acc: 0.594, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000824\n",
      " step: 222, acc: 0.609, loss: 1.001,data_loss: 1.001, reg_loss: 0.000 lr: 0.000824\n",
      " step: 223, acc: 0.570, loss: 1.152,data_loss: 1.152, reg_loss: 0.000 lr: 0.000824\n",
      " step: 224, acc: 0.757, loss: 0.677,data_loss: 0.677, reg_loss: 0.000 lr: 0.000824\n",
      "training , acc: 0.599, loss: 1.040,data_loss: 1.040, reg_loss: 0.000 lr: 0.000824\n",
      "validation, acc:0.460 ,loss: 1.564 \n",
      "epoch: 20\n",
      " step: 0, acc: 0.531, loss: 1.111,data_loss: 1.111, reg_loss: 0.000 lr: 0.000824\n",
      " step: 1, acc: 0.609, loss: 1.128,data_loss: 1.128, reg_loss: 0.000 lr: 0.000824\n",
      " step: 2, acc: 0.562, loss: 1.038,data_loss: 1.038, reg_loss: 0.000 lr: 0.000824\n",
      " step: 3, acc: 0.680, loss: 0.916,data_loss: 0.916, reg_loss: 0.000 lr: 0.000824\n",
      " step: 4, acc: 0.641, loss: 1.028,data_loss: 1.028, reg_loss: 0.000 lr: 0.000824\n",
      " step: 5, acc: 0.648, loss: 1.044,data_loss: 1.044, reg_loss: 0.000 lr: 0.000824\n",
      " step: 6, acc: 0.578, loss: 1.042,data_loss: 1.042, reg_loss: 0.000 lr: 0.000824\n",
      " step: 7, acc: 0.586, loss: 1.068,data_loss: 1.068, reg_loss: 0.000 lr: 0.000824\n",
      " step: 8, acc: 0.688, loss: 0.984,data_loss: 0.984, reg_loss: 0.000 lr: 0.000824\n",
      " step: 9, acc: 0.672, loss: 0.988,data_loss: 0.988, reg_loss: 0.000 lr: 0.000824\n",
      " step: 10, acc: 0.680, loss: 0.896,data_loss: 0.896, reg_loss: 0.000 lr: 0.000824\n",
      " step: 11, acc: 0.656, loss: 0.970,data_loss: 0.970, reg_loss: 0.000 lr: 0.000824\n",
      " step: 12, acc: 0.656, loss: 0.969,data_loss: 0.969, reg_loss: 0.000 lr: 0.000823\n",
      " step: 13, acc: 0.547, loss: 1.168,data_loss: 1.168, reg_loss: 0.000 lr: 0.000823\n",
      " step: 14, acc: 0.617, loss: 1.070,data_loss: 1.070, reg_loss: 0.000 lr: 0.000823\n",
      " step: 15, acc: 0.586, loss: 1.034,data_loss: 1.034, reg_loss: 0.000 lr: 0.000823\n",
      " step: 16, acc: 0.672, loss: 0.846,data_loss: 0.846, reg_loss: 0.000 lr: 0.000823\n",
      " step: 17, acc: 0.609, loss: 0.922,data_loss: 0.922, reg_loss: 0.000 lr: 0.000823\n",
      " step: 18, acc: 0.633, loss: 0.985,data_loss: 0.985, reg_loss: 0.000 lr: 0.000823\n",
      " step: 19, acc: 0.641, loss: 0.935,data_loss: 0.935, reg_loss: 0.000 lr: 0.000823\n",
      " step: 20, acc: 0.633, loss: 0.971,data_loss: 0.971, reg_loss: 0.000 lr: 0.000823\n",
      " step: 21, acc: 0.688, loss: 0.997,data_loss: 0.997, reg_loss: 0.000 lr: 0.000823\n",
      " step: 22, acc: 0.609, loss: 0.982,data_loss: 0.982, reg_loss: 0.000 lr: 0.000823\n",
      " step: 23, acc: 0.594, loss: 1.030,data_loss: 1.030, reg_loss: 0.000 lr: 0.000823\n",
      " step: 24, acc: 0.656, loss: 0.955,data_loss: 0.955, reg_loss: 0.000 lr: 0.000823\n",
      " step: 25, acc: 0.664, loss: 0.912,data_loss: 0.912, reg_loss: 0.000 lr: 0.000823\n",
      " step: 26, acc: 0.562, loss: 1.063,data_loss: 1.063, reg_loss: 0.000 lr: 0.000823\n",
      " step: 27, acc: 0.641, loss: 0.968,data_loss: 0.968, reg_loss: 0.000 lr: 0.000823\n",
      " step: 28, acc: 0.594, loss: 0.918,data_loss: 0.918, reg_loss: 0.000 lr: 0.000823\n",
      " step: 29, acc: 0.656, loss: 1.002,data_loss: 1.002, reg_loss: 0.000 lr: 0.000823\n",
      " step: 30, acc: 0.523, loss: 1.134,data_loss: 1.134, reg_loss: 0.000 lr: 0.000823\n",
      " step: 31, acc: 0.711, loss: 0.814,data_loss: 0.814, reg_loss: 0.000 lr: 0.000823\n",
      " step: 32, acc: 0.617, loss: 0.947,data_loss: 0.947, reg_loss: 0.000 lr: 0.000823\n",
      " step: 33, acc: 0.617, loss: 0.992,data_loss: 0.992, reg_loss: 0.000 lr: 0.000823\n",
      " step: 34, acc: 0.625, loss: 1.031,data_loss: 1.031, reg_loss: 0.000 lr: 0.000823\n",
      " step: 35, acc: 0.617, loss: 1.014,data_loss: 1.014, reg_loss: 0.000 lr: 0.000823\n",
      " step: 36, acc: 0.578, loss: 1.006,data_loss: 1.006, reg_loss: 0.000 lr: 0.000823\n",
      " step: 37, acc: 0.641, loss: 1.063,data_loss: 1.063, reg_loss: 0.000 lr: 0.000823\n",
      " step: 38, acc: 0.625, loss: 0.967,data_loss: 0.967, reg_loss: 0.000 lr: 0.000823\n",
      " step: 39, acc: 0.516, loss: 1.136,data_loss: 1.136, reg_loss: 0.000 lr: 0.000823\n",
      " step: 40, acc: 0.586, loss: 1.118,data_loss: 1.118, reg_loss: 0.000 lr: 0.000823\n",
      " step: 41, acc: 0.492, loss: 1.177,data_loss: 1.177, reg_loss: 0.000 lr: 0.000823\n",
      " step: 42, acc: 0.641, loss: 1.030,data_loss: 1.030, reg_loss: 0.000 lr: 0.000822\n",
      " step: 43, acc: 0.555, loss: 1.100,data_loss: 1.100, reg_loss: 0.000 lr: 0.000822\n",
      " step: 44, acc: 0.641, loss: 0.923,data_loss: 0.923, reg_loss: 0.000 lr: 0.000822\n",
      " step: 45, acc: 0.586, loss: 1.037,data_loss: 1.037, reg_loss: 0.000 lr: 0.000822\n",
      " step: 46, acc: 0.648, loss: 0.945,data_loss: 0.945, reg_loss: 0.000 lr: 0.000822\n",
      " step: 47, acc: 0.586, loss: 1.057,data_loss: 1.057, reg_loss: 0.000 lr: 0.000822\n",
      " step: 48, acc: 0.594, loss: 0.959,data_loss: 0.959, reg_loss: 0.000 lr: 0.000822\n",
      " step: 49, acc: 0.617, loss: 0.994,data_loss: 0.994, reg_loss: 0.000 lr: 0.000822\n",
      " step: 50, acc: 0.648, loss: 0.902,data_loss: 0.902, reg_loss: 0.000 lr: 0.000822\n",
      " step: 51, acc: 0.516, loss: 1.174,data_loss: 1.174, reg_loss: 0.000 lr: 0.000822\n",
      " step: 52, acc: 0.625, loss: 0.975,data_loss: 0.975, reg_loss: 0.000 lr: 0.000822\n",
      " step: 53, acc: 0.648, loss: 1.039,data_loss: 1.039, reg_loss: 0.000 lr: 0.000822\n",
      " step: 54, acc: 0.562, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000822\n",
      " step: 55, acc: 0.625, loss: 1.001,data_loss: 1.001, reg_loss: 0.000 lr: 0.000822\n",
      " step: 56, acc: 0.656, loss: 0.894,data_loss: 0.894, reg_loss: 0.000 lr: 0.000822\n",
      " step: 57, acc: 0.609, loss: 1.042,data_loss: 1.042, reg_loss: 0.000 lr: 0.000822\n",
      " step: 58, acc: 0.578, loss: 1.069,data_loss: 1.069, reg_loss: 0.000 lr: 0.000822\n",
      " step: 59, acc: 0.680, loss: 0.907,data_loss: 0.907, reg_loss: 0.000 lr: 0.000822\n",
      " step: 60, acc: 0.617, loss: 0.984,data_loss: 0.984, reg_loss: 0.000 lr: 0.000822\n",
      " step: 61, acc: 0.656, loss: 1.003,data_loss: 1.003, reg_loss: 0.000 lr: 0.000822\n",
      " step: 62, acc: 0.594, loss: 1.049,data_loss: 1.049, reg_loss: 0.000 lr: 0.000822\n",
      " step: 63, acc: 0.617, loss: 1.023,data_loss: 1.023, reg_loss: 0.000 lr: 0.000822\n",
      " step: 64, acc: 0.523, loss: 1.128,data_loss: 1.128, reg_loss: 0.000 lr: 0.000822\n",
      " step: 65, acc: 0.586, loss: 1.040,data_loss: 1.040, reg_loss: 0.000 lr: 0.000822\n",
      " step: 66, acc: 0.672, loss: 0.879,data_loss: 0.879, reg_loss: 0.000 lr: 0.000822\n",
      " step: 67, acc: 0.609, loss: 1.042,data_loss: 1.042, reg_loss: 0.000 lr: 0.000822\n",
      " step: 68, acc: 0.617, loss: 1.009,data_loss: 1.009, reg_loss: 0.000 lr: 0.000822\n",
      " step: 69, acc: 0.555, loss: 1.096,data_loss: 1.096, reg_loss: 0.000 lr: 0.000822\n",
      " step: 70, acc: 0.617, loss: 0.990,data_loss: 0.990, reg_loss: 0.000 lr: 0.000822\n",
      " step: 71, acc: 0.641, loss: 0.986,data_loss: 0.986, reg_loss: 0.000 lr: 0.000821\n",
      " step: 72, acc: 0.602, loss: 1.030,data_loss: 1.030, reg_loss: 0.000 lr: 0.000821\n",
      " step: 73, acc: 0.547, loss: 1.119,data_loss: 1.119, reg_loss: 0.000 lr: 0.000821\n",
      " step: 74, acc: 0.617, loss: 1.056,data_loss: 1.056, reg_loss: 0.000 lr: 0.000821\n",
      " step: 75, acc: 0.523, loss: 1.124,data_loss: 1.124, reg_loss: 0.000 lr: 0.000821\n",
      " step: 76, acc: 0.695, loss: 0.877,data_loss: 0.877, reg_loss: 0.000 lr: 0.000821\n",
      " step: 77, acc: 0.602, loss: 0.947,data_loss: 0.947, reg_loss: 0.000 lr: 0.000821\n",
      " step: 78, acc: 0.594, loss: 0.957,data_loss: 0.957, reg_loss: 0.000 lr: 0.000821\n",
      " step: 79, acc: 0.547, loss: 1.113,data_loss: 1.113, reg_loss: 0.000 lr: 0.000821\n",
      " step: 80, acc: 0.594, loss: 1.129,data_loss: 1.129, reg_loss: 0.000 lr: 0.000821\n",
      " step: 81, acc: 0.609, loss: 1.121,data_loss: 1.121, reg_loss: 0.000 lr: 0.000821\n",
      " step: 82, acc: 0.625, loss: 1.050,data_loss: 1.050, reg_loss: 0.000 lr: 0.000821\n",
      " step: 83, acc: 0.602, loss: 1.034,data_loss: 1.034, reg_loss: 0.000 lr: 0.000821\n",
      " step: 84, acc: 0.602, loss: 1.060,data_loss: 1.060, reg_loss: 0.000 lr: 0.000821\n",
      " step: 85, acc: 0.602, loss: 1.083,data_loss: 1.083, reg_loss: 0.000 lr: 0.000821\n",
      " step: 86, acc: 0.602, loss: 0.993,data_loss: 0.993, reg_loss: 0.000 lr: 0.000821\n",
      " step: 87, acc: 0.586, loss: 1.091,data_loss: 1.091, reg_loss: 0.000 lr: 0.000821\n",
      " step: 88, acc: 0.578, loss: 1.076,data_loss: 1.076, reg_loss: 0.000 lr: 0.000821\n",
      " step: 89, acc: 0.578, loss: 1.041,data_loss: 1.041, reg_loss: 0.000 lr: 0.000821\n",
      " step: 90, acc: 0.586, loss: 1.073,data_loss: 1.073, reg_loss: 0.000 lr: 0.000821\n",
      " step: 91, acc: 0.539, loss: 1.117,data_loss: 1.117, reg_loss: 0.000 lr: 0.000821\n",
      " step: 92, acc: 0.594, loss: 1.001,data_loss: 1.001, reg_loss: 0.000 lr: 0.000821\n",
      " step: 93, acc: 0.703, loss: 0.954,data_loss: 0.954, reg_loss: 0.000 lr: 0.000821\n",
      " step: 94, acc: 0.531, loss: 1.254,data_loss: 1.254, reg_loss: 0.000 lr: 0.000821\n",
      " step: 95, acc: 0.633, loss: 0.910,data_loss: 0.910, reg_loss: 0.000 lr: 0.000821\n",
      " step: 96, acc: 0.547, loss: 1.184,data_loss: 1.184, reg_loss: 0.000 lr: 0.000821\n",
      " step: 97, acc: 0.648, loss: 0.931,data_loss: 0.931, reg_loss: 0.000 lr: 0.000821\n",
      " step: 98, acc: 0.672, loss: 0.892,data_loss: 0.892, reg_loss: 0.000 lr: 0.000821\n",
      " step: 99, acc: 0.648, loss: 0.935,data_loss: 0.935, reg_loss: 0.000 lr: 0.000821\n",
      " step: 100, acc: 0.609, loss: 0.981,data_loss: 0.981, reg_loss: 0.000 lr: 0.000821\n",
      " step: 101, acc: 0.586, loss: 0.976,data_loss: 0.976, reg_loss: 0.000 lr: 0.000820\n",
      " step: 102, acc: 0.562, loss: 1.113,data_loss: 1.113, reg_loss: 0.000 lr: 0.000820\n",
      " step: 103, acc: 0.633, loss: 0.916,data_loss: 0.916, reg_loss: 0.000 lr: 0.000820\n",
      " step: 104, acc: 0.633, loss: 0.949,data_loss: 0.949, reg_loss: 0.000 lr: 0.000820\n",
      " step: 105, acc: 0.586, loss: 1.031,data_loss: 1.031, reg_loss: 0.000 lr: 0.000820\n",
      " step: 106, acc: 0.680, loss: 0.867,data_loss: 0.867, reg_loss: 0.000 lr: 0.000820\n",
      " step: 107, acc: 0.625, loss: 0.950,data_loss: 0.950, reg_loss: 0.000 lr: 0.000820\n",
      " step: 108, acc: 0.633, loss: 0.955,data_loss: 0.955, reg_loss: 0.000 lr: 0.000820\n",
      " step: 109, acc: 0.656, loss: 0.987,data_loss: 0.987, reg_loss: 0.000 lr: 0.000820\n",
      " step: 110, acc: 0.633, loss: 0.970,data_loss: 0.970, reg_loss: 0.000 lr: 0.000820\n",
      " step: 111, acc: 0.633, loss: 1.031,data_loss: 1.031, reg_loss: 0.000 lr: 0.000820\n",
      " step: 112, acc: 0.586, loss: 1.055,data_loss: 1.055, reg_loss: 0.000 lr: 0.000820\n",
      " step: 113, acc: 0.641, loss: 1.005,data_loss: 1.005, reg_loss: 0.000 lr: 0.000820\n",
      " step: 114, acc: 0.547, loss: 1.142,data_loss: 1.142, reg_loss: 0.000 lr: 0.000820\n",
      " step: 115, acc: 0.641, loss: 0.967,data_loss: 0.967, reg_loss: 0.000 lr: 0.000820\n",
      " step: 116, acc: 0.578, loss: 1.155,data_loss: 1.155, reg_loss: 0.000 lr: 0.000820\n",
      " step: 117, acc: 0.633, loss: 0.954,data_loss: 0.954, reg_loss: 0.000 lr: 0.000820\n",
      " step: 118, acc: 0.562, loss: 1.049,data_loss: 1.049, reg_loss: 0.000 lr: 0.000820\n",
      " step: 119, acc: 0.664, loss: 0.838,data_loss: 0.838, reg_loss: 0.000 lr: 0.000820\n",
      " step: 120, acc: 0.617, loss: 1.015,data_loss: 1.015, reg_loss: 0.000 lr: 0.000820\n",
      " step: 121, acc: 0.633, loss: 1.133,data_loss: 1.133, reg_loss: 0.000 lr: 0.000820\n",
      " step: 122, acc: 0.555, loss: 1.083,data_loss: 1.083, reg_loss: 0.000 lr: 0.000820\n",
      " step: 123, acc: 0.547, loss: 1.125,data_loss: 1.125, reg_loss: 0.000 lr: 0.000820\n",
      " step: 124, acc: 0.594, loss: 1.034,data_loss: 1.034, reg_loss: 0.000 lr: 0.000820\n",
      " step: 125, acc: 0.523, loss: 1.119,data_loss: 1.119, reg_loss: 0.000 lr: 0.000820\n",
      " step: 126, acc: 0.688, loss: 0.998,data_loss: 0.998, reg_loss: 0.000 lr: 0.000820\n",
      " step: 127, acc: 0.555, loss: 1.131,data_loss: 1.131, reg_loss: 0.000 lr: 0.000820\n",
      " step: 128, acc: 0.594, loss: 0.962,data_loss: 0.962, reg_loss: 0.000 lr: 0.000820\n",
      " step: 129, acc: 0.648, loss: 0.986,data_loss: 0.986, reg_loss: 0.000 lr: 0.000820\n",
      " step: 130, acc: 0.547, loss: 1.197,data_loss: 1.197, reg_loss: 0.000 lr: 0.000820\n",
      " step: 131, acc: 0.625, loss: 1.059,data_loss: 1.059, reg_loss: 0.000 lr: 0.000819\n",
      " step: 132, acc: 0.656, loss: 0.946,data_loss: 0.946, reg_loss: 0.000 lr: 0.000819\n",
      " step: 133, acc: 0.578, loss: 1.110,data_loss: 1.110, reg_loss: 0.000 lr: 0.000819\n",
      " step: 134, acc: 0.688, loss: 0.908,data_loss: 0.908, reg_loss: 0.000 lr: 0.000819\n",
      " step: 135, acc: 0.547, loss: 1.060,data_loss: 1.060, reg_loss: 0.000 lr: 0.000819\n",
      " step: 136, acc: 0.562, loss: 1.136,data_loss: 1.136, reg_loss: 0.000 lr: 0.000819\n",
      " step: 137, acc: 0.711, loss: 0.893,data_loss: 0.893, reg_loss: 0.000 lr: 0.000819\n",
      " step: 138, acc: 0.656, loss: 0.851,data_loss: 0.851, reg_loss: 0.000 lr: 0.000819\n",
      " step: 139, acc: 0.586, loss: 1.130,data_loss: 1.130, reg_loss: 0.000 lr: 0.000819\n",
      " step: 140, acc: 0.625, loss: 0.882,data_loss: 0.882, reg_loss: 0.000 lr: 0.000819\n",
      " step: 141, acc: 0.594, loss: 1.134,data_loss: 1.134, reg_loss: 0.000 lr: 0.000819\n",
      " step: 142, acc: 0.672, loss: 0.872,data_loss: 0.872, reg_loss: 0.000 lr: 0.000819\n",
      " step: 143, acc: 0.586, loss: 1.091,data_loss: 1.091, reg_loss: 0.000 lr: 0.000819\n",
      " step: 144, acc: 0.594, loss: 1.088,data_loss: 1.088, reg_loss: 0.000 lr: 0.000819\n",
      " step: 145, acc: 0.648, loss: 0.896,data_loss: 0.896, reg_loss: 0.000 lr: 0.000819\n",
      " step: 146, acc: 0.570, loss: 1.158,data_loss: 1.158, reg_loss: 0.000 lr: 0.000819\n",
      " step: 147, acc: 0.625, loss: 1.041,data_loss: 1.041, reg_loss: 0.000 lr: 0.000819\n",
      " step: 148, acc: 0.625, loss: 0.951,data_loss: 0.951, reg_loss: 0.000 lr: 0.000819\n",
      " step: 149, acc: 0.586, loss: 1.071,data_loss: 1.071, reg_loss: 0.000 lr: 0.000819\n",
      " step: 150, acc: 0.680, loss: 0.889,data_loss: 0.889, reg_loss: 0.000 lr: 0.000819\n",
      " step: 151, acc: 0.617, loss: 1.018,data_loss: 1.018, reg_loss: 0.000 lr: 0.000819\n",
      " step: 152, acc: 0.680, loss: 0.826,data_loss: 0.826, reg_loss: 0.000 lr: 0.000819\n",
      " step: 153, acc: 0.602, loss: 0.985,data_loss: 0.985, reg_loss: 0.000 lr: 0.000819\n",
      " step: 154, acc: 0.625, loss: 1.073,data_loss: 1.073, reg_loss: 0.000 lr: 0.000819\n",
      " step: 155, acc: 0.625, loss: 0.963,data_loss: 0.963, reg_loss: 0.000 lr: 0.000819\n",
      " step: 156, acc: 0.523, loss: 1.116,data_loss: 1.116, reg_loss: 0.000 lr: 0.000819\n",
      " step: 157, acc: 0.609, loss: 0.946,data_loss: 0.946, reg_loss: 0.000 lr: 0.000819\n",
      " step: 158, acc: 0.594, loss: 1.095,data_loss: 1.095, reg_loss: 0.000 lr: 0.000819\n",
      " step: 159, acc: 0.602, loss: 1.045,data_loss: 1.045, reg_loss: 0.000 lr: 0.000819\n",
      " step: 160, acc: 0.586, loss: 1.246,data_loss: 1.246, reg_loss: 0.000 lr: 0.000818\n",
      " step: 161, acc: 0.562, loss: 1.109,data_loss: 1.109, reg_loss: 0.000 lr: 0.000818\n",
      " step: 162, acc: 0.570, loss: 1.063,data_loss: 1.063, reg_loss: 0.000 lr: 0.000818\n",
      " step: 163, acc: 0.602, loss: 1.047,data_loss: 1.047, reg_loss: 0.000 lr: 0.000818\n",
      " step: 164, acc: 0.633, loss: 0.917,data_loss: 0.917, reg_loss: 0.000 lr: 0.000818\n",
      " step: 165, acc: 0.586, loss: 1.073,data_loss: 1.073, reg_loss: 0.000 lr: 0.000818\n",
      " step: 166, acc: 0.523, loss: 1.242,data_loss: 1.242, reg_loss: 0.000 lr: 0.000818\n",
      " step: 167, acc: 0.539, loss: 1.216,data_loss: 1.216, reg_loss: 0.000 lr: 0.000818\n",
      " step: 168, acc: 0.594, loss: 1.067,data_loss: 1.067, reg_loss: 0.000 lr: 0.000818\n",
      " step: 169, acc: 0.625, loss: 0.998,data_loss: 0.998, reg_loss: 0.000 lr: 0.000818\n",
      " step: 170, acc: 0.625, loss: 1.084,data_loss: 1.084, reg_loss: 0.000 lr: 0.000818\n",
      " step: 171, acc: 0.656, loss: 0.934,data_loss: 0.934, reg_loss: 0.000 lr: 0.000818\n",
      " step: 172, acc: 0.562, loss: 1.066,data_loss: 1.066, reg_loss: 0.000 lr: 0.000818\n",
      " step: 173, acc: 0.641, loss: 1.069,data_loss: 1.069, reg_loss: 0.000 lr: 0.000818\n",
      " step: 174, acc: 0.594, loss: 0.976,data_loss: 0.976, reg_loss: 0.000 lr: 0.000818\n",
      " step: 175, acc: 0.570, loss: 1.019,data_loss: 1.019, reg_loss: 0.000 lr: 0.000818\n",
      " step: 176, acc: 0.555, loss: 1.138,data_loss: 1.138, reg_loss: 0.000 lr: 0.000818\n",
      " step: 177, acc: 0.633, loss: 0.927,data_loss: 0.927, reg_loss: 0.000 lr: 0.000818\n",
      " step: 178, acc: 0.570, loss: 1.145,data_loss: 1.145, reg_loss: 0.000 lr: 0.000818\n",
      " step: 179, acc: 0.672, loss: 0.799,data_loss: 0.799, reg_loss: 0.000 lr: 0.000818\n",
      " step: 180, acc: 0.633, loss: 0.956,data_loss: 0.956, reg_loss: 0.000 lr: 0.000818\n",
      " step: 181, acc: 0.586, loss: 1.079,data_loss: 1.079, reg_loss: 0.000 lr: 0.000818\n",
      " step: 182, acc: 0.633, loss: 0.998,data_loss: 0.998, reg_loss: 0.000 lr: 0.000818\n",
      " step: 183, acc: 0.633, loss: 0.915,data_loss: 0.915, reg_loss: 0.000 lr: 0.000818\n",
      " step: 184, acc: 0.547, loss: 1.107,data_loss: 1.107, reg_loss: 0.000 lr: 0.000818\n",
      " step: 185, acc: 0.547, loss: 1.128,data_loss: 1.128, reg_loss: 0.000 lr: 0.000818\n",
      " step: 186, acc: 0.594, loss: 0.934,data_loss: 0.934, reg_loss: 0.000 lr: 0.000818\n",
      " step: 187, acc: 0.680, loss: 0.905,data_loss: 0.905, reg_loss: 0.000 lr: 0.000818\n",
      " step: 188, acc: 0.555, loss: 1.080,data_loss: 1.080, reg_loss: 0.000 lr: 0.000818\n",
      " step: 189, acc: 0.562, loss: 1.019,data_loss: 1.019, reg_loss: 0.000 lr: 0.000818\n",
      " step: 190, acc: 0.555, loss: 1.030,data_loss: 1.030, reg_loss: 0.000 lr: 0.000817\n",
      " step: 191, acc: 0.594, loss: 1.030,data_loss: 1.030, reg_loss: 0.000 lr: 0.000817\n",
      " step: 192, acc: 0.594, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000817\n",
      " step: 193, acc: 0.570, loss: 1.115,data_loss: 1.115, reg_loss: 0.000 lr: 0.000817\n",
      " step: 194, acc: 0.641, loss: 0.951,data_loss: 0.951, reg_loss: 0.000 lr: 0.000817\n",
      " step: 195, acc: 0.641, loss: 0.947,data_loss: 0.947, reg_loss: 0.000 lr: 0.000817\n",
      " step: 196, acc: 0.602, loss: 1.038,data_loss: 1.038, reg_loss: 0.000 lr: 0.000817\n",
      " step: 197, acc: 0.586, loss: 1.012,data_loss: 1.012, reg_loss: 0.000 lr: 0.000817\n",
      " step: 198, acc: 0.656, loss: 0.941,data_loss: 0.941, reg_loss: 0.000 lr: 0.000817\n",
      " step: 199, acc: 0.562, loss: 1.078,data_loss: 1.078, reg_loss: 0.000 lr: 0.000817\n",
      " step: 200, acc: 0.641, loss: 0.921,data_loss: 0.921, reg_loss: 0.000 lr: 0.000817\n",
      " step: 201, acc: 0.633, loss: 0.991,data_loss: 0.991, reg_loss: 0.000 lr: 0.000817\n",
      " step: 202, acc: 0.539, loss: 1.122,data_loss: 1.122, reg_loss: 0.000 lr: 0.000817\n",
      " step: 203, acc: 0.602, loss: 0.988,data_loss: 0.988, reg_loss: 0.000 lr: 0.000817\n",
      " step: 204, acc: 0.617, loss: 1.021,data_loss: 1.021, reg_loss: 0.000 lr: 0.000817\n",
      " step: 205, acc: 0.570, loss: 0.979,data_loss: 0.979, reg_loss: 0.000 lr: 0.000817\n",
      " step: 206, acc: 0.609, loss: 0.970,data_loss: 0.970, reg_loss: 0.000 lr: 0.000817\n",
      " step: 207, acc: 0.609, loss: 1.069,data_loss: 1.069, reg_loss: 0.000 lr: 0.000817\n",
      " step: 208, acc: 0.664, loss: 0.919,data_loss: 0.919, reg_loss: 0.000 lr: 0.000817\n",
      " step: 209, acc: 0.609, loss: 1.080,data_loss: 1.080, reg_loss: 0.000 lr: 0.000817\n",
      " step: 210, acc: 0.625, loss: 0.957,data_loss: 0.957, reg_loss: 0.000 lr: 0.000817\n",
      " step: 211, acc: 0.586, loss: 0.976,data_loss: 0.976, reg_loss: 0.000 lr: 0.000817\n",
      " step: 212, acc: 0.594, loss: 0.988,data_loss: 0.988, reg_loss: 0.000 lr: 0.000817\n",
      " step: 213, acc: 0.602, loss: 1.028,data_loss: 1.028, reg_loss: 0.000 lr: 0.000817\n",
      " step: 214, acc: 0.578, loss: 1.115,data_loss: 1.115, reg_loss: 0.000 lr: 0.000817\n",
      " step: 215, acc: 0.711, loss: 0.813,data_loss: 0.813, reg_loss: 0.000 lr: 0.000817\n",
      " step: 216, acc: 0.703, loss: 0.845,data_loss: 0.845, reg_loss: 0.000 lr: 0.000817\n",
      " step: 217, acc: 0.570, loss: 1.077,data_loss: 1.077, reg_loss: 0.000 lr: 0.000817\n",
      " step: 218, acc: 0.633, loss: 0.988,data_loss: 0.988, reg_loss: 0.000 lr: 0.000817\n",
      " step: 219, acc: 0.633, loss: 0.992,data_loss: 0.992, reg_loss: 0.000 lr: 0.000817\n",
      " step: 220, acc: 0.578, loss: 1.036,data_loss: 1.036, reg_loss: 0.000 lr: 0.000816\n",
      " step: 221, acc: 0.641, loss: 0.973,data_loss: 0.973, reg_loss: 0.000 lr: 0.000816\n",
      " step: 222, acc: 0.625, loss: 1.020,data_loss: 1.020, reg_loss: 0.000 lr: 0.000816\n",
      " step: 223, acc: 0.539, loss: 1.156,data_loss: 1.156, reg_loss: 0.000 lr: 0.000816\n",
      " step: 224, acc: 0.676, loss: 0.782,data_loss: 0.782, reg_loss: 0.000 lr: 0.000816\n",
      "training , acc: 0.609, loss: 1.019,data_loss: 1.019, reg_loss: 0.000 lr: 0.000816\n",
      "validation, acc:0.459 ,loss: 1.562 \n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "model.train(x,y,epochs=20,validation_data=(x_test,y_test),batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss & accuracy(for both training & validation) visualisation during training \n",
    "import matplotlib.pyplot as plt\n",
    "from network_full import *\n",
    "L=range(1,21)#number of epochs\n",
    "plt.figure(1)\n",
    "plt.plot(L,model.history['accuracy_train'],'b-', label=\"training accuracy\")\n",
    "plt.plot(L,model.history['accuracy_val'],'g',label=\"validation accuracy\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.figure(2)\n",
    "plt.plot(L,model.history['loss_train'],'b-', label=\"training loss\")\n",
    "plt.plot(L,model.history['loss_val'],'g',label=\"validation loss\")\n",
    "plt.title(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "model.save('fer13.model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
