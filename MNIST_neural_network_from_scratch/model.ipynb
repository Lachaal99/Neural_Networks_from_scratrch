{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "from array import array\n",
    "from os.path  import join\n",
    "\n",
    "#\n",
    "# MNIST Data Loader Class\n",
    "#\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return np.array(images), np.array(labels)\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)        \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "# Set file paths based on added MNIST Datasets\n",
    "#\n",
    "\n",
    "training_images_filepath = 'train-images-idx3-ubyte/train-images-idx3-ubyte'\n",
    "training_labels_filepath ='train-labels-idx1-ubyte/train-labels-idx1-ubyte'\n",
    "test_images_filepath = 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte'\n",
    "test_labels_filepath = 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte'\n",
    "\n",
    "\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.uint8'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_train.shape\n",
    "plt.imshow(x_train[0],cmap='gray')\n",
    "print(type(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.uint8(255), np.uint8(0))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max(),x_train.min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping and normalisation\n",
    "\n",
    "x_train=(x_train.reshape(x_train.shape[0],x_train.shape[1]*x_train.shape[2]).astype(np.float32)-127.5)/127.5\n",
    "x_test=(x_test.reshape(x_test.shape[0],x_test.shape[1]*x_test.shape[2]).astype(np.float32)-127.5)/127.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float32(1.0), np.float32(-1.0))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.max(),x_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 8, 1, 7, 3], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data shuffling\n",
    "keys=np.array(range(x_train.shape[0]))\n",
    "np.random.shuffle(keys)\n",
    "x_train=x_train[keys]\n",
    "y_train=y_train[keys]\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model building\n",
    "from network_full import *\n",
    "model=Model()\n",
    "\n",
    "model.add(layer_dense(x_train.shape[1],256))\n",
    "model.add(Activation_ReLu())\n",
    "model.add(Layer_Dropout(0.1))\n",
    "model.add(layer_dense(256,128))\n",
    "model.add(Activation_ReLu())\n",
    "model.add(Layer_Dropout(0.1))\n",
    "model.add(layer_dense(128,10))\n",
    "model.add(Activation_SoftMax())\n",
    "model.set(loss=Loss_CategoricalCrossEntropy(),\n",
    "          optimizer=Optimizer_Adam(learning_rate=0.001,decay=1e-7),\n",
    "          accuracy=Accuracy_Categorical())\n",
    "model.finalize()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      " step: 0, acc: 0.109, loss: 2.302,data_loss: 2.302, reg_loss: 0.000 lr: 0.001000\n",
      " step: 100, acc: 0.797, loss: 0.632,data_loss: 0.632, reg_loss: 0.000 lr: 0.001000\n",
      " step: 200, acc: 0.844, loss: 0.449,data_loss: 0.449, reg_loss: 0.000 lr: 0.001000\n",
      " step: 300, acc: 0.875, loss: 0.457,data_loss: 0.457, reg_loss: 0.000 lr: 0.001000\n",
      " step: 400, acc: 0.859, loss: 0.368,data_loss: 0.368, reg_loss: 0.000 lr: 0.001000\n",
      " step: 500, acc: 0.844, loss: 0.599,data_loss: 0.599, reg_loss: 0.000 lr: 0.001000\n",
      " step: 600, acc: 0.891, loss: 0.411,data_loss: 0.411, reg_loss: 0.000 lr: 0.001000\n",
      " step: 700, acc: 0.938, loss: 0.194,data_loss: 0.194, reg_loss: 0.000 lr: 0.001000\n",
      " step: 800, acc: 0.953, loss: 0.184,data_loss: 0.184, reg_loss: 0.000 lr: 0.001000\n",
      " step: 900, acc: 0.875, loss: 0.291,data_loss: 0.291, reg_loss: 0.000 lr: 0.001000\n",
      " step: 937, acc: 0.969, loss: 0.332,data_loss: 0.332, reg_loss: 0.000 lr: 0.001000\n",
      "training , acc: 0.861, loss: 0.445,data_loss: 0.445, reg_loss: 0.000 lr: 0.001000\n",
      "epoch: 2\n",
      " step: 0, acc: 0.953, loss: 0.194,data_loss: 0.194, reg_loss: 0.000 lr: 0.001000\n",
      " step: 100, acc: 0.938, loss: 0.281,data_loss: 0.281, reg_loss: 0.000 lr: 0.001000\n",
      " step: 200, acc: 0.938, loss: 0.192,data_loss: 0.192, reg_loss: 0.000 lr: 0.001000\n",
      " step: 300, acc: 0.938, loss: 0.358,data_loss: 0.358, reg_loss: 0.000 lr: 0.001000\n",
      " step: 400, acc: 0.984, loss: 0.088,data_loss: 0.088, reg_loss: 0.000 lr: 0.001000\n",
      " step: 500, acc: 0.906, loss: 0.336,data_loss: 0.336, reg_loss: 0.000 lr: 0.001000\n",
      " step: 600, acc: 0.938, loss: 0.209,data_loss: 0.209, reg_loss: 0.000 lr: 0.001000\n",
      " step: 700, acc: 0.969, loss: 0.079,data_loss: 0.079, reg_loss: 0.000 lr: 0.001000\n",
      " step: 800, acc: 0.938, loss: 0.166,data_loss: 0.166, reg_loss: 0.000 lr: 0.001000\n",
      " step: 900, acc: 0.906, loss: 0.199,data_loss: 0.199, reg_loss: 0.000 lr: 0.001000\n",
      " step: 937, acc: 0.969, loss: 0.172,data_loss: 0.172, reg_loss: 0.000 lr: 0.001000\n",
      "training , acc: 0.939, loss: 0.196,data_loss: 0.196, reg_loss: 0.000 lr: 0.001000\n",
      "epoch: 3\n",
      " step: 0, acc: 0.984, loss: 0.102,data_loss: 0.102, reg_loss: 0.000 lr: 0.001000\n",
      " step: 100, acc: 0.938, loss: 0.274,data_loss: 0.274, reg_loss: 0.000 lr: 0.001000\n",
      " step: 200, acc: 0.953, loss: 0.116,data_loss: 0.116, reg_loss: 0.000 lr: 0.001000\n",
      " step: 300, acc: 0.938, loss: 0.158,data_loss: 0.158, reg_loss: 0.000 lr: 0.001000\n",
      " step: 400, acc: 0.984, loss: 0.081,data_loss: 0.081, reg_loss: 0.000 lr: 0.001000\n",
      " step: 500, acc: 0.922, loss: 0.275,data_loss: 0.275, reg_loss: 0.000 lr: 0.001000\n",
      " step: 600, acc: 0.938, loss: 0.233,data_loss: 0.233, reg_loss: 0.000 lr: 0.001000\n",
      " step: 700, acc: 0.953, loss: 0.129,data_loss: 0.129, reg_loss: 0.000 lr: 0.001000\n",
      " step: 800, acc: 0.969, loss: 0.052,data_loss: 0.052, reg_loss: 0.000 lr: 0.001000\n",
      " step: 900, acc: 0.984, loss: 0.089,data_loss: 0.089, reg_loss: 0.000 lr: 0.001000\n",
      " step: 937, acc: 0.969, loss: 0.089,data_loss: 0.089, reg_loss: 0.000 lr: 0.001000\n",
      "training , acc: 0.955, loss: 0.146,data_loss: 0.146, reg_loss: 0.000 lr: 0.001000\n",
      "epoch: 4\n",
      " step: 0, acc: 0.984, loss: 0.105,data_loss: 0.105, reg_loss: 0.000 lr: 0.001000\n",
      " step: 100, acc: 0.938, loss: 0.205,data_loss: 0.205, reg_loss: 0.000 lr: 0.001000\n",
      " step: 200, acc: 0.953, loss: 0.104,data_loss: 0.104, reg_loss: 0.000 lr: 0.001000\n",
      " step: 300, acc: 0.953, loss: 0.119,data_loss: 0.119, reg_loss: 0.000 lr: 0.001000\n",
      " step: 400, acc: 0.969, loss: 0.052,data_loss: 0.052, reg_loss: 0.000 lr: 0.001000\n",
      " step: 500, acc: 0.922, loss: 0.214,data_loss: 0.214, reg_loss: 0.000 lr: 0.001000\n",
      " step: 600, acc: 0.922, loss: 0.133,data_loss: 0.133, reg_loss: 0.000 lr: 0.001000\n",
      " step: 700, acc: 1.000, loss: 0.046,data_loss: 0.046, reg_loss: 0.000 lr: 0.001000\n",
      " step: 800, acc: 0.984, loss: 0.053,data_loss: 0.053, reg_loss: 0.000 lr: 0.001000\n",
      " step: 900, acc: 0.984, loss: 0.036,data_loss: 0.036, reg_loss: 0.000 lr: 0.001000\n",
      " step: 937, acc: 0.969, loss: 0.058,data_loss: 0.058, reg_loss: 0.000 lr: 0.001000\n",
      "training , acc: 0.961, loss: 0.123,data_loss: 0.123, reg_loss: 0.000 lr: 0.001000\n",
      "epoch: 5\n",
      " step: 0, acc: 0.969, loss: 0.105,data_loss: 0.105, reg_loss: 0.000 lr: 0.001000\n",
      " step: 100, acc: 0.984, loss: 0.098,data_loss: 0.098, reg_loss: 0.000 lr: 0.001000\n",
      " step: 200, acc: 0.969, loss: 0.085,data_loss: 0.085, reg_loss: 0.000 lr: 0.001000\n",
      " step: 300, acc: 0.953, loss: 0.130,data_loss: 0.130, reg_loss: 0.000 lr: 0.001000\n",
      " step: 400, acc: 0.969, loss: 0.126,data_loss: 0.126, reg_loss: 0.000 lr: 0.001000\n",
      " step: 500, acc: 0.938, loss: 0.232,data_loss: 0.232, reg_loss: 0.000 lr: 0.001000\n",
      " step: 600, acc: 0.953, loss: 0.090,data_loss: 0.090, reg_loss: 0.000 lr: 0.001000\n",
      " step: 700, acc: 0.969, loss: 0.060,data_loss: 0.060, reg_loss: 0.000 lr: 0.001000\n",
      " step: 800, acc: 1.000, loss: 0.045,data_loss: 0.045, reg_loss: 0.000 lr: 0.001000\n",
      " step: 900, acc: 1.000, loss: 0.023,data_loss: 0.023, reg_loss: 0.000 lr: 0.001000\n",
      " step: 937, acc: 1.000, loss: 0.027,data_loss: 0.027, reg_loss: 0.000 lr: 0.001000\n",
      "training , acc: 0.967, loss: 0.108,data_loss: 0.108, reg_loss: 0.000 lr: 0.001000\n",
      "epoch: 6\n",
      " step: 0, acc: 0.969, loss: 0.108,data_loss: 0.108, reg_loss: 0.000 lr: 0.001000\n",
      " step: 100, acc: 0.984, loss: 0.134,data_loss: 0.134, reg_loss: 0.000 lr: 0.001000\n",
      " step: 200, acc: 0.922, loss: 0.161,data_loss: 0.161, reg_loss: 0.000 lr: 0.001000\n",
      " step: 300, acc: 0.953, loss: 0.194,data_loss: 0.194, reg_loss: 0.000 lr: 0.001000\n",
      " step: 400, acc: 1.000, loss: 0.020,data_loss: 0.020, reg_loss: 0.000 lr: 0.000999\n",
      " step: 500, acc: 0.969, loss: 0.171,data_loss: 0.171, reg_loss: 0.000 lr: 0.000999\n",
      " step: 600, acc: 0.969, loss: 0.085,data_loss: 0.085, reg_loss: 0.000 lr: 0.000999\n",
      " step: 700, acc: 0.969, loss: 0.110,data_loss: 0.110, reg_loss: 0.000 lr: 0.000999\n",
      " step: 800, acc: 0.984, loss: 0.041,data_loss: 0.041, reg_loss: 0.000 lr: 0.000999\n",
      " step: 900, acc: 0.984, loss: 0.044,data_loss: 0.044, reg_loss: 0.000 lr: 0.000999\n",
      " step: 937, acc: 1.000, loss: 0.006,data_loss: 0.006, reg_loss: 0.000 lr: 0.000999\n",
      "training , acc: 0.970, loss: 0.097,data_loss: 0.097, reg_loss: 0.000 lr: 0.000999\n",
      "epoch: 7\n",
      " step: 0, acc: 0.953, loss: 0.088,data_loss: 0.088, reg_loss: 0.000 lr: 0.000999\n",
      " step: 100, acc: 0.984, loss: 0.045,data_loss: 0.045, reg_loss: 0.000 lr: 0.000999\n",
      " step: 200, acc: 0.984, loss: 0.070,data_loss: 0.070, reg_loss: 0.000 lr: 0.000999\n",
      " step: 300, acc: 0.953, loss: 0.111,data_loss: 0.111, reg_loss: 0.000 lr: 0.000999\n",
      " step: 400, acc: 1.000, loss: 0.035,data_loss: 0.035, reg_loss: 0.000 lr: 0.000999\n",
      " step: 500, acc: 0.953, loss: 0.218,data_loss: 0.218, reg_loss: 0.000 lr: 0.000999\n",
      " step: 600, acc: 0.969, loss: 0.114,data_loss: 0.114, reg_loss: 0.000 lr: 0.000999\n",
      " step: 700, acc: 0.984, loss: 0.066,data_loss: 0.066, reg_loss: 0.000 lr: 0.000999\n",
      " step: 800, acc: 0.938, loss: 0.156,data_loss: 0.156, reg_loss: 0.000 lr: 0.000999\n",
      " step: 900, acc: 1.000, loss: 0.018,data_loss: 0.018, reg_loss: 0.000 lr: 0.000999\n",
      " step: 937, acc: 0.969, loss: 0.066,data_loss: 0.066, reg_loss: 0.000 lr: 0.000999\n",
      "training , acc: 0.972, loss: 0.088,data_loss: 0.088, reg_loss: 0.000 lr: 0.000999\n",
      "epoch: 8\n",
      " step: 0, acc: 0.984, loss: 0.053,data_loss: 0.053, reg_loss: 0.000 lr: 0.000999\n",
      " step: 100, acc: 1.000, loss: 0.031,data_loss: 0.031, reg_loss: 0.000 lr: 0.000999\n",
      " step: 200, acc: 0.969, loss: 0.109,data_loss: 0.109, reg_loss: 0.000 lr: 0.000999\n",
      " step: 300, acc: 0.969, loss: 0.062,data_loss: 0.062, reg_loss: 0.000 lr: 0.000999\n",
      " step: 400, acc: 0.984, loss: 0.103,data_loss: 0.103, reg_loss: 0.000 lr: 0.000999\n",
      " step: 500, acc: 0.922, loss: 0.217,data_loss: 0.217, reg_loss: 0.000 lr: 0.000999\n",
      " step: 600, acc: 0.969, loss: 0.082,data_loss: 0.082, reg_loss: 0.000 lr: 0.000999\n",
      " step: 700, acc: 0.969, loss: 0.070,data_loss: 0.070, reg_loss: 0.000 lr: 0.000999\n",
      " step: 800, acc: 1.000, loss: 0.036,data_loss: 0.036, reg_loss: 0.000 lr: 0.000999\n",
      " step: 900, acc: 0.969, loss: 0.062,data_loss: 0.062, reg_loss: 0.000 lr: 0.000999\n",
      " step: 937, acc: 1.000, loss: 0.004,data_loss: 0.004, reg_loss: 0.000 lr: 0.000999\n",
      "training , acc: 0.972, loss: 0.086,data_loss: 0.086, reg_loss: 0.000 lr: 0.000999\n",
      "epoch: 9\n",
      " step: 0, acc: 0.984, loss: 0.065,data_loss: 0.065, reg_loss: 0.000 lr: 0.000999\n",
      " step: 100, acc: 1.000, loss: 0.017,data_loss: 0.017, reg_loss: 0.000 lr: 0.000999\n",
      " step: 200, acc: 0.984, loss: 0.072,data_loss: 0.072, reg_loss: 0.000 lr: 0.000999\n",
      " step: 300, acc: 0.953, loss: 0.076,data_loss: 0.076, reg_loss: 0.000 lr: 0.000999\n",
      " step: 400, acc: 1.000, loss: 0.020,data_loss: 0.020, reg_loss: 0.000 lr: 0.000999\n",
      " step: 500, acc: 0.984, loss: 0.056,data_loss: 0.056, reg_loss: 0.000 lr: 0.000999\n",
      " step: 600, acc: 0.969, loss: 0.071,data_loss: 0.071, reg_loss: 0.000 lr: 0.000999\n",
      " step: 700, acc: 0.969, loss: 0.098,data_loss: 0.098, reg_loss: 0.000 lr: 0.000999\n",
      " step: 800, acc: 0.984, loss: 0.032,data_loss: 0.032, reg_loss: 0.000 lr: 0.000999\n",
      " step: 900, acc: 1.000, loss: 0.019,data_loss: 0.019, reg_loss: 0.000 lr: 0.000999\n",
      " step: 937, acc: 1.000, loss: 0.006,data_loss: 0.006, reg_loss: 0.000 lr: 0.000999\n",
      "training , acc: 0.976, loss: 0.075,data_loss: 0.075, reg_loss: 0.000 lr: 0.000999\n",
      "epoch: 10\n",
      " step: 0, acc: 0.969, loss: 0.095,data_loss: 0.095, reg_loss: 0.000 lr: 0.000999\n",
      " step: 100, acc: 0.984, loss: 0.027,data_loss: 0.027, reg_loss: 0.000 lr: 0.000999\n",
      " step: 200, acc: 0.984, loss: 0.056,data_loss: 0.056, reg_loss: 0.000 lr: 0.000999\n",
      " step: 300, acc: 0.969, loss: 0.104,data_loss: 0.104, reg_loss: 0.000 lr: 0.000999\n",
      " step: 400, acc: 1.000, loss: 0.019,data_loss: 0.019, reg_loss: 0.000 lr: 0.000999\n",
      " step: 500, acc: 0.953, loss: 0.148,data_loss: 0.148, reg_loss: 0.000 lr: 0.000999\n",
      " step: 600, acc: 0.984, loss: 0.066,data_loss: 0.066, reg_loss: 0.000 lr: 0.000999\n",
      " step: 700, acc: 0.984, loss: 0.047,data_loss: 0.047, reg_loss: 0.000 lr: 0.000999\n",
      " step: 800, acc: 1.000, loss: 0.027,data_loss: 0.027, reg_loss: 0.000 lr: 0.000999\n",
      " step: 900, acc: 1.000, loss: 0.005,data_loss: 0.005, reg_loss: 0.000 lr: 0.000999\n",
      " step: 937, acc: 1.000, loss: 0.019,data_loss: 0.019, reg_loss: 0.000 lr: 0.000999\n",
      "training , acc: 0.978, loss: 0.069,data_loss: 0.069, reg_loss: 0.000 lr: 0.000999\n"
     ]
    }
   ],
   "source": [
    "model.train(x_train,y_train,epochs=10,batch_size=64,print_every=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc:0.970 ,loss: 0.099 \n"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('hand_written_digits.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
